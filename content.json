{"posts":[{"title":"GeoHash","text":"Redis 在 3.2 版本以后增加了地理位置 GEO 模块 GeoHash 是一种将地理坐标（经度和纬度）编码为字符串的技术，主要用于空间索引和地理位置查询。它通过将经纬度信息编码成一个紧凑的字符串，使得相近的地理位置具有相似的 GeoHash 值，从而方便进行邻近搜索和区域查询。 主要特点 紧凑性：GeoHash 编码后的字符串通常较短，便于存储和传输。 局部性：相邻的地理位置具有相似的 GeoHash 值，便于进行邻近搜索。 层次性：GeoHash 可以通过截取字符串的不同长度来实现不同精度的地理位置表示。 易于实现：GeoHash 的编码和解码算法相对简单，易于实现。 GeoHash 的编码过程如下： 经纬度范围划分： 经度范围：-180° 到 180° 纬度范围：-90° 到 90° 交替编码： 将经度和纬度交替地划分为二进制区间。 每次划分后，将结果编码为一个二进制位。 经度和纬度的二进制位交替组合，形成最终的 GeoHash 编码。 Base32 编码： 为了提高可读性和减少存储空间，GeoHash 使用 Base32 编码将二进制位转换为字符。 Base32 字符集包含 32 个字符：0123456789bcdefghjkmnpqrstuvwxyz 编码过程示例1. 确定初始区间，经度为 [-180°, +180°]，纬度为 [-90°, +90°]。 2. 将初始区间对半拆分得到左半区间和右半区间，根据目标位置的经度或纬度是落在左区间还是右区间，决定当前位的二进制编码。左区间取 0，右区间取 1。 3. 对上一步中目标位置所在的子区间进行对半划分，按照同样的方式计算出下一位的二进制编码。 4. 重复划分上面的步骤，直到达到期望的编码长度。 进行二进制编码： 30.6599157 的 15 位二进制编码的过程： 将 [-90°, 90°] 对半拆分得到 [-90°, 0°] 和 [0°, 90°]，30.6599157 位于右区间，取 1 。 将 [0°, 90°] 对半拆分得到 [0°, 45°] 和 [45°, 90°]，30.6599157 位于左区间，取0 通过以上计算，纬度 30.6599157 的二进制编码为：10101 01110 01101。 经度 104.0638546 的 15 位二进制编码： 经度 104.0638546 的二进制编码为 11001 01000 00000。 交叉合并经度和纬度的二进制编码 从第 0 位开始，偶数位放经度，奇数位放纬度，得到完整的二进制编码： 将二进制编码分组并计算出对应的 Base32 编码 二进制编码看起来很长，不方便记忆。为了压缩编码长度，geohash 采用了自己的 Base32 编码，将二进制编码转换成方便识别的文本。Geohash 所用的编码表由数字和字母组成，不过去掉了 a，i，l 和 o 四个字母： Geohash 解码Geohash 的解码实际上编码的逆过程，先通过 Base32 编码表找出每个字符的十进制值，然后将十进制转为二进制，最后通过二进制计算出对应的区域范围。 我们得出 wm6n2j 表示的是经度在 (104.062500, 104.073486) 之间，纬度在 (30.657349, 30.662842) 之间的一个矩形区域。 对比（latitude: 30.6599157, longitude: 104.0638546），它恰好在计算出来的范围之内。这个例子很好地说明了 geohash 是如何表示一个区域范围的。 边界问题Geohash 将邻近搜索（proximity search）转换为了字符串前缀匹配，和基于经纬度的算法相比，极大地提高了计算效率。由于 geohash 是将地图划分为矩形网格，并单独对每个矩形进行编码，这就会带来以下问题。比如下图中有 A、B、C 三个点，要查找离 B 最近的点。可以发现，距离较远的 A 和 B 有着相同的 geohash 编码，而较近的 C 的 geohash 编码却有所不同。 非线性问题Geohash 是基于经纬度的，它能反映出两个点在经纬度上面的距离，但是却不能反映出实际距离。在不同的纬度下，单位经度所表示的距离是不一样的。在赤道，单位经度对应的距离为 111.320km，而在 30°N 和 30°S，单位经度对应的距离为110.852km。 这种非线性问题并不是 geohash 和经纬度系统的问题，而是在于将球体表面的坐标映射到二维平面的坐标的不均匀性。在不同的纬度下，指定长度的 geohash 所表示的矩形区域大小也是不一样的。矩形用南北方向的高度（height）和东西方向的宽度（width）来衡量。例如在赤道： Blake Haugen 在他的博客 Geohash Size Variation by Latitude 中展示了不同纬度下不同长度的 geohash 所表示的矩形区域的大小。当 geohash 长度相同时，矩形的高度在不同纬度下是相同的，而矩形的宽度在不同纬度下并不相同。这一点从经纬度的划分上很好理解，假设地球是一个完美的球体，经线圈的周长是相同的，而纬线圈的周长在赤道最大，越靠近两极越小并不断趋近于零。 Redis 的 Geo 指令geoadd 指令携带集合名称以及多个经纬度名称三元组 12345678127.0.0.1:6379&gt; geoadd company 116.48105 39.996794 juejin(integer) 1127.0.0.1:6379&gt; geoadd company 116.514203 39.905409 ireader(integer) 1127.0.0.1:6379&gt; geoadd company 116.489033 40.007669 meituan(integer) 1127.0.0.1:6379&gt; geoadd company 116.562108 39.787602 jd 116.334255 40.027400 xiaomi(integer) 2 geodist 指令可以用来计算两个元素之间的距离，携带集合名称、2 个名称和距离单位 12345678910127.0.0.1:6379&gt; geodist company juejin ireader km&quot;10.5501&quot;127.0.0.1:6379&gt; geodist company juejin meituan km&quot;1.3878&quot;127.0.0.1:6379&gt; geodist company juejin jd km&quot;24.2739&quot;127.0.0.1:6379&gt; geodist company juejin xiaomi km&quot;12.9606&quot;127.0.0.1:6379&gt; geodist company juejin juejin km&quot;0.0000&quot; geopos 指令可以获取集合中任意元素的经纬度坐标，可以一次获取多个。 123456789101127.0.0.1:6379&gt; geopos company juejin1) 1) &quot;116.48104995489120483&quot;2) &quot;39.99679348858259686&quot;127.0.0.1:6379&gt; geopos company ireader1) 1) &quot;116.5142020583152771&quot;2) &quot;39.90540918662494363&quot;127.0.0.1:6379&gt; geopos company juejin ireader1) 1) &quot;116.48104995489120483&quot;2) &quot;39.99679348858259686&quot;2) 1) &quot;116.5142020583152771&quot;2) &quot;39.90540918662494363&quot; geohash 可以获取元素的经纬度编码字符串，上面已经提到，它是 base32 编码。 你可 以使用这个编码值去 http://geohash.org/${hash}中进行直接定位，它是 geohash 的标准编码 值。 1234127.0.0.1:6379&gt; geohash company ireader1) &quot;wx4g52e1ce0&quot;127.0.0.1:6379&gt; geohash company juejin1) &quot;wx4gd94yjn0&quot;","link":"/2024/09/01/redis/GeoHash/"},{"title":"HyperLogLog","text":"HyperLogLog 是一种用于估计大数据集中不同元素数量（即基数）的算法。它特别适用于处理大规模数据集，能够在占用较少内存的情况下提供高效的基数估算。以下是 HyperLogLog 的主要特点和工作原理 应用场景 流量统计：网站或应用的独立访客数（UV）统计。 日志分析：分析日志文件中的唯一IP地址数量。 数据库系统：用于优化查询计划，估算表中的唯一值数量。 网络监控：监测网络流量中的唯一设备数量。 redis HyperLogLog 使用HyperLogLog 提供了两个指令 pfadd 和 pfcount，根据字面意义很好理解，一个是增加 计数，一个是获取计数。pfadd 用法和 set 集合的 sadd 是一样的，来一个用户 ID，就将用 户 ID 塞进去就是。pfcount 和 scard 用法是一样的，直接获取计数值。 12345678910127.0.0.1:6379&gt; pfadd codehole user1(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 1127.0.0.1:6379&gt; pfadd codehole user2(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 2127.0.0.1:6379&gt; pfadd codehole user3(integer) 1 HyperLogLog 实现原理伯努利实验与极大似然估计伯努利实验伯努利实验其实就是扔硬币，只有正反两面，且每一面朝上的概率都是50%。 场景1：我们随机扔一次硬币，那么得到正面或反面的可能性是相同的。如果我们扔10000次硬币，那么可以估计到大概率是接近5000次正面，5000次反面。这是最简单的正向推测。 场景2：如果我们扔2次硬币，是否可能2次都是正面？当然有可能，并且概率为1/4。如果我们扔10次硬币呢，是否可能10次都是正面？虽然概率很小，但依然是有可能的，概率为1/1024。同样的，无论是100次、1000次，即使概率很小，也依然存在全部都是正面朝上的情况，假如扔了n次，那么n次都是正面的概率为\\frac{1}{2^n} 场景3：现在我们按下面这种规则扔硬币：不断扔硬币，如果是正面朝上，那么就继续扔，直到出现反面朝上，此时记录下扔硬币的总次数。例如我们抛了5次硬币，前4次都是正面朝上，第5次是反面朝上，我们就记录下次数5。通过场景2，我们可以知道这种情况发生的概率为1/32。按我们的直觉可以推测，如果一个结果发生的概率是1/32，那么我们大体上就需要做32次同样的事情才能得到这个结果（当然从更严谨的数学角度，并不能这么说，但本文不想涉及专业的数学描述，所以姑且这么理解，其实也挺符合一般常识判断的） 那么假如张三做了若干次这种实验，我观察结果，发现记录下的总次数的最大值是5，那就说明在这若干次实验中，至少发生了一次4次正面朝上，第5次反面朝上的情况，而这种情况发生的概率是1/32，于是我推测，张三大概率总共做了32次实验。这就是一种反向推测：即根据结果（发生了一次1/32概率才会出现的结果），推测条件（大概率做了32次实验）。更通俗来说，如果一个结果出现的概率很小，但却实际发生了了，就可以推测这件事情被重复执行了很多次。结果出现的概率越小，事情被重复执行的次数就应当越多。 伯努利试验是数学概率论中的一部分内容，它的典故来源于抛硬币硬币拥有正反两面，一次的上抛至落下，最终出现正反面的概率都是50%。假设一直抛硬币，直到它出现正面为止，我们记录为一次完整的试验，间中可能抛了一次就出现了正面，也可能抛了4次才出现正面。无论抛了多少次，只要出现了正面，就记录为一次试验。这个试验就是伯努利试验。 那么对于多次的伯努利试验，假设这个多次为n次。就意味着出现了n次的正面。假设每次伯努利试验所经历了的抛掷次数为k。第一次伯努利试验，次数设为k1，以此类推，第n次对应的是kn。 其中，对于这n次伯努利试验中，必然会有一个最大的抛掷次数k，例如抛了12次才出现正面，那么称这个为k_max，代表抛了最多的次数。 伯努利试验容易得出有以下结论： n 次伯努利过程的投掷次数都不大于 k_max。 n 次伯努利过程，至少有一次投掷次数等于 k_max 最终结合极大似然估算的方法，发现在n和k_max中存在估算关联：n = 2^{k_max} 。这种通过局部信息预估整体数据流特性的方法似乎有些超出我们的基本认知，需要用概率和统计的方法才能推导和验证这种关联关系。 第一次试验: 抛了3次才出现正面，此时 k=3，n=1第二次试验: 抛了2次才出现正面，此时 k=2，n=2第三次试验: 抛了6次才出现正面，此时 k=6，n=3第n 次试验：抛了12次才出现正面，此时我们估算，n = 2^{12} 假设上面例子中实验组数共3组，那么 k_max = 6，最终 n=3，我们放进估算公式中去，明显： 3 ≠ 2^6 。也即是说，当试验次数很小的时候，这种估算方法的误差是很大的。 估算的优化如进行 100 轮或者更多轮次的试验，然后再取每轮的 k_max，再取平均数，即: k_mx/100。最终再估算出 n。下面是LogLog的估算公式： 面公式的DVLL对应的就是n，constant是修正因子，它的具体值是不定的，可以根据实际情况而分支设置。m代表的是试验的轮数。头上有一横的R就是平均数：(k_max_1 + … + k_max_m)/m。 这种通过增加试验轮次，再取k_max平均数的算法优化就是LogLog的做法。而 HyperLogLog和LogLog的区别就是，它采用的不是平均数，而是调和平均数.调和平均数比平均数的好处就是不容易受到大的数值的影响 求平均工资:A的是1000/月，B的30000/月。采用平均数的方式就是： (1000 + 30000) / 2 = 15500采用调和平均数的方式就是： 2/(1/1000 + 1/30000) ≈ 1935.484 redis中的具体数据结构redis使用了12kb的存储空间来存储hyperloglog的结果，那这12kb是如何具体分配的呢？接下去就来讨论这个部分。 比特串通过hash函数，将数据转为比特串，例如输入5，便转为：101。为什么要这样转化呢？ 是因为要和抛硬币对应上，比特串中，0 代表了反面，1 代表了正面，如果一个数据最终被转化了 10010000，那么从右往左，从低位往高位看，我们可以认为，首次出现 1 的时候，就是正面。 那么基于上面的估算结论，我们可以通过多次抛硬币实验的最大抛到正面的次数来预估总共进行了多少次实验，同样也就可以根据存入数据中，转化后的出现了 1 的最大的位置 k_max 来估算存入了多少数据。 redis的分桶要使用极大似然估计，需要可观察的结果足够多，但这个“足够多”其实并没有严谨的规定，和100比1万也挺多了，但和100万比较又显得少了，况且观察结果再多，误差总是有的，一些极端情况也是有可能发生的（就像有的人可能买一次彩票就中奖了，有的人可能买一辈子也没有中过）。为了减小这种误差，redis将统计结果分散到了总计16384个桶中，在最终计算总的结果的时候，再将这每一个桶的统计结果再做一次调和平均，使得各种极端情况的影响降到最低。 分桶就是分多少轮。抽象到计算机存储中去，就是存储的是一个以单位是比特(bit)，长度为 L 的大数组 S ，将 S 平均分为 m 组，注意这个 m 组，就是对应多少轮，然后每组所占有的比特个数是平均的，设为 P。容易得出下面的关系： L = S.length L = m * p 以 K 为单位，S 占用的内存 = L / 8 / 1024 在 Redis 中，HyperLogLog设置为：m=16834，p=6，L=16834 * 6。占用内存为=16834 * 6 / 8 / 1024 = 12K 数据存储结构redis采用的hash算法能得到一个64bit的结果，前面讲到redis进行了分桶，于是为了确定这个hash的结果需要放到哪个桶中，就需要拿出14个bit来计算桶的序号，2的14次方正好是16384。确定好放入哪个桶后，剩下的50个bit就作为扔硬币的实验结果，而最坏的实验结果是最左边的bit为1，其他bit都为0：10000….0000，此时我们需要记录的可能的最大数字就是50（即第一个为1的bit出现在第50位），而50的二进制是110010，需要6个bit存放。因此对于任意的hash结果，一个桶最多最多只需要6个bit就能存放下所有可能结果了redis总共分了16384个桶，每个桶需要6bit，于是总计：16384×6+8+1024= 12kb 稀疏结构与密集结构当redis刚创建完一个hyperloglog结构的时候，其中的所有bit都为0。为了避免重复数据对存储空间的浪费，redis使用了几种特殊的数据结构来表示重复数据： ZERO : 一字节，表示连续多少个桶计数为0，前两位为标志00，后6位表示有多少个桶，最大为64。XZERO : 两个字节，表示连续多少个桶计数为0，前两位为标志01，后14位表示有多少个桶，最大为16384VAL : 一字节，表示连续多少个桶的计数为多少，前一位为标志1，四位表示连桶内计数，所以最大表示桶的计数为32。后两位表示连续多少个桶。（ZERO和XZERO的区别在于如果连续为0的桶数量小于64个的时候，就没必要用14个bit来表示数量，进一步节约空间） 当redis创建完一个新的hyperloglog结构时，因为其中的所有bit都为0，所以并不需要实际使用12kb的空间存放16384个0，而是用2个字节的XZERO来表示： 经过用户的少数几次访问后，redis可能用如下结构存储： 总结 设 APP 主页的 key 为： main用户 id 为：idn , n-&gt;0,1,2,3…. 在这个统计问题中，不同的用户 id 标识了一个用户，那么我们可以把用户的 id 作为被hash的输入。即： hash(id) = 比特串 不同的用户 id，必然拥有不同的比特串。每一个比特串，也必然会至少出现一次 1 的位置。我们类比每一个比特串为一次伯努利试验。 现在要分轮，也就是分桶。所以我们可以设定，每个比特串的前多少位转为10进制后，其值就对应于所在桶的标号。假设比特串的低两位用来计算桶下标志，此时有一个用户的id的比特串是：1001011000011。它的所在桶下标为：11(2) = 12^1 + 12^0 = 3，处于第3个桶，即第3轮中。 上面例子中，计算出桶号后，剩下的比特串是：10010110000，从低位到高位看，第一次出现 1 的位置是 5 。也就是说，此时第3个桶，第3轮的试验中，k_max = 5。5 对应的二进制是：101，又因为每个桶有 p 个比特位。当 p&gt;=3 时，便可以将 101 存进去。 模仿上面的流程，多个不同的用户 id，就被分散到不同的桶中去了，且每个桶有其 k_max。然后当要统计出 mian 页面有多少用户点击量的时候，就是一次估算。最终结合所有桶中的 k_max，代入估算公式，便能得出估算值。","link":"/2024/09/01/redis/HyperLogLog/"},{"title":"PubSub 消息多播","text":"Redis 发布订阅 (pub/sub) 是一种消息通信模式：发送者 (pub) 发送消息，订阅者 (sub) 接收消息。 Redis 客户端可以订阅任意数量的频道。 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系 当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端： 123456789101112131415# Terminal 1：订阅频道127.0.0.1:6379&gt; SUBSCRIBE channel1Reading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;channel1&quot;3) (integer) 1# Terminal 2：发布消息127.0.0.1:6379&gt; PUBLISH channel1 &quot;Hello, subscribers!&quot;(integer) 1# Terminal 1：接收到消息1) &quot;message&quot;2) &quot;channel1&quot;3) &quot;Hello, subscribers!&quot; PubSub 的生产者传递过来一个消息，Redis 会直接找到相应的消费者传递过去。如果一 个消费者都没有，那么消息直接丢弃。如果开始有三个消费者，一个消费者突然挂掉了，生 产者会继续发送消息，另外两个消费者可以持续收到消息。但是挂掉的消费者重新连上的时 候，这断连期间生产者发送的消息，对于这个消费者来说就是彻底丢失了。如果 Redis 停机重启，PubSub 的消息是不会持久化的，毕竟 Redis 宕机就相当于一个 消费者都没有，所有的消息直接被丢弃。","link":"/2024/09/01/redis/PubSub%20%E6%B6%88%E6%81%AF%E5%A4%9A%E6%92%AD%20(%E7%BC%BA%E7%82%B9%E5%A4%9A%E4%B8%8D%E8%A2%AB%E4%BD%BF%E7%94%A8)/"},{"title":"Scan指令","text":"SCAN 指令是 Redis 提供的一个用于迭代数据库键的命令。 1SCAN cursor [MATCH pattern] [COUNT count] [TYPE type] cursor: 迭代器游标，初始值为 0，每次调用 SCAN 后会返回一个新的游标值，直到返回 0 表示迭代结束。 MATCH pattern: 可选参数，用于匹配特定模式的键，例如 MATCH user:* 会匹配所有以 user: 开头的键。 COUNT count: 可选参数，用于指定每次迭代返回的大致键数量，默认值为 10。 TYPE type: 可选参数，用于指定返回的键的类型，例如 TYPE string 会返回所有字符串类型的键。 SCAN 命令返回一个包含两个元素的数组： 第一个元素是下一个游标的值。第二个元素是一个包含当前迭代结果的数组。 SCAN 命令，以及其他增量式迭代命令， 在进行完整遍历的情况下可以为用户带来以下保证：从完整遍历开始直到完整遍历结束期间，一直存在于数据集内的所有元素都会被完整遍历返回；这意味着，如果有一个元素，它从遍历开始直到遍历结束期间都存在于被遍历的数据集当中，那么 SCAN 命令总会在某次迭代中将这个元素返回给用户。 因为增量式命令仅仅使用游标来记录迭代状态，所以这些命令带有以下缺点： 同一个元素可能会被返回多次。处理重复元素的工作交由应用程序负责，比如说， 可以考虑将迭代返回的元素仅仅用于可以安全地重复执行多次的操作上。 如果一个元素是在迭代过程中被添加到数据集的，又或者是在迭代过程中从数据集中被删除的， 那么这个元素可能会被返回，也可能不会， 这是未定义的（undefined）。 Scan遍历顺序如果不考虑扩容与缩容,那么无论是从前遍历还是从后遍历都可以获取所有的key值,但是有扩容,缩容后就需要考虑遍历的准确性,是否存在重复遍历,是否存在遗漏的遍历.如果我们按照低位加法,即从前向后遍历,当扩容或者缩容时进行的rehash操作使得数据分散到不同的槽位,这就有可能发生重复遍历与遗漏遍历的情况.","link":"/2024/09/01/redis/Scan%E6%8C%87%E4%BB%A4/"},{"title":"Stream","text":"Redis Stream 主要用于消息队列（MQ，Message Queue），Redis 本身是有一个 Redis 发布订阅 (pub/sub) 来实现消息队列的功能，但它有个缺点就是消息无法持久化，如果出现网络断开、Redis 宕机等，消息就会被丢弃。 简单来说发布订阅 (pub/sub) 可以分发消息，但无法记录历史消息。 而 *Redis Stream 提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失。 每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用 xadd 指令追加消息时自动创建。 Consumer Group ：消费组，使用 XGROUP CREATE 命令创建，一个消费组有多个消费者(Consumer)。 last_delivered_id ：游标，每个消费组会有个游标 last_delivered_id，任意一个消费者读取了消息都会使游标 last_delivered_id 往前移动。 pending_ids ：消费者(Consumer)的状态变量，作用是维护消费者的未确认的 id。 pending_ids 记录了当前已经被客户端读取的消息，但是还没有 ack (Acknowledge character：确认字符）。 Redis Stream角色关系 每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用 xadd 指令追加消 息时自动创建。 每个 Stream 都可以有多个消费组，每个消费组会有个游标last_delivered_id 在 Stream 数组之上往前移动，表示当前消费组已经消费到哪条消息了。每个消费组都有一个 Stream 内唯一的名称，消费组不会自动创建，它需要单独的指令 xgroup create 进行创建，需要指定 从 Stream 的某个消息 ID 开始消费，这个 ID 用来初始化 last_delivered_id 变量。 每个消费组 (Consumer Group) 的状态都是独立的，相互不受影响。也就是说同一份 Stream 内部的消息会被每个消费组都消费到。 同一个消费组 (Consumer Group) 可以挂接多个消费者 (Consumer)，这些消费者之间是竞争关系，任意一个消费者读取了消息都会使游标 last_delivered_id 往前移动。每个消费者有一个组内唯一名称 消费者 (Consumer) 内部会有个状态变量 pending_ids，它记录了当前已经被客户端读取 的消息，但是还没有 ack。如果客户端没有 ack，这个变量里面的消息 ID 会越来越多，一 旦某个消息被 ack，它就开始减少。这个 pending_ids 变量在 Redis 官方被称之为 PEL，也就是 Pending Entries List，这是一个很核心的数据结构，它用来确保客户端至少消费了消息一次，而不会在网络传输的中途丢失了没处理。 消息队列相关命令：XADD - 添加消息到末尾 XTRIM - 对流进行修剪，限制长度 XDEL - 删除消息 XLEN - 获取流包含的元素数量，即消息长度 XRANGE - 获取消息列表，会自动过滤已经删除的消息 XREVRANGE - 反向获取消息列表，ID 从大到小 XREAD - 以阻塞或非阻塞方式获取消息列表 消费者组相关命令：XGROUP CREATE - 创建消费者组 XREADGROUP GROUP - 读取消费者组中的消息 XACK - 将消息标记为&quot;已处理&quot; XGROUP SETID - 为消费者组设置新的最后递送消息ID XGROUP DELCONSUMER - 删除消费者 XGROUP DESTROY - 删除消费者组 XPENDING - 显示待处理消息的相关信息 XCLAIM - 转移消息的归属权 XINFO - 查看流和消费者组的相关信息； XINFO GROUPS - 打印消费者组的信息； XINFO STREAM - 打印流信息 独立消费Redis 设计了一个单独的消费指令 xread，可以将 Stream 当成普 通的消息队列 (list) 来使用。使用 xread 时，我们可以完全忽略消费组 (Consumer Group) 的存在，就好比 Stream 就是一个普通的列表 (list)。 客户端如果想要使用 xread 进行顺序消费，一定要记住当前消费到哪里了，也就是返回 的消息 ID。下次继续调用 xread 时，将上次返回的最后一个消息 ID 作为参数传递进去， 就可以继续消费后续的消息。block 0 表示永远阻塞，直到消息到来，block 1000 表示阻塞 1s，如果 1s 内没有任何 消息到来，就返回 nil。 消费Stream 提供了 xreadgroup 指令可以进行消费组的组内消费，需要提供消费组名称、消 费者名称和起始消息 ID。它同 xread 一样，也可以阻塞等待新消息。读到新消息后，对应的消息 ID 就会进入消费者的 PEL(正在处理的消息) 结构里，客户端处理完毕后使用 xack 指令通知服务器，本条消息已经处理完毕，该消息 ID 就会从 PEL 中移除。 Stream 消息太多怎么办?读者很容易想到，要是消息积累太多，Stream 的链表岂不是很长，内容会不会爆掉?xdel 指令又不会删除消息，它只是给消息做了个标志位。Redis 自然考虑到了这一点，所以它提供了一个定长 Stream 功能。在 xadd 的指令提供 一个定长长度 maxlen，就可以将老的消息干掉，确保最多不超过指定长度。 消息如果忘记 ACK 会怎样?Stream 在每个消费者结构中保存了正在处理中的消息 ID 列表 PEL，如果消费者收到了消息处理完了但是没有回复 ack，就会导致 PEL 列表不断增长，如果有很多消费组的话，那么这个 PEL 占用的内存就会放大。 PEL 如何避免消息丢失?在客户端消费者读取 Stream 消息时，Redis 服务器将消息回复给客户端的过程中，客户端突然断开了连接，消息就丢失了。但是 PEL 里已经保存了发出去的消息 ID。待客户端 重新连上之后，可以再次收到 PEL 中的消息 ID 列表。不过此时 xreadgroup 的起始消息ID不能为参数&gt;，而必须是任意有效的消息 ID，一般将参数设为 0-0，表示读取所有的 PEL 消息以及自 last_delivered_id 之后的新消息","link":"/2024/09/01/redis/Stream/"},{"title":"codis 集群方案","text":"https://github.com/CodisLabs/codis","link":"/2024/09/01/redis/codis%20%E9%9B%86%E7%BE%A4%E6%96%B9%E6%A1%88/"},{"title":"redis list应用——延迟队列","text":"异步消息队列Redis 的 list(列表) 数据结构常用来作为异步消息队列使用，使用rpush/lpush 操作入队列， 使用 lpop 和 rpop 来出队列。 客户端是通过队列的 pop 操作来获取消息，然后进行处理。处理完了再接着获取消息， 再进行处理。如此循环往复。 可是如果队列空了，客户端就会陷入 pop 的死循环，不停地 pop，没有数据，接着再 pop， 又没有数据。这就是浪费生命的空轮询。空轮询不但拉高了客户端的 CPU，redis 的 QPS 也会被拉高，如果这样空轮询的客户端有几十来个，Redis 的慢查询可能会显著增多。通常我们使用 sleep 来解决这个问题，让线程睡一会，睡个 1s 钟就可以了。不但客户端 的 CPU 能降下来，Redis 的 QPS 也降下来了。 blpop/brpop 前缀字符 b 代表的是 blocking，也就是阻塞读。阻塞读在队列没有数据的时候，会立即进入休眠状态，一旦数据到来，则立刻醒过来。消 息的延迟几乎为零。用 blpop/brpop 替代前面的 lpop/rpop，就完美解决了队列空了。… 延迟队列延时队列可以通过 Redis 的 zset(有序列表) 来实现。我们将消息序列化成一个字符串作 为 zset 的 value，这个消息的到期处理时间作为 score，然后用多个线程轮询 zset 获取到期 的任务进行处理，多个线程是为了保障可用性，万一挂了一个线程还有其它线程可以继续处 理。因为有多个线程，所以需要考虑并发争抢任务，确保任务不能被多次执行。","link":"/2024/09/01/redis/redis%20list%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94%E5%BB%B6%E8%BF%9F%E9%98%9F%E5%88%97/"},{"title":"redis 分布式锁","text":"setnx lock:codehole trueOK… do something critical …del lock:codehole 逻辑执行到中间出现异常了，可能会导致 del 指令没有被调用，这样 就会陷入死锁，锁永远得不到释放。于是我们在拿到锁之后，再给锁加上一个过期时间，比如 5s，这样即使中间出现异常也 可以保证 5 秒之后锁会自动释放。 setnx lock:codehole trueOKexpire lock:codehole 5… do something critical …del lock:codehole (integer) 1 如果在 setnx 和 expire 之间服务器进程突然挂掉了，可能是因 为机器掉电或者是被人为杀掉的，就会导致 expire 得不到执行，也会造成死锁。 Redis 2.8 版本中作者加入了 set 指令的扩展参数，使得 setnx 和 expire 指令可以一起执行。 set lock:codehole true ex 5 nx OK… do something critical …del lock:codehole上面这个指令就是 setnx 和 expire 组合在一起的原子指令。 超时问题如果在加锁和释放锁之间的逻辑执行的太长，以至于超出了锁的超时限制，就会出现问题。因为这时候锁过期了，第二个线程重新持有了这把锁， 但是紧接着第一个线程执行完了业务逻辑，就把锁给释放了，第三个线程就会在第二个线程逻辑执行完之间拿到了锁。 有一个更加安全的方案是为 set 指令的 value 参数设置为一个随机数，释放锁时先匹配 随机数是否一致，然后再删除 key。但是匹配 value 和删除 key 不是一个原子操作，Redis 也 没有提供类似于 delifequals 这样的指令，这就需要使用 Lua 脚本来处理了，因为 Lua 脚本可以保证连续多个指令的原子性执行。","link":"/2024/09/01/redis/redis%20%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"},{"title":"redis 事务","text":"Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。 事务相关指令 Redis事务相关命令和使用MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。 MULTI ：开启事务，redis会将后续的命令逐个放入队列中，然后使用EXEC命令来原子化执行这个命令系列。 EXEC：执行事务中的所有操作命令。 DISCARD：取消事务，放弃执行事务块中的所有命令。 WATCH：监视一个或多个key,如果事务在执行前，这个key(或多个key)被其他命令修改，则事务被中断，不会执行事务中的任何命令。 UNWATCH：取消WATCH对所有key的监视。 所有的指令在 exec 之前不执行，而是缓存在 服务器的一个事务队列中，服务器一旦收到 exec 指令，才开执行整个事务队列，执行完毕 后一次性返回所有指令的运行结果 Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。 事务使用示例 执行事务 取消事务 在事务队列中存在语法性错误，则执行EXEC命令时，所有命令都不会执行 在事务队列中存在指令执行错误，则执行EXEC命令时，其他正确命令会被执行，错误命令抛出异常。 watch 使用watch检测balance，事务期间balance数据未变动，事务执行成功。 使用watch检测balance，在开启事务后（标注1处），在新窗口执行标注2中的操作，更改balance的值，模拟其他客户端在事务执行期间更改watch监控的数据，然后再执行标注1后命令，执行EXEC后，事务未成功执行。 为什么有 WATCH 命令在redis的事务中， 无法使用get操作获取value，get操作往往放在事务外面，为保证并发问题，需要watch命令监控get的key不被更改","link":"/2024/09/01/redis/redis%20%E4%BA%8B%E5%8A%A1/"},{"title":"redis 位图","text":"位图不是特殊的数据结构，它的内容其实就是普通的字符串，也就是 byte 数组。我们 可以使用普通的 get/set 直接获取和设置整个位图的内容，也可以使用位图操作 getbit/setbit 等将 byte 数组看成「位数组」来处理。 Redis 的位数组是自动扩展，如果设置了某个偏移位置超出了现有的内容范围，就会自 动将位数组进行零扩充。 12345678910111213141516171819127.0.0.1:6379&gt; setbit s 1 1 (integer) 0 127.0.0.1:6379&gt; setbit s 2 1 (integer) 0 127.0.0.1:6379&gt; setbit s 4 1 (integer) 0 127.0.0.1:6379&gt; setbit s 9 1 (integer) 0 127.0.0.1:6379&gt; setbit s 10 1 (integer) 0 127.0.0.1:6379&gt; setbit s 13 1 (integer) 0 127.0.0.1:6379&gt; setbit s 15 1 (integer) 0 127.0.0.1:6379&gt; get s &quot;he&quot;127.0.0.1:6379&gt; set w h # 整存 (integer) 0127.0.0.1:6379&gt; getbit w 1(integer) 1127.0.0.1:6379&gt; getbit w 2(integer) 1127.0.0.1:6379&gt; getbit w 4 (integer) 1127.0.0.1:6379&gt; getbit w 5 (integer) 0 统计和查找Redis 提供了位图统计指令 bitcount 和位图查找指令 bitpos，bitcount 用来统计指定位置范围内 1 的个数，bitpos 用来查找指定范围内出现的第一个 0 或 1。比如我们可以通过 bitcount 统计用户一共签到了多少天，通过 bitpos 指令查找用户从哪一天开始第一次签到。如果指定了范围参数[start, end]，就可以统计在某个时间范围内用户签到了多少天，用户自某天以后的哪天开始签到。遗憾的是， start 和 end 参数是字节索引，也就是说指定的位范围必须是 8 的倍数， 而不能任意指定。 12345678910111213127.0.0.1:6379&gt; set w helloOK127.0.0.1:6379&gt; bitcount w(integer) 21127.0.0.1:6379&gt; bitcount w 0 0 # 第一个字符中 1 的位数 (integer) 3127.0.0.1:6379&gt; bitcount w 0 1 # 前两个字符中 1 的位数 (integer) 7127.0.0.1:6379&gt; bitpos w 0 # 第一个 0 位(integer) 0127.0.0.1:6379&gt; bitpos w 1 # 第一个 1 位(integer) 1127.0.0.1:6379&gt; bitpos w 1 1 1 # 从第二个字符算起，第一个 1 位 (integer) 9127.0.0.1:6379&gt; bitpos w 1 2 2 # 从第三个字符算起，第一个 1 位 (integer) 17 大规模标签查找","link":"/2024/09/01/redis/redis%20%E4%BD%8D%E5%9B%BE/"},{"title":"redis 高可用","text":"Sentinel(哨兵)Redis Sentinel是社区版本推出的原生高可用解决方案，其部署架构主要包括两部分：Redis Sentinel集群和Redis数据集群。 其中Redis Sentinel集群是由若干Sentinel节点组成的分布式集群，可以实现故障发现、故障自动转移、配置中心和客户端通知。Redis Sentinel的节点数量要满足2n+1（n&gt;=1）的奇数个。 优点： Redis Sentinel集群部署简单； 能够解决Redis主从模式下的高可用切换问题； 很方便实现Redis数据节点的线形扩展，轻松突破Redis自身单线程瓶颈，可极大满足Redis大容量或高性能的业务需求； 可以实现一套Sentinel监控一组Redis数据节点或多组数据节点。 缺点： 部署相对Redis主从模式要复杂一些，原理理解更繁琐； 资源浪费，Redis数据节点中slave节点作为备份节点不提供服务； Redis Sentinel主要是针对Redis数据节点中的主节点的高可用切换，对Redis的数据节点做失败判定分为主观下线和客观下线两种，对于Redis的从节点有对节点做主观下线操作，并不执行故障转移。 不能解决读写分离问题，实现起来相对复杂。 建议： 如果监控同一业务，可以选择一套Sentinel集群监控多组Redis数据节点的方案，反之选择一套Sentinel监控一组Redis数据节点的方案。 sentinel monitor 配置中的建议设置成Sentinel节点的一半加1，当Sentinel部署在多个IDC的时候，单个IDC部署的Sentinel数量不建议超过（Sentinel数量 – quorum）。 合理设置参数，防止误切，控制切换灵敏度控制： a. quorum b. down-after-milliseconds 30000 c. failover-timeout 180000 d. maxclient e. timeout 部署的各个节点服务器时间尽量要同步，否则日志的时序性会混乱。 Redis建议使用pipeline和multi-keys操作，减少RTT次数，提高请求效率。 自行搞定配置中心（zookeeper），方便客户端对实例的链接访问。 Redis ClusterRedis Cluster是社区版推出的Redis分布式集群解决方案，主要解决Redis分布式方面的需求，比如，当遇到单机内存，并发和流量等瓶颈的时候，Redis Cluster能起到很好的负载均衡的目的。 Redis Cluster集群节点最小配置6个节点以上（3主3从），其中主节点提供读写操作，从节点作为备用节点，不提供请求，只作为故障转移使用。 Redis Cluster采用虚拟槽分区，所有的键根据哈希函数映射到0～16383个整数槽内，每个节点负责维护一部分槽以及槽所印映射的键值数据。 优点： 无中心架构； 数据按照slot存储分布在多个节点，节点间数据共享，可动态调整数据分布； 可扩展性：可线性扩展到1000多个节点，节点可动态添加或删除； 高可用性：部分节点不可用时，集群仍可用。通过增加Slave做standby数据副本，能够实现故障自动failover，节点之间通过gossip协议交换状态信息，用投票机制完成Slave到Master的角色提升； 降低运维成本，提高系统的扩展性和可用性。 缺点： Client实现复杂，驱动要求实现Smart Client，缓存slots mapping信息并及时更新，提高了开发难度，客户端的不成熟影响业务的稳定性。目前仅JedisCluster相对成熟，异常处理部分还不完善，比如常见的“max redirect exception”。 节点会因为某些原因发生阻塞（阻塞时间大于clutser-node-timeout），被判断下线，这种failover是没有必要的。 数据通过异步复制，不保证数据的强一致性。 多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况。 Slave在集群中充当“冷备”，不能缓解读压力，当然可以通过SDK的合理设计来提高Slave资源的利用率。 Key批量操作限制，如使用mset、mget目前只支持具有相同slot值的Key执行批量操作。对于映射为不同slot值的Key由于Keys不支持跨slot查询，所以执行mset、mget、sunion等操作支持不友好。 Key事务操作支持有限，只支持多key在同一节点上的事务操作，当多个Key分布于不同的节点上时无法使用事务功能。 Key作为数据分区的最小粒度，不能将一个很大的键值对象如hash、list等映射到不同的节点。 不支持多数据库空间，单机下的redis可以支持到16个数据库，集群模式下只能使用1个数据库空间，即db 0。 复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构。 避免产生hot-key，导致主库节点成为系统的短板。 避免产生big-key，导致网卡撑爆、慢查询等。 重试时间应该大于cluster-node-time时间。 Redis Cluster不建议使用pipeline和multi-keys操作，减少max redirect产生的场景。","link":"/2024/09/01/redis/redis%20%E9%AB%98%E5%8F%AF%E7%94%A8/"},{"title":"主从同步","text":"全量同步主从第一次连接时会执行全量同步，将master节点所有的数据拷贝给slave节点 主从节点复制偏移量 参与复制的主从节点都会维护自身的复制偏移量。 主节点在处理完写入命令后，会把命令的字节长度做累加记录，统计信息在info replication中的master_repl_offset指标中。 从节点每秒钟上报自身的的复制偏移量（slave_repl_offset）给主节点，主节点会保存从节点的复制偏移量。 从节点在接收到主节点发送的命令后，会累加自身的偏移量，统计信息在info replication中的slave_repl_offset指标中。 通过对比主从节点的复制偏移量，可以判断主从节点数据是否一致。 增量同步 Redis 同步的是指令流，主节点会将那些对自己的状态产生修改性影响的指令记录在本 地的内存 buffer 中，然后异步将 buffer 中的指令同步到从节点，从节点一边执行同步的指 令流来达到和主节点一样的状态，一遍向主节点反馈自己同步到哪里了 (偏移量)。因为内存的 buffer 是有限的，所以 Redis 主库不能将所有的指令都记录在内存 buffer 中。Redis 的复制内存 buffer 是一个定长的环形数组，如果数组内容满了，就会从头开始覆盖前面的内容。 如果因为网络状况不好，从节点在短时间内无法和主节点进行同步，那么当网络状况恢 复时，Redis 的主节点中那些没有同步的指令在 buffer 中有可能已经被后续的指令覆盖掉 了，从节点将无法直接通过指令流来进行同步，这个时候就需要用到更加复杂的同步机制 — — 快照同步。","link":"/2024/09/01/redis/%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"},{"title":"ziplist (压缩列表)","text":"ziplist 是一个经过特殊编码的双向链表，旨在提高内存效率。 它存储字符串和整数值，其中整数被编码为实际整数而不是一系列字符。 它允许在 O(1) 时间内在列表的任一侧进行推送和弹出操作。 但是，由于每个操作都需要重新分配 ziplist 使用的内存，因此实际复杂性与 ziplist 使用的内存量有关。 ziplist 为了节省内存，采用了紧凑的连续存储。所以在修改操作下并不能像一般的链表那么容易，需要从新分配新的内存，然后复制到新的空间。 ziplist 是一个双向链表，可以在时间复杂度为 O(1) 从下头部、尾部进行 pop 或 push。 新增或更新元素可能会出现连锁更新现象。 不能保存过多的元素，否则查询效率就会降低。 特殊双向链表ziplist 是一个特殊双向链表，不像普通的链表使用前后指针关联在一起，它是存储在连续内存上的. zlbytes: 32 位无符号整型，记录 ziplist 整个结构体的占用空间大小。当然了也包括 zlbytes 本身。这个结构有个很大的用处，就是当需要修改 ziplist 时候不需要遍历即可知道其本身的大小。 这个 SDS 中记录字符串的长度有相似之处，这些好的设计往往在平时的开发中可以采纳一下。 zltail: 32 位无符号整型, 记录整个 ziplist 中最后一个 entry 的偏移量。所以在尾部进行 POP 操作时候不需要先遍历一次。 zllen: 16 位无符号整型, 记录 entry 的数量， 所以只能表示 2^16。但是 Redis 作了特殊的处理：当实体数超过 2^16 ,该值被固定为 2^16 - 1。 所以这种时候要知道所有实体的数量就必须要遍历整个结构了。 entry: 真正存数据的结构。 zlend: 8 位无符号整型, 固定为 255 (0xFF)。为 ziplist 的结束标识。 entry节点每个 entry 都包含两条信息的元数据为前缀： 第一元数据用来存储前一个 entry 的长度，以便能够从后向前遍历列表。 第二元数据是表示 entry 的编码形式。 用来表示 entry 类型，整数或字符串，在字符串的情况下，它还表示字符串有效的长度。 prelen 记录前一个 entry 的长度。若前一个 entry 的长度小于 254 , 则使用 1 个字节的 8 位无符号整数来表示。若前一个 entry 长度大于等于 254，则使用 5 个字节来表示。第 1 个字节固定为 254 (FE) 作为标识，剩余 4 字节则用来表示前一个 entry 的实际大小。 连锁更新ziplist 在更新或者新增时候，如空间不够则需要对整个列表进行重新分配。当新插入的元素较大时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起「连锁更新」问题，导致每个元素的空间都要重新分配，造成访问压缩列表性能的下降。 ziplist 节点的 prevlen 属性会根据前一个节点的长度进行不同的空间大小分配： 如果前一个节点的长度小于 254 字节，那么 prevlen 属性需要用 1 字节的空间来保存这个长度值。 如果前一个节点的长度大于等于 254 字节，那么 prevlen 属性需要用 5 字节的空间来保存这个长度值。","link":"/2024/09/01/redis/ziplist%20(%E5%8E%8B%E7%BC%A9%E5%88%97%E8%A1%A8)/"},{"title":"五种基本数据结构","text":"五种基本数据结构 String（字符串） * 特点：最简单的数据结构，可以存储字符串、整数或浮点数。支持原子操作，如递增、递减等。 * 应用场景： * 存储简单的键值对，如缓存数据、计数器等。 * 实现分布式锁。 * 计数器，如网站的访问次数统计。 List（列表） 特点：有序的字符串列表，支持从两端进行插入和删除操作。 应用场景： 消息队列，如生产者-消费者模型。 最近浏览记录，如用户最近查看的商品列表。 聊天应用的消息存储。 Set（集合） 特点：无序的、不重复的字符串集合，支持交集、并集、差集等操作。 应用场景： 去重，如统计某段时间内的唯一访客。 社交网络的好友关系管理。 标签系统，如文章的标签管理。 Hash（哈希） 特点：键值对的集合，适合存储对象。 应用场景： 存储对象属性，如用户信息（用户名、密码、邮箱等）。 会话管理，如存储用户的会话信息。 商品详情，如商品的价格、库存、描述等。 ZSet（有序集合） 特点：每个成员都有一个分数，集合中的成员按照分数排序，支持范围查询。 应用场景： 排行榜，如游戏得分排行榜。 时间轴，如微博的时间线。 优先级队列，如任务调度系统。","link":"/2024/09/01/redis/%E4%BA%94%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"title":"布隆过滤器","text":"布隆过滤器（Bloom Filter）是一种空间效率高的概率数据结构，用于测试一个元素是否属于一个集合。它允许少量的误判（即可能会错误地认为一个元素属于集合，但实际上不属于），但不会出现误判为不属于的情况。布隆过滤器广泛应用于缓存系统、数据库、网络路由等领域。 缓存系统：用于快速判断一个元素是否在缓存中。 数据库：用于索引和快速查找。 网络路由：用于快速判断一个 IP 地址是否在黑名单中。 垃圾邮件过滤：用于快速判断一封邮件是否为垃圾邮件 Redis 中的布隆过滤器使用Redis 官方提供的布隆过滤器到了 Redis 4.0 提供了插件功能之后才正式登场。布隆过滤 器作为一个插件加载到 Redis Server 中，给 Redis 提供了强大的布隆去重功能。 布隆过滤器有二个基本指令，bf.add 添加元素，bf.exists 查询元素是否存在，它的用法 和 set 集合的 sadd 和 sismember 差不多。注意 bf.add 只能一次添加一个元素，如果想要 一次添加多个，就需要用到 bf.madd 指令。同样如果需要一次查询多个元素是否存在，就需 要用到 bf.mexists 指令。 12345678910111213127.0.0.1:6379&gt; bf.add codehole user1(integer) 1127.0.0.1:6379&gt; bf.add codehole user2(integer) 1127.0.0.1:6379&gt; bf.add codehole user3(integer) 1127.0.0.1:6379&gt; bf.exists codehole user1(integer) 1127.0.0.1:6379&gt; bf.exists codehole user2(integer) 1127.0.0.1:6379&gt; bf.exists codehole user3(integer) 1127.0.0.1:6379&gt; bf.exists codehole user4 Redis 其实还提供了自定义参数的布隆过滤器，需要我们在 add 之前使用 bf.reserve 指令显式创建。如果对应的 key 已经存在，bf.reserve 会报错。bf.reserve 有三个参数，分别是 key, error_rate 和 initial_size。错误率越低，需要的空间越大。initial_size 参数表示预计放 入的元素数量，当实际数量超出这个数值时，误判率会上升。所以需要提前设置一个较大的数值避免超出导致误判率升高。如果不使用 bf.reserve，默 认的 error_rate 是 0.01，默认的 initial_size 是 100。 布隆过滤器原理下图表示向布隆过滤器中添加元素 www.123.com 和 www.456.com 的过程，它使用了 func1 和 func2 两个简单的哈希函数。 初始化：当我们创建一个布隆过滤器时，我们首先创建一个全由0组成的位数组（bit array)。同时，我们还需选择几个独立的哈希函数，每个函数都可以将集合中的元素映射到这个位数组的某个位置。 添加元素：在布隆过滤器中添加一个元素时，我们会将此元素通过所有的哈希函数进行映射，得到在位数组中的几个位置，然后将这些位置标记为1。 查询元素：如果我们要检查一个元素是否在集合中，我们同样使用这些哈希函数将元素映射到位数组中的几个位置，如果所有的位置都被标记为1，那么我们就可以说该元素可能在集合中。如果有任何一个位置不为1，那么该元素肯定不在集合中。 我们可以提高数组长度以及 hash 计算次数来降低误报率，但是相应的 CPU、内存的消耗也会相应地提高，会增加存储和计算的开销。因此，布隆过滤器的使用需要在误判率和性能之间进行权衡。","link":"/2024/09/01/redis/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"title":"过期策略与内存淘汰","text":"redis 会将每个设置了过期时间的 key 放入到一个独立的字典中，以后会定时遍历这个 字典来删除到期的 key。 过期策略惰性删除 惰性策略就是在客户端访问这个 key 的时候，redis 对 key 的过期时间进行检查，如果过期了就立即删除。 定时扫描Redis 默认会每秒进行十次过期扫描，过期扫描不会遍历过期字典中所有的 key，而是 采用了一种简单的贪心策略。 从过期字典中随机 20 个 key； 删除这 20 个 key 中已经过期的 key； 如果过期的 key 比率超过 1/4，那就重复步骤 1； 同时，为了保证过期扫描不会出现循环过度，导致线程卡死现象，算法还增加了扫描时 间的上限，默认不会超过 25ms。 内存淘汰为了限制最大使用内存，Redis 提供了配置参数 maxmemory 来限制内存超出期望大小。当实际内存超出 maxmemory 时，Redis 提供了几种可选策略 (maxmemory-policy) 来让用户自己决定该如何腾出新的空间以继续提供读写服务。 noeviction 不会继续服务写请求 (DEL 请求可以继续服务)，读请求可以继续进行。这样 可以保证不会丢失数据，但是会让线上的业务不能持续进行。这是默认的淘汰策略。 volatile-lru 尝试淘汰设置了过期时间的 key，最少使用的 key 优先被淘汰。没有设置过 期时间的 key 不会被淘汰，这样可以保证需要持久化的数据不会突然丢失。 volatile-ttl 跟上面一样，除了淘汰的策略不是 LRU，而是 key 的剩余寿命 ttl 的值，ttl 越小越优先被淘汰。 volatile-random 跟上面一样，不过淘汰的 key 是过期 key 集合中随机的 key。 allkeys-lru 区别于 volatile-lru，这个策略要淘汰的 key 对象是全体的 key 集合，而不 只是过期的 key 集合。这意味着没有设置过期时间的 key 也会被淘汰。 allkeys-random 跟上面一样，不过淘汰的策略是随机的 key。 volatile-xxx 策略只会针对带过期时间的 key 进行淘汰，allkeys-xxx 策略会对所有的 key 进行淘汰。如果你只是拿 Redis 做缓存，那应该使用 allkeys-xxx，客户端写缓存时 不必携带过期时间。如果你还想同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略，这样可以保留没有设置过期时间的 key，它们是永久的 key 不会被 LRU 算法淘 汰。","link":"/2024/09/01/redis/%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5%E4%B8%8E%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0/"},{"title":"redis持久化","text":"RDB（Redis Database Backup） RDB 是一种快照持久化方式，它会在指定的时间间隔内将内存中的数据集快照写入磁盘。 优点 生成速度快：RDB文件是一个紧凑的二进制文件，生成速度快，对系统性能影响较小。 恢复速度快：由于RDB文件包含了某一时刻的完整数据集，恢复速度非常快。 缺点 数据丢失风险：如果Redis在两次生成RDB文件之间发生故障，这段时间内的数据将会丢失。 文件体积较大：RDB 文件会占用较多的磁盘空间。 CPU和I/O开销：生成RDB文件时，Redis需要进行大量数据的序列化和I/O操作，会对CPU和I/O资源造成一定的压力。 手动方式 save：save 命令会阻塞 Redis 服务器进程，直到 RDB 文件创建完毕为止，在服务器进程阻塞期间，服务器不能处理任何命令请求。 bgsave：bgsave 命令会 fork 一个子进程（注意是子进程，不是子线程）在后台生成快照文件，不会阻塞 Redis 服务器，服务器进程（父进程）可以继续处理命令请求。 bgsave命令执行期间，客户端发送的 save 和 bgsave 命令会被拒绝，这样的目的是为了防止父进程和子进程之间产生竞争。 自动方式自动方式是指通过服务器配置文件的 save 选项，来让 Redis 每隔一段时间自动执行 bgsave ，本质上还是通过 bgsave 命令去实现。 在 redis.conf 文件中，可以通过以下配置项来启用和调整 RDB 持久化： 12345678910# 是否开启 RDB 持久化 save 3600 1 （3600秒后，如果至少发生了1次写入，则存一次） save 300 100 （300秒后，如果至少发生了100次写入，则存一次） save 60 10000 （60秒后，如果至少发生了10000次写入，则存一次）# RDB 文件的名称dbfilename dump.rdb# RDB 文件的保存路径dir /var/lib/redis 注意1：二者要必须同时满足，即写入次数和发生时间。 比如距离上次写入过了59秒，在59秒内累积发生了9999次写入，那么在60秒的时候并不会发生写入操作，因为60秒的时候9999 &lt; 10000，等到了300秒后，因为9999 &gt; 100，条件都满足，所以会在距离上次写入300秒的时候发生一次写入db操作。 注意2：条件由上往下执行，如果上一个条件满足了，则不进行下个条件的判断了，直接存入db 注意3：配置原则是判断时间越长（第一个参数），写入次数越少（第二个参数），如果配置错误，导致频繁的存入db，可能会造成性能问题 注意4：这里是写入操作，并不是说改变次数，比如说 set a a; set a a;执行两次这个操作，算两次写入，虽然值没有变，但是也是会算在写入次数里面的。 AOF（Append Only File）AOF 是一种日志持久化方式，它会记录服务器接收到的每个写操作命令，并在服务器启动时通过重新执行这些命令来恢复数据。 优点 数据安全性高：AOF文件记录了每个写操作命令，即使Redis发生故障，也可以通过重放AOF文件中的命令来恢复数据，数据安全性高。 可读性好：AOF文件是以文本形式记录的，可以方便地进行查看和编辑。 缺点 文件体积大：AOF文件记录了每个写操作命令，文件体积通常比RDB文件大。 恢复速度慢：由于需要重放AOF文件中的所有命令，恢复速度比RDB慢。 当启用 AOF 时，Redis 发生写命令时其实并不是直接写入到AOF 文件，而是将写命令追加到AOF缓冲区的末尾，之后 AOF缓存区再同步至 AOF文件中。 AOF 缓存区同步至 AOF 文件，这一过程由名为 flushAppendonlyFile 的函数完成。 而 flushAppendOnlyFile 函数的行为由服务器配置文件的 appendfsync 选项来决定，该参数有以下三个选项： always：每次发生写命令，都同步到 AOF 文件，是最安全的选项。 everysec：每秒钟同步写入一次到 AOF 文件，在性能和安全之间做了一个平衡。 no：不主动写入 AOF 文件，何时同步由操作系统来决定。 默认情况下，Redis的 appendfsync 参数为 everysec 。如果需要提高持久化安全性，可以将其改为 always ，如果更关注性能，则可以将其改为 no。但是需要注意的是，使用 no 可能会导致数据丢失的风险，建议在应用场景允许的情况下谨慎使用。 AOF 重写Redis的 AOF 重写机制指的是将 AOF 文件中的冗余命令删除，以减小 AOF 文件的大小并提高读写性能的过程。 通过该功能，Redis 服务器可以创建一个新的 AOF 文件来替代现有的 AOF 文件，新旧两个 AOF 文件所保存的数据库状态相同，但新 AOF 文件不会包含任何浪费空间的冗余命令，所以新 AOF 文件的体积通常会比旧 AOF 文件的体积要小得多。 混合持久化让用户能够同时拥有上述两种持久化的优点， Redis 4.0 推出了一个“鱼和熊掌兼得”的持久化方案 —— RDB-AOF 混合持久化。 这种持久化能够通过 AOF 重写操作创建出一个同时包含 RDB 数据和 AOF 数据的 AOF 文件， 其中 RDB 数据位于 AOF 文件的开头， 它们储存了服务器开始执行重写操作时的数据库状态。至于那些在重写操作执行之后执行的 Redis 命令， 则会继续以 AOF 格式追加到 AOF 文件的末尾， 也即是 RDB 数据之后。 也就是说当开启混合持久化之后，AOF文件中的内容：前半部分是二进制的RDB内容，后面跟着AOF增加的数据，AOF位于两次RDB之间。 格式会类似下面这样： 优点 恢复速度快：RDB 文件恢复速度快，减少了重启时的数据加载时间。 数据完整性：AOF 记录所有写操作，确保数据完整性。 文件大小适中：混合持久化文件比纯 AOF 文件小，但比纯 RDB 文件大。 缺点 复杂性增加：混合持久化增加了 Redis 的复杂性，需要更多的配置和管理。 内存消耗：AOF 重写时会生成临时的 RDB 文件，可能会增加内存消耗。","link":"/2024/09/01/redis/%E6%8C%81%E4%B9%85%E5%8C%96/"},{"title":"序列构成的数组","text":"序列类型Python 标准库用 C 实现了丰富的序列类型，列举如下。 容器序列 list、tuple 和 collections.deque 这些序列能存放不同类型的数据。 扁平序列 str、bytes、bytearray、memoryview 和 array.array，这类序列只能容纳一种类型。 容器序列存放的是它们所包含的任意类型的对象的引用，而扁平序列里存放的是值而不是引用。换句话说，扁平序列其实是一段连续的内存空间。由此可见扁平序列其实更加紧凑，但是它里面只能存放诸如字符、字节和数值这种基础类型。 序列类型还能按照能否被修改来分类。 可变序列 list、bytearray、array.array、collections.deque 和 memoryview。 不可变序列 tuple、str 和 bytes。 列表推导列表推导（List Comprehension）是 Python 中一种简洁且高效的创建列表的语法。它允许你在一行代码内通过对可迭代对象（如列表、元组、范围等）进行遍历、筛选和转换，从而生成新的列表。 列表推导同filter和map的比较 列表推导（List Comprehension）、filter() 和 map() 都可用于处理可迭代对象，它们的功能存在重叠，但语法和适用场景各有不同。下面对它们进行详细对比。 1. 列表推导 语法：[表达式 for 变量 in 可迭代对象 if 条件] 作用：结合筛选和转换，生成新列表。 示例：1234# 筛选偶数并平方nums = [1, 2, 3, 4]result = [x**2 for x in nums if x % 2 == 0]# 输出: [4, 16] 2. map() 函数 语法：map(函数, 可迭代对象) 作用：对每个元素应用函数，返回迭代器。 示例：12result = map(lambda x: x**2, nums)# 需用 list() 转换: [1, 4, 9, 16] 3. filter() 函数 语法：filter(判断函数, 可迭代对象) 作用：筛选出满足条件的元素，返回迭代器。 示例：12result = filter(lambda x: x % 2 == 0, nums)# 需用 list() 转换: [2, 4] 笛卡儿积 使用列表推导计算笛卡儿积12345678910111213141516171819202122232425&gt;&gt;&gt; colors = ['black', 'white']&gt;&gt;&gt; sizes = ['S', 'M', 'L']&gt;&gt;&gt; tshirts = [(color, size) for color in colors for size in sizes] &gt;&gt;&gt; tshirts[('black', 'S'), ('black', 'M'), ('black', 'L'), ('white', 'S'), ('white', 'M'), ('white', 'L')] &gt;&gt;&gt; for color in colors: ... for size in sizes:... print((color, size))...('black', 'S')('black', 'M')('black', 'L')('white', 'S')('white', 'M')('white', 'L')# 想依照先尺码后颜色的顺序来排列，只需要调整从句的顺序&gt;&gt;&gt; tshirts = [(color, size) for size in sizes ... for color in colors]&gt;&gt;&gt; tshirts[('black', 'S'), ('white', 'S'), ('black', 'M'), ('white', 'M'), ('black', 'L'), ('white', 'L')] 生成器表达式虽然也可以用列表推导来初始化元组、数组或其他序列类型，但是生成器表达式是更好的选择。这是因为生成器表达式背后遵守了迭代器协议，可以逐个地产出元素，而不是先建立一个完整的列表，然后再把这个列表传递到某个构造函数里。前面那种方式显然能够节省内存。 生成器表达式的语法跟列表推导差不多，只不过把方括号换成圆括号而已。 1234567891011&gt;&gt;&gt; symbols = '$¢£¥€¤'&gt;&gt;&gt; tuple(ord(symbol) for symbol in symbols) ➊ 如果生成器表达式是一个函数调用过程中的唯一参数，那么不需要额外再用括号把它围起来。(36, 162, 163, 165, 8364, 164)&gt;&gt;&gt; import array&gt;&gt;&gt; array.array('I', (ord(symbol) for symbol in symbols)) ➋array 的构造方法需要两个参数，因此括号是必需的。array 构造方法的第一个参数指定了数组中数字的存储方式。array('I', [36, 162, 163, 165, 8364, 164]) 使用生成器表达式计算笛卡儿积利用生成器表达式实现了一个笛卡儿积，用以打印出上文中我们提到过的 T恤衫的 2 种颜色和 3 种尺码的所有组合。与示例 2-4 不同的是，用到生成器表达式之后，内存里不会留下一个有 6 个组合的列表，因为生成器表达式会在每次 for 循环运行时才生成一个组合。如果要计算两个各有 1000 个元素的列表的笛卡儿积，生成器表达式就可以帮忙省掉运行 for 循环的开销，即一个含有 100 万个元素的列表。 123456789101112&gt;&gt;&gt; colors = ['black', 'white']&gt;&gt;&gt; sizes = ['S', 'M', 'L']&gt;&gt;&gt; for tshirt in ('%s %s' % (c, s) for c in colors for s in sizes): ➊ 生成器表达式逐个产出元素，从来不会一次性产出一个含有 6 个 T 恤样式的列表... print(tshirt)...black Sblack Mblack Lwhite Swhite Mwhite L 元组元组其实是对数据的记录：元组中的每个元素都存放了记录中一个字段的数据，外加个字段的位置。正是这个位置信息给数据赋予了意义。如果只把元组理解为不可变的列表，那其他信息——它所含有的元素的总数和它们的位置——似乎就变得可有可无。但是如果把元组当作一些字段的集合，那么数量和位置信息就变得非常重要了。 123456789101112131415161718192021&gt;&gt;&gt; lax_coordinates = (33.9425, -118.408056) &gt;&gt;&gt; city, year, pop, chg, area = ('Tokyo', 2003, 32450, 0.66, 8014) &gt;&gt;&gt; traveler_ids = [('USA', '31195855'), ('BRA', 'CE342567'), ... ('ESP', 'XDA205856')]&gt;&gt;&gt; for passport in sorted(traveler_ids): ... print('%s/%s' % passport) ...BRA/CE342567ESP/XDA205856USA/31195855# for 循环可以分别提取元组里的元素，也叫作拆包（unpacking）。因为元组中第二个元素对我们没有什么用，所以它赋值给“_”占位符。&gt;&gt;&gt; for country, _ in traveler_ids: ... print(country)...USABRAESP 元组拆包拆包让元组可以完美地被当作记录来使用，最好辨认的元组拆包形式就是平行赋值，也就是说把一个可迭代对象里的元素，一并赋值到由对应的变量组成的元组中。 12345678910111213141516171819202122232425262728293031323334353637&gt;&gt;&gt; lax_coordinates = (33.9425, -118.408056)&gt;&gt;&gt; latitude, longitude = lax_coordinates # 元组拆包&gt;&gt;&gt; latitude33.9425&gt;&gt;&gt; longitude-118.408056## 不使用中间变量交换两个变量的值&gt;&gt;&gt; b, a = a, b## 以用 * 运算符把一个可迭代对象拆开作为函数的参数&gt;&gt;&gt; divmod(20, 8)(2, 4)&gt;&gt;&gt; t = (20, 8)&gt;&gt;&gt; divmod(*t)(2, 4)&gt;&gt;&gt; quotient, remainder = divmod(*t)&gt;&gt;&gt; quotient, remainder(2, 4)## 让一个函数可以用元组的形式返回多个值，然后调用函数的代码就能轻松地接受这些返回值。比如 os.path.split() 函数就会返回以路径和最后一个文件名组成的元组 (path, last_part):&gt;&gt;&gt; import os&gt;&gt;&gt; _, filename = os.path.split('/home/luciano/.ssh/idrsa.pub')&gt;&gt;&gt; filename'idrsa.pub'## 元组拆包中使用 * 也可以帮助我们把注意力集中在元组的部分元素上。&gt;&gt;&gt; a, b, *rest = range(5)&gt;&gt;&gt; a, b, rest(0, 1, [2, 3, 4])&gt;&gt;&gt; a, b, *rest = range(3)&gt;&gt;&gt; a, b, rest(0, 1, [2])&gt;&gt;&gt; a, b, *rest = range(2)&gt;&gt;&gt; a, b, rest(0, 1, []) 嵌套元组拆包12345678910111213141516# 每个元组内有 4 个元素，其中最后一个元素是一对坐标。metro_areas = [ ('Tokyo','JP',36.933,(35.689722,139.691667)), ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)), ('Mexico City', 'MX', 20.142, (19.433333, -99.133333)), ('New York-Newark', 'US', 20.104, (40.808611, -74.020386)), ('Sao Paulo', 'BR', 19.649, (-23.547778, -46.635833)),]print('{:15} | {:^9} | {:^9}'.format('', 'lat.', 'long.'))fmt = '{:15} | {:9.4f} | {:9.4f}'# 们把输入元组的最后一个元素拆包到由变量构成的元组里，这样就获取了坐标for name, cc, pop, (latitude, longitude) in metro_areas: if longitude &lt;= 0: print(fmt.format(name, latitude, longitude)) 具名元组collections.namedtuple 是一个工厂函数，它可以用来构建一个带字段名的元组和一个有名字的类 1234567891011121314151617181920212223&gt;&gt;&gt; from collections import namedtuple# 创建一个具名元组需要两个参数，一个是类名，另一个是类的各个字段的名字。后者可以是由数个字符串组成的可迭代对象，或者是由空格分隔开的字段名组成的字符串&gt;&gt;&gt; City = namedtuple('City', 'name country population coordinates') # 存放在对应字段里的数据要以一串参数的形式传入到构造函数中（注意，元组的构造函数却只接受单一的可迭代对象）。&gt;&gt;&gt; tokyo = City('Tokyo', 'JP', 36.933, (35.689722, 139.691667)) &gt;&gt;&gt; tokyoCity(name='Tokyo', country='JP', population=36.933, coordinates=(35.689722,139.691667))# 通过字段名或者位置来获取一个字段的信息。&gt;&gt;&gt; tokyo.population ➌36.933&gt;&gt;&gt; tokyo.coordinates(35.689722, 139.691667)&gt;&gt;&gt; tokyo[1]'JP' 除了从普通元组那里继承来的属性之外，具名元组还有一些自己专有的属性 123456789101112131415161718192021222324# fields 属性是一个包含这个类所有字段名称的元组。&gt;&gt;&gt; City._fields ('name', 'country', 'population', 'coordinates')&gt;&gt;&gt; LatLong = namedtuple('LatLong', 'lat long')&gt;&gt;&gt; delhi_data = ('Delhi NCR', 'IN', 21.935, LatLong(28.613889, 77.208889))# 用 _make() 通过接受一个可迭代对象来生成这个类的一个实例，它的作用跟City(*delhi_data) 是一样的。&gt;&gt;&gt; delhi = City._make(delhi_data) ➋# _asdict() 把具名元组以 collections.OrderedDict 的形式返回，我们可以利用它来把元组里的信息友好地呈现出来。&gt;&gt;&gt; delhi._asdict() OrderedDict([('name', 'Delhi NCR'), ('country', 'IN'), ('population',21.935), ('coordinates', LatLong(lat=28.613889, long=77.208889))])&gt;&gt;&gt; for key, value in delhi._asdict().items(): print(key + ':', value)name: Delhi NCRcountry: INpopulation: 21.935coordinates: LatLong(lat=28.613889, long=77.208889)&gt;&gt;&gt; 列表或元组的方法和属性 切片在 Python 里，像列表（list）、元组（tuple）和字符串（str）这类序列类型都支持切片操作 在切片和区间操作里不包含区间范围的最后一个元素 12345678910111213&gt;&gt;&gt; l = [10, 20, 30, 40, 50, 60]&gt;&gt;&gt; l[:2] # 在下标2的地方分割[10, 20]&gt;&gt;&gt; l[2:][30, 40, 50, 60]&gt;&gt;&gt; l[:3] # 在下标3的地方分割[10, 20, 30] &gt;&gt;&gt; l[3:][40, 50, 60] 对对象进行切片可以用 s[a:b:c] 的形式对 s 在 a 和 b 之间以 c 为间隔取值。c 的值还可以为负，负值意味着反向取值。a:b:c 这种用法只能作为索引或者下标用在 [] 中来返回一个切片对象：slice(a,b,c)。 1234567&gt;&gt;&gt; s = 'bicycle'&gt;&gt;&gt; s[::3]'bye'&gt;&gt;&gt; s[::-1]'elcycib'&gt;&gt;&gt; s[::-2]'eccb' slice() 是 Python 内置函数，用于创建一个切片对象，表示在序列（如列表、字符串、元组等）上进行切片操作的范围。 s = 'Hello, World!' print(s[0:5]) # 输出 'Hello' 等价于 sl = slice(0, 5) print(s[sl]) # 输出 'Hello' 1234567891011121314151617181920&gt;&gt;&gt; invoice = &quot;&quot;&quot;... 0.....6................................40........52...55........... 1909 Pimoroni PiBrella $17.50 3 $52.50... 1489 6mm Tactile Switch x20 $4.95 2 $9.90... 1510 Panavise Jr. - PV-201 $28.00 1 $28.00... 1601 PiTFT Mini Kit 320x240 $34.95 1 $34.95... &quot;&quot;&quot;&gt;&gt;&gt; SKU = slice(0, 6)&gt;&gt;&gt; DESCRIPTION = slice(6, 40)&gt;&gt;&gt; UNIT_PRICE = slice(40, 52)&gt;&gt;&gt; QUANTITY = slice(52, 55)&gt;&gt;&gt; ITEM_TOTAL = slice(55, None)&gt;&gt;&gt; line_items = invoice.split('\\n')[2:]&gt;&gt;&gt; for item in line_items:... print(item[UNIT_PRICE], item[DESCRIPTION])... $17.50 Pimoroni PiBrella $4.95 6mm Tactile Switch x20 $28.00 Panavise Jr. - PV-201 $34.95 PiTFT Mini Kit 320x240 多维切片和省略[] 运算符里还可以使用以逗号分开的多个索引或者是切片，外部库 NumPy 里就用到了这个特性，二维的 numpy.ndarray 就可以用 a[i, j] 这种形式来获取，抑或是用 a[m:n,k:l] 的方式来得到二维切片。 要正确处理这种 [] 运算符的话，对象的特殊方法 getitem 和 setitem 需要以元组的形式来接收a[i, j] 中的索引。也就是说，如果要得到 a[i, j] 的值，Python 会调用a.getitem((i, j))。 f(a, …, z)，或 a[i:…]。在 NumPy中，… 用作多维数组切片的快捷方式。如果 x 是四维数组，那么 x[i, …] 就是 x[i,:, :, :] 的缩写。 给切片赋值把切片放在赋值语句的左边，或把它作为 del 操作的对象，我们就可以对序列进行嫁接、切除或就地修改操作 1234567891011121314151617181920212223242526&gt;&gt;&gt; l = list(range(10))&gt;&gt;&gt; l[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; l[2:5] = [20, 30]&gt;&gt;&gt; l[0, 1, 20, 30, 5, 6, 7, 8, 9]&gt;&gt;&gt; del l[5:7]&gt;&gt;&gt; l[0, 1, 20, 30, 5, 8, 9]&gt;&gt;&gt; l[3::2] = [11, 22]&gt;&gt;&gt; l[0, 1, 20, 11, 5, 22, 9]#如果赋值的对象是一个切片，那么赋值语句的右侧必须是个可迭代对象。即便只有单独一个值，也要把它转换成可迭代的序列&gt;&gt;&gt; l[2:5] = 100 Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: can only assign an iterable&gt;&gt;&gt; l[2:5] = [100]&gt;&gt;&gt; l[0, 1, 100, 22, 9] 对序列使用+和*序列是支持 + 和 * 操作的。通常 + 号两侧的序列由相同类型的数据所构成，在拼接的过程中，两个被操作的序列都不会被修改，Python 会新建一个包含同样类型数据的序列来作为拼接的结果。 + 和 * 都遵循不修改原有的操作对象，而是构建一个全新的序列 123456&gt;&gt;&gt; l = [1, 2, 3]&gt;&gt;&gt; l * 5[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]&gt;&gt;&gt; 5 * 'abcd''abcdabcdabcdabcdabcd 如果在 a * n 这个语句中，序列 a 里的元素是对其他可变对象的引用的话，你就需要格外注意了。比如，你想用my_list = [[]] * 3 来初始化一个由列表组成的列表，但是你得到的列表里包含的 3 个元素其实是 3 个引用，而且这 3 个引用指向的都是同一个列表。 建立由列表组成的列表一个包含 3 个列表的列表，嵌套的 3 个列表各自有 3 个元素来代表井字游戏的一行方块列表推导 123456&gt;&gt;&gt; board = [['_'] * 3 for i in range(3)] ➊&gt;&gt;&gt; board[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]&gt;&gt;&gt; board[1][2] = 'X' ➋&gt;&gt;&gt; board[['_', '_', '_'], ['_', '_', 'X'], ['_', '_', '_']] 使用 “*” 12345678&gt;&gt;&gt; weird_board = [['_'] * 3] * 3 &gt;&gt;&gt; weird_board[['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]## 因为['_'] * 3是可变对象的应用 导致原对象变化&gt;&gt;&gt; weird_board[1][2] = 'O' &gt;&gt;&gt; weird_board[['_', '_', 'O'], ['_', '_', 'O'], ['_', '_', 'O']] 序列的增量赋值增量赋值运算符 += 和 *= 的表现取决于它们的第一个操作对象。+= 背后的特殊方法是 __ iadd __（用于“就地加法”）。但是如果一个类没有实现这个方法的话，Python 会退一步调用 __ add __ 。 a += b如果 a 实现了 iadd 方法，就会调用这个方法。同时对可变序列（例如list、bytearray 和 array.array）来说，a 会就地改动，就像调用了 a.extend(b)一样。但是如果 a 没有实现 iadd 的话，a += b 这个表达式的效果就变得跟 a = a+ b 一样了：首先计算 a + b，得到一个新的对象，然后赋值给 a。也就是说，在这个表达式中，变量名会不会被关联到新的对象，完全取决于这个类型有没有实现 iadd 这个方法。 1234567891011121314151617181920212223&gt;&gt;&gt; l = [1, 2, 3]&gt;&gt;&gt; id(l)4311953800 ➊&gt;&gt;&gt; l *= 2&gt;&gt;&gt; l[1, 2, 3, 1, 2, 3]&gt;&gt;&gt; id(l)4311953800 ➋&gt;&gt;&gt; t = (1, 2, 3)&gt;&gt;&gt; id(t)4312681568 ➌&gt;&gt;&gt; t *= 2&gt;&gt;&gt; id(t)4301348296 ➍❶ 刚开始时列表的 ID。❷ 运用增量乘法后，列表的 ID 没变，新元素追加到列表上。❸ 元组最开始的 ID。❹ 运用增量乘法后，新的元组被创建。 list.sort方法和内置函数sortedlist.sort 方法会就地排序列表，也就是说不会把原列表复制一份。与 list.sort 相反的是内置函数 sorted，它会新建一个列表作为返回值。这个方法可以接受任何形式的可迭代对象作为参数，甚至包括不可变序列或生成器。而不管 sorted 接受的是怎样的参数，它最后都会返回一个列表。 不管是 list.sort 方法还是 sorted 函数，都有两个可选的关键字参数。reverse 如果被设定为 True，被排序的序列里的元素会以降序输出（也就是说把最大值当作最小值来排序）。这个参数的默认值是 False。key 一个只有一个参数的函数，这个函数会被用在序列里的每一个元素上，所产生的结果将是排序算法依赖的对比关键字。比如说，在对一些字符串排序时，可以用key=str.lower 来实现忽略大小写的排序，或者是用 key=len 进行基于字符串长度的排序。这个参数的默认值是恒等函数（identity function），也就是默认用元素自己的值来排序。 1234567891011121314151617181920212223242526&gt;&gt;&gt; fruits = ['grape', 'raspberry', 'apple', 'banana']&gt;&gt;&gt; 新建了一个按照字母排序的字符串列表。&gt;&gt;&gt; sorted(fruits)&gt;&gt;&gt; ['apple', 'banana', 'grape', 'raspberry'] &gt;&gt;&gt; fruits['grape', 'raspberry', 'apple', 'banana'] &gt;&gt;&gt; sorted(fruits, reverse=True) 按照字母降序排序。['raspberry', 'grape', 'banana', 'apple'] 新建一个按照长度排序的字符串列表。因为这个排序算法是稳定的，grape 和 apple 的长度都是 5，它们的相对位置跟在原来的列表里是一样的。&gt;&gt;&gt; sorted(fruits, key=len)['grape', 'apple', 'banana', 'raspberry'] 按照长度降序排序的结果。结果并不是上面那个结果的完全翻转，因为用到的排序算法是稳定的，也就是说在长度一样时，grape 和 apple 的相对位置不会改变。&gt;&gt;&gt; sorted(fruits, key=len, reverse=True)['raspberry', 'banana', 'grape', 'apple'] &gt;&gt;&gt; fruits['grape', 'raspberry', 'apple', 'banana'] 对原列表就地排序，返回值 None 会被控制台忽略。 此时 fruits 本身被排序。&gt;&gt;&gt; fruits.sort() &gt;&gt;&gt; fruits['apple', 'banana', 'grape', 'raspberry'] 用bisect来管理已排序的序列bisect 模块包含两个主要函数，bisect 和 insort，两个函数都利用二分查找算法来在有序序列中查找或插入元素。 用bisect来搜索bisect 模块包含两个主要函数，bisect 和 insort，两个函数都利用二分查找算法来在有序序列中查找或插入元素。 12345678910111213141516171819202122import bisectimport sysHAYSTACK = [1, 4, 5, 6, 8, 12, 15, 20, 21, 23, 23, 26, 29, 30]NEEDLES = [0, 1, 2, 5, 8, 10, 22, 23, 29, 30, 31]ROW_FMT = '{0:2d} @ {1:2d} {2}{0:&lt;2d}'def demo(bisect_fn): for needle in reversed(NEEDLES): # 用特定的 bisect 函数来计算元素应该出现的位置。 position = bisect_fn(HAYSTACK, needle) # 利用该位置来算出需要几个分隔符号。 offset = position * ' |' print(ROW_FMT.format(needle, position, offset)) if __name__ == '__main__': if sys.argv[-1] == 'left': bisect_fn = bisect.bisect_left else: bisect_fn = bisect.bisect print('DEMO:', bisect_fn.__name__) print('haystack -&gt;', ' '.join('%2d' % n for n in HAYSTACK)) demo(bisect_fn) bisect 的表现可以从两个方面来使用。 可选参数——lo 和 hi——来缩小搜寻的范围。lo 的默认值是 0，hi的默认值是序列的长度，即 len() 作用于该序列的返回值。 其次，bisect 函数其实是 bisect_right 函数的别名，后者还有个姊妹函数叫bisect_left。它们的区别在于，bisect_left 返回的插入位置是原序列中跟被插元素相等的元素的位置，也就是新元素会被放置于它相等的元素的前面，bisect_right返回的则是跟它相等的元素之后的位置。这个细微的差别可能对于整数序列来讲没什么用，但是对于那些值相等但是形式不同的数据类型来讲，结果就不一样了。比如说虽然1== 1.0 的返回值是 True，1 和 1.0 其实是两个不同的元素。 1234567#建立一个用数字作为索引的查询表格，比如说把分数和成绩 对应起来&gt;&gt;&gt; def grade(score, breakpoints=[60, 70, 80, 90], grades='FDCBA'):... i = bisect.bisect(breakpoints, score)... return grades[i]...&gt;&gt;&gt; [grade(score) for score in [33, 99, 77, 70, 89, 90, 100]]['F', 'A', 'C', 'C', 'B', 'A', 'A'] 用bisect.insort插入新元素insort(seq, item) 把变量 item 插入到序列 seq 中，并能保持 seq 的升序顺序 123456789import bisectimport randomSIZE=7random.seed(1729)my_list = []for i in range(SIZE): new_item = random.randrange(SIZE*2) bisect.insort(my_list, new_item) print('%2d -&gt;' % new_item, my_list) insort 跟 bisect 一样，有 lo 和 hi 两个可选参数用来控制查找的范围。它也有个变体叫 insort_left，这个变体在背后用的是 bisect_left。 数组需要一个只包含数字的列表，那么 array.array 比 list 更高效。 12345678910111213141516171819&gt;&gt;&gt; from array import array 引入 array 类型。&gt;&gt;&gt; from random import random&gt;&gt;&gt; floats = array('d', (random() for i in range(10**7))) 建立一个双精度浮点数组（类型码是 'd'）&gt;&gt;&gt; floats[-1] 查看数组的最后一个元素。0.07802343889111107&gt;&gt;&gt; fp = open('floats.bin', 'wb')&gt;&gt;&gt; floats.tofile(fp) 把数组存入一个二进制文件里。&gt;&gt;&gt; fp.close()&gt;&gt;&gt; floats2 = array('d') 新建一个双精度浮点空数组。&gt;&gt;&gt; fp = open('floats.bin', 'rb')&gt;&gt;&gt; floats2.fromfile(fp, 10**7) 把 1000 万个浮点数从二进制文件里读取出来。&gt;&gt;&gt; fp.close()&gt;&gt;&gt; floats2[-1] 查看数组的最后一个元素。0.07802343889111107&gt;&gt;&gt; floats2 == floats 检查两个数组的内容是不是完全一样。true 内存视图memoryview 是一个内置类，它能让用户在不复制内容的情况下操作同一个数组的不同切片 内存视图其实是泛化和去数学化的 NumPy 数组。它让你在不需要复制内容的前提下，在数据结构之间共享内存。其中数据结构可以是任何形式，比如 PIL 图片、SQLite 数据库和 NumPy 的数组，等等。这个功能在处理大型数据集合的时候非常重要。 memoryview.cast 的概念跟数组模块类似，能用不同的方式读写同一块内存数据，而且内容字节不会随意移动。 通过改变数组中的一个字节来更新数组里某个元素的值: 12345678910111213141516171819202122&gt;&gt;&gt; numbers = array.array('h', [-2, -1, 0, 1, 2])&gt;&gt;&gt; memv = memoryview(numbers) 5 个短整型有符号整数的数组（类型码是 'h'）&gt;&gt;&gt; len(memv)5&gt;&gt;&gt; memv[0] memv 里的 5 个元素跟数组里的没有区别。-2&gt;&gt;&gt; memv_oct = memv.cast('B') 创建一个 memv_oct，这一次是把 memv 里的内容转换成 'B' 类型，也就是无符号字符。&gt;&gt;&gt; memv_oct.tolist() 以列表的形式查看 memv_oct 的内容。[254, 255, 255, 255, 0, 0, 1, 0, 2, 0]&gt;&gt;&gt; memv_oct[5] = 4 把位于位置 5 的字节赋值成 4。&gt;&gt;&gt; numbersarray('h', [-2, -1, 1024, 1, 2]) 因为我们把占 2 个字节的整数的高位字节改成了 4，所以这个有符号整数的值就变成了 1024。numbers = array.array('h', [-2, -1, 0, 1, 2])：创建一个由有符号短整型（类型码 'h'）组成的数组，包含5个元素。memv = memoryview(numbers)：创建一个 memoryview 对象 memv，它引用了 numbers 数组的数据。memoryview 提供了一种访问原始字节数据的方式，而无需复制数据。len(memv)：返回 memv 中的元素数量，这里是 5，因为数组中有5个 short 类型的值。memv[0]：访问第一个元素，其值为 -2，与原数组一致。memv_oct = memv.cast('B')：将 memoryview 的类型转换为 'B'，即无符号字符（unsigned char）。这一步不会改变底层数据，只是改变了看待数据的方式。memv_oct.tolist()：将 memv_oct 转换为列表形式查看，输出 [254, 255, 255, 255, 0, 0, 1, 0, 2, 0]。这是因为每个 short 占两个字节，在小端序机器上，-2 表示为 0b11111110 11111111，即 254 和 255，依此类推。memv_oct[5] = 4：修改第6个字节的值为 4，由于 numbers 是以 short 类型存储的，因此这个操作会改变 numbers 中第3个元素的高位字节。numbers：此时数组变为 array('h', [-2, -1, 1024, 1, 2])，因为第3个 short 值的高位字节被改为 4，低位字节仍然是 0，所以该值变成 4 * 256 + 0 = 1024。 NumPy和SciPyNumPy 实现了多维同质数组（homogeneous array）和矩阵，这些数据结构不但能处理数字，还能存放其他由用户定义的记录。通过 NumPy，用户能对这些数据结构里的元素进行高效的操作。 SciPy 是基于 NumPy 的另一个库，它提供了很多跟科学计算有关的算法，专为线性代数、数值积分和统计学而设计。SciPy 的高效和可靠性归功于其背后的 C 和 Fortran 代码，而这些跟计算有关的部分都源自于 Netlib 库 12345678910111213141516171819202122232425262728&gt;&gt;&gt; import numpy &gt;&gt;&gt; a = numpy.arange(12) 新建一个 0~11 的整数的 numpy.ndarry，然后把它打印出来。&gt;&gt;&gt; aarray([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])&gt;&gt;&gt; type(a)&lt;class 'numpy.ndarray'&gt;&gt;&gt;&gt; a.shape 看看数组的维度，它是一个一维的、有 12 个元素的数组。(12,)&gt;&gt;&gt; a.shape = 3, 4 把数组变成二维的&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) &gt;&gt;&gt; a[2] 打印出第 2 行。array([ 8, 9, 10, 11])&gt;&gt;&gt; a[2, 1] 打印第 2 行第 1 列的元素。9&gt;&gt;&gt; a[:, 1] 把第 1 列打印出来。array([1, 5, 9])&gt;&gt;&gt; a.transpose() 把行和列交换，就得到了一个新数组。array([[ 0, 4, 8], [ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11]]) 1234567891011121314151617181920&gt;&gt;&gt; import numpy&gt;&gt;&gt; floats = numpy.loadtxt('floats-10M-lines.txt') 从文本文件里读取 1000 万个浮点数。&gt;&gt;&gt; floats[-3:] 利用序列切片来读取其中的最后 3 个数。array([ 3016362.69195522, 535281.10514262, 4566560.44373946])&gt;&gt;&gt; floats *= .5 把数组里的每个数都乘以 0.5&gt;&gt;&gt; floats[-3:]array([ 1508181.34597761, 267640.55257131, 2283280.22186973])&gt;&gt;&gt; from time import perf_counter as pc 导入精度和性能都比较高的计时器&gt;&gt;&gt; t0 = pc(); floats /= 3; pc() - t0 把每个元素都除以 3，可以看到处理 1000 万个浮点数所需的时间还不足 40 毫秒。0.03690556302899495&gt;&gt;&gt; numpy.save('floats-10M', floats) 把数组存入后缀为 .npy 的二进制文件。&gt;&gt;&gt; floats2 = numpy.load('floats-10M.npy', 'r+') 将上面的数据导入到另外一个数组里，这次 load 方法利用了一种叫作内存映射的机制，它让我们在内存不足的情况下仍然可以对数组做切片。&gt;&gt;&gt; floats2 *= 6&gt;&gt;&gt; floats2[-3:] 把数组里每个数乘以 6 之后，再检视一下数组的最后 3 个数。memmap([3016362.69195522, 535281.10514262, 4566560.44373946]) 双向队列和其他形式的队列collections.deque 类（双向队列）是一个线程安全、可以快速从两端添加或者删除元素的数据类型。而且如果想要有一种数据类型来存放“最近用到的几个元素”，deque 也是一个很好的选择。这是因为在新建一个双向队列的时候，你可以指定这个队列的大小，如果这个队列满员了，还可以从反向端删除过期的元素，然后在尾端添加新的元素。 1234567891011121314151617181920212223242526&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; dq = deque(range(10), maxlen=10) maxlen 是一个可选参数，代表这个队列可以容纳的元素的数量，而且一旦设定，这个属性就不能修改&gt;&gt;&gt; dqdeque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)&gt;&gt;&gt; dq.rotate(3) 队列的旋转操作接受一个参数 n，当 n &gt; 0 时，队列的最右边的 n 个元素会被移动到队列的左边。当 n &lt; 0 时，最左边的 n 个元素会被移动到右边。&gt;&gt;&gt; dqdeque([7, 8, 9, 0, 1, 2, 3, 4, 5, 6], maxlen=10)&gt;&gt;&gt; dq.rotate(-4)&gt;&gt;&gt; dqdeque([1, 2, 3, 4, 5, 6, 7, 8, 9, 0], maxlen=10)&gt;&gt;&gt; dq.appendleft(-1) 当试图对一个已满（len(d) == d.maxlen）的队列做尾部添加操作的时候，它头部的元素会被删除掉。注意在下一行里，元素 0 被删除了&gt;&gt;&gt; dqdeque([-1, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10)&gt;&gt;&gt; dq.extend([11, 22, 33]) 在尾部添加 3 个元素的操作会挤掉 -1、1 和 2。&gt;&gt;&gt; dqdeque([3, 4, 5, 6, 7, 8, 9, 11, 22, 33], maxlen=10)&gt;&gt;&gt; dq.extendleft([10, 20, 30, 40]) extendleft(iter) 方法会把迭代器里的元素逐个添加到双向队列的左边，因此迭代器里的元素会逆序出现在队列里。&gt;&gt;&gt; dqdeque([40, 30, 20, 10, 3, 4, 5, 6, 7, 8], maxlen=10) queue 提供了同步（线程安全）类 Queue、LifoQueue 和 PriorityQueue，不同的线程可以利用这些数据类型来交换信息。这三个类的构造方法都有一个可选参数 maxsize，它接收正整数作为输入值，用来限定队列的大小。但是在满员的时候，这些类不会扔掉旧的元素来腾出位置。相反，如果队列满了，它就会被锁住，直到另外的线程移除了某个元素而腾出了位置。这一特性让这些类很适合用来控制活跃线程的数量。 multiprocessing 这个包实现了自己的 Queue，它跟 queue.Queue 类似，是设计给进程间通信用的。同时还有一个专门的 multiprocessing.JoinableQueue 类型，可以让任务管理变得更方便。 asyncio Python 3.4 新提供的包，里面有 Queue、LifoQueue、PriorityQueue 和JoinableQueue，这些类受到 queue 和 multiprocessing 模块的影响，但是为异步编程里的任务管理提供了专门的便利。 heapq 跟上面三个模块不同的是，heapq 没有队列类，而是提供了 heappush 和 heappop方法，让用户可以把可变序列当作堆队列或者优先队列来使用。","link":"/2025/05/26/python/%E5%9F%BA%E7%A1%80/%E5%BA%8F%E5%88%97%E6%9E%84%E6%88%90%E7%9A%84%E6%95%B0%E7%BB%84/"},{"title":"双下方法","text":"在 Python 中，双下方法（Dunder Methods）也被称为特殊方法或魔术方法，是指以双下划线 __ 开头和结尾的方法。这些方法在类中具有特殊的用途，主要用于实现类的内置行为，例如对象的创建、比较、运算、字符串表示等。以下是一些常见的双下方法及其用途： 1. 基础双下方法__init__(self, ...) 用途：对象初始化方法，创建对象时自动调用。 示例：123456class Person: def __init__(self, name, age): self.name = name self.age = agep = Person(&quot;Alice&quot;, 30) # 调用 __init__ __str__(self) 用途：定义对象的字符串表示（str(obj) 或 print(obj) 时调用）。 示例：1234def __str__(self): return f&quot;Person(name={self.name}, age={self.age})&quot;print(p) # 输出: Person(name=Alice, age=30) __repr__(self) 用途：定义对象的“官方”字符串表示（调试时使用，需能通过 eval() 重建对象）。 示例：1234def __repr__(self): return f&quot;Person('{self.name}', {self.age})&quot;repr(p) # 输出: &quot;Person('Alice', 30)&quot; 2. 比较运算符相关__eq__(self, other) 用途：定义对象的相等性比较（== 运算符）。 示例：123456def __eq__(self, other): return self.age == other.agep1 = Person(&quot;Alice&quot;, 30)p2 = Person(&quot;Bob&quot;, 30)p1 == p2 # 输出: True __lt__(self, other)、__gt__(self, other) 等 用途：定义对象的大小比较（&lt;、&gt; 等运算符）。 示例：1234def __lt__(self, other): return self.age &lt; other.agep1 &lt; p2 # 基于年龄比较 3. 算术运算符相关__add__(self, other)、__sub__(self, other) 等 用途：定义对象的加减乘除等运算。 示例：1234567891011class Vector: def __init__(self, x, y): self.x = x self.y = y def __add__(self, other): return Vector(self.x + other.x, self.y + other.y)v1 = Vector(1, 2)v2 = Vector(3, 4)v3 = v1 + v2 # Vector(4, 6) 4. 容器相关__len__(self) 用途：定义对象的长度（len(obj) 时调用）。 示例：123456789class MyList: def __init__(self, items): self.items = items def __len__(self): return len(self.items)lst = MyList([1, 2, 3])len(lst) # 输出: 3 __getitem__(self, key)、__setitem__(self, key, value) 用途：定义对象的索引访问（obj[key]）和赋值（obj[key] = value）。 示例：1234def __getitem__(self, idx): return self.items[idx]lst[0] # 输出: 1 5. 上下文管理器（with 语句）__enter__(self)、__exit__(self, exc_type, exc_val, exc_tb) 用途：定义对象在 with 语句中的行为。 示例：12345678910111213class File: def __init__(self, path): self.path = path def __enter__(self): self.file = open(self.path, 'r') return self.file def __exit__(self, *args): self.file.close()with File(&quot;test.txt&quot;) as f: data = f.read() 6. 其他常见双下方法__call__(self, ...) 用途：使对象可以像函数一样被调用。 示例：123456class Adder: def __call__(self, a, b): return a + badd = Adder()add(3, 4) # 输出: 7 __del__(self) 用途：对象被销毁时调用（慎用，不推荐依赖）。 特殊方法一览 反向算术运算符（Reverse Operators）当你尝试对两个对象进行算术运算，而左侧对象没有定义对应的运算符时，Python 会尝试调用右侧对象的反向运算符方法。 增量赋值运算符（In-place Operators）增量赋值允许你在原对象上直接修改值，而不是创建新对象。例如 +=, *=, -= 等。","link":"/2025/05/26/python/%E5%9F%BA%E7%A1%80/%E5%8F%8C%E4%B8%8B%E6%96%B9%E6%B3%95%EF%BC%88Dunder%20Methods%EF%BC%89%E3%80%81%E7%89%B9%E6%AE%8A%E6%96%B9%E6%B3%95/"},{"title":"保密","text":"保密（Confidentiality）系统如何保证敏感数据无法被包括系统管理员在内的内外部人员所窃取、滥用？ 按照需要保密信息所处的环节不同，可以划分为“信息在客户端时的保密”、“信息在传输时的保密”和“信息在服务端时的保密”三类，或者进一步概括为“端的保密”和“链路的保密”两类 保密的强度保密是有成本的，追求越高的安全等级，就要付出越多的工作量与算力消耗。 以摘要代替明文：如果密码本身比较复杂，那一次简单的哈希摘要至少可以保证即使传输过程中有信息泄漏，也不会被逆推出原信息；即使密码在一个系统中泄漏了，也不至于威胁到其他系统的使用，但这种处理不能防止弱密码被彩虹表攻击所破解。 先加盐值再做哈希是应对弱密码的常用方法：盐值可以替弱密码建立一道防御屏障，一定程度上防御已有的彩虹表攻击，但并不能阻止加密结果被监听、窃取后，攻击者直接发送加密结果给服务端进行冒认。 将盐值变为动态值能有效防止冒认：如果每次密码向服务端传输时都掺入了动态的盐值，让每次加密的结果都不同，那即使传输给服务端的加密结果被窃取了，也不能冒用来进行另一次调用。尽管在双方通信均可能泄漏的前提下协商出只有通信双方才知道的保密信息是完全可行的（后续介绍“传输安全层”时会提到），但这样协商出盐值的过程将变得极为复杂，而且每次协商只保护一次操作，也难以阻止对其他服务的重放攻击。 给服务加入动态令牌，在网关或其他流量公共位置建立校验逻辑，服务端愿意付出在集群中分发令牌信息等代价的前提下，可以做到防止重放攻击，但是依然不能抵御传输过程中被嗅探而泄漏信息的问题。 启用 HTTPS 可以防御链路上的恶意嗅探，也能在通信层面解决了重放攻击的问题。但是依然有因客户端被攻破产生伪造根证书风险、有因服务端被攻破产生的证书泄漏而被中间人冒认的风险、有因CRL更新不及时或者OCSP Soft-fail 产生吊销证书被冒用的风险、有因 TLS 的版本过低或密码学套件选用不当产生加密强度不足的风险。 为了抵御上述风险，保密强度还要进一步提升，譬如银行会使用独立于客户端的存储证书的物理设备（俗称的 U 盾）来避免根证书被客户端中的恶意程序窃取伪造；大型网站涉及到账号、金钱等操作时，会使用双重验证开辟一条独立于网络的信息通道（如手机验证码、电子邮件）来显著提高冒认的难度；甚至一些关键企业（如国家电网）或机构（如军事机构）会专门建设遍布全国各地的与公网物理隔离的专用内部网络来保障通信安全。 客户端加密 客户端在用户登录、注册一类场景里是否需要对密码进行加密，这个问题一直存有争议。 为了保证信息不被黑客窃取而做客户端加密没有太多意义，对绝大多数的信息系统来说，启用 HTTPS 可以说是唯一的实际可行的方案。 但是！为了保证密码不在服务端被滥用，在客户端就开始加密是很有意义的。大网站被拖库的事情层出不穷，密码明文被写入数据库、被输出到日志中之类的事情也屡见不鲜，做系统设计时就应该把明文密码这种东西当成是最烫手的山芋来看待，越早消灭掉越好，将一个潜在的炸弹从客户端运到服务端，对绝大多数系统来说都没有必要。 为什么客户端加密对防御泄密会没有意义？原因是网络通信并非由发送方和接收方点对点进行的，客户端无法决定用户送出的信息能不能到达服务端，或者会经过怎样的路径到达服务端，在传输链路必定是不安全的假设前提下，无论客户端做什么防御措施，最终都会沦为“马其诺防线”。中间人攻击它是通过劫持掉了客户端到服务端之间的某个节点，包括但不限于代理（通过 HTTP 代理返回赝品）、路由器（通过路由导向赝品）、DNS 服务（直接将你机器的 DNS 查询结果替换为赝品地址）等，来给你访问的页面或服务注入恶意的代码，极端情况下，甚至可能把要访问的服务或页面整个给取代掉，此时不论你在页面上设计了多么精巧严密的加密措施，都不会有保护作用。而攻击者只需地劫持路由器，或在局域网内其他机器释放 ARP 病毒便有可能做到这一点。 中间人攻击（Man-in-the-Middle Attack，MitM） 在消息发出方和接收方之间拦截双方通信。用日常生活中的写信来类比的话：你给朋友写了一封信，邮递员可以把每一份你寄出去的信都拆开看，甚至把信的内容改掉，然后重新封起来，再寄出去给你的朋友。朋友收到信之后给你回信，邮递员又可以拆开看，看完随便改，改完封好再送到你手上。你全程都不知道自己寄出去的信和收到的信都经过邮递员这个“中间人”转手和处理——换句话说，对于你和你朋友来讲，邮递员这个“中间人”角色是不可见的。 密码存储和验证对多数信息系统来说，只要配合一定的密码规则约束，譬如密码要求长度、特殊字符等，再配合 HTTPS 传输，已足防御大多数风险了。即使在用户采用了弱密码、客户端通信被监听、服务端被拖库、泄漏了存储的密文和盐值等问题同时发生，也能够最大限度避免用户明文密码被逆推出来。下面先介绍密码创建的过程： 用户在客户端注册，输入明文密码：123456。 1password = 123456 客户端对用户密码进行简单哈希摘要，可选的算法有 MD2/4/5、SHA1/256/512、BCrypt、PBKDF1/2，等等。为了突出“简单”的哈希摘要，这里笔者故意没有排除掉 MD 系这些已经有了高效碰撞手段的算法。 1client_hash = MD5(password) // e10adc3949ba59abbe56e057f20f883e 为了防御彩虹表攻击应加盐处理，客户端加盐只取固定的字符串即可，如实在不安心，最多用伪动态的盐值（“伪动态”是指服务端不需要额外通信可以得到的信息，譬如由日期或用户名等自然变化的内容，加上固定字符串构成）。 1client_hash = MD5(MD5(password) + salt) // SALT = $2a$10$o5L.dWYEjZjaejOmN3x4Qu 假设攻击者截获了客户端发出的信息，得到了摘要结果和采用的盐值，那攻击者就可以枚举遍历所有 8 位字符以内（“8 位”只是举个例子，反正就是指弱密码，你如果拿 1024 位随机字符当密码用，加不加盐，彩虹表都跟你没什么关系）的弱密码，然后对每个密码再加盐计算，就得到一个针对固定盐值的对照彩虹表。为了应对这种暴力破解，并不提倡在盐值上做动态化，更理想的方式是引入慢哈希函数来解决。 慢哈希函数是指这个函数执行时间是可以调节的哈希函数，通常是以控制调用次数来实现的。BCrypt 算法就是一种典型的慢哈希函数，它做哈希计算时接受盐值 Salt 和执行成本 Cost 两个参数（代码层面 Cost 一般是混入在 Salt 中，譬如上面例子中的 Salt 就是混入了 10 轮运算的盐值，10 轮的意思是 210次哈希，Cost 参数是放在指数上的，最大取值就 31）。如果我们控制 BCrypt 的执行时间大概是 0.1 秒完成一次哈希计算的话，按照 1 秒生成 10 个哈希值的速度，算完所有的 10 位大小写字母和数字组成的弱密码大概需要 P(62,10)/(3600×24×365)/0.1=1,237,204,169 年时间。 1client_hash = BCrypt(MD5(password) + salt) // MFfTW3uNI4eqhwDkG7HP9p2mzEUu/r2 只需防御被拖库后针对固定盐值的批量彩虹表攻击。具体做法是为每一个密码（指客户端传来的哈希值）产生一个随机的盐值。建议采用“密码学安全伪随机数生成器”（Cryptographically Secure Pseudo-Random Number Generator，CSPRNG）来生成一个长度与哈希值长度相等的随机字符串。对于 Java 语言，从 Java SE 7 起提供了java.security.SecureRandom类，用于支持 CSPRNG 字符串生成。 123SecureRandom random = new SecureRandom();byte server_salt[] = new byte[36];random.nextBytes(server_salt); // tq2pdxrblkbgp8vt8kbdpmzdh1w8bex 将动态盐值混入客户端传来的哈希值再做一次哈希，产生出最终的密文，并和上一步随机生成的盐值一起写入到同一条数据库记录中。由于慢哈希算法占用大量处理器资源，并不推荐在服务端中采用。不过，如果你阅读了 Fenix’s Bookstore 的源码，会发现这步依然采用了 Spring Security 5 中的BcryptPasswordEncoder，但是请注意它默认构造函数中的 Cost 参数值为-1，经转换后实际只进行了 210=1024 次计算，并不会对服务端造成太大的压力。此外，代码中并未显式传入 CSPRNG 生成的盐值，这是因为BCryptPasswordEncoder本身就会自动调用 CSPRNG 产生盐值，并将该盐值输出在结果的前 32 位之中，因此也无须专门在数据库中设计存储盐值字段。这个过程以伪代码表示如下 12server_hash = SHA256(client_hash + server_salt); // 55b4b5815c216cf80599990e781cd8974a1e384d49fbde7776d096e1dd436f67DB.save(server_hash, server_salt); 以上加密存储的过程相对复杂，但是运算压力最大的过程（慢哈希）是在客户端完成的，对服务端压力很小，也不惧怕因网络通信被截获而导致明文密码泄漏。密码存储后，以后验证的过程与加密是类似的，步骤如下 客户端，用户在登录页面中输入密码明文：123456，经过与注册相同的加密过程，向服务端传输加密后的结果。 1authentication_hash = MFfTW3uNI4eqhwDkG7HP9p2mzEUu/r2 服务端，接受到客户端传输上来的哈希值，从数据库中取出登录用户对应的密文和盐值，采用相同的哈希算法，对客户端传来的哈希值、服务端存储的盐值计算摘要结果。 1result = SHA256(authentication_hash + server_salt); // 55b4b5815c216cf80599990e781cd8974a1e384d49fbde7776d096e1dd436f67 比较上一步的结果和数据库储存的哈希值是否相同，如果相同那么密码正确，反之密码错误。 1authentication = compare(result, server_hash) // yes","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%9E%B6%E6%9E%84%E5%AE%89%E5%85%A8%E6%80%A7/%E4%BF%9D%E5%AF%86/"},{"title":"传输","text":"传输（Transport Security）系统如何保证通过网络传输的信息无法被第三方窃听、篡改和冒充？ 摘要、加密与签名摘要摘要也称之为数字摘要（Digital Digest）或数字指纹（Digital Fingerprint）。JWT 令牌中默认的签名信息是对令牌头、负载和密钥三者通过令牌头中指定的哈希算法（HMAC SHA256）计算出来的摘要值。 1signature = SHA256(base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload) , secret) 理想的哈希算法都具备两个特性： 一是易变性，这是指算法的输入端发生了任何一点细微变动，都会引发雪崩效应（Avalanche Effect），使得输出端的结果产生极大的变化。这个特性常被用来做校验，保护信息未被篡改，譬如互联网上下载大文件，常会附有一个哈希校验码，以确保下载下来的文件没有因网络或其他原因与原文件产生任何偏差。 二是不可逆性，摘要的过程是单向的，不可能从摘要的结果中逆向还原出输入值来。这点只要具备初中数学知识就能想明白，世间的信息有无穷多种，而摘要的结果无论其位数是 32、128、512 Bits，再大也总归是个有限的数字，因此输入数据与输出的摘要结果必然不是一一对应的关系，如果我把一部电影做摘要形成 256 Bits 的哈希值，应该没有人会指望能从这个哈希值中还原出一部电影的。偶尔能听到 MD5、SHA1 或其他哈希算法被破解了的新闻，这里的“破解”并不是“解密”的意思，而是指找到了该算法的高效率碰撞方法，能够在合理的时间内生成两个摘要结果相同的输入比特流，但并不能指定这两个输入流中的某一个，更不代表碰撞产生的比特流就会是原来的输入源。 由这两个特点可见，摘要的意义是在源信息不泄漏的前提下辨别其真伪。易变性保证了从公开的特征上可以甄别出是否来自于源信息，不可逆性保证了从公开的特征并不会暴露出源信息，这与今天用做身份甄别的指纹、面容和虹膜的生物特征是具有高度可比性的。在一些场合中，摘要也会被借用来做加密（如保密中介绍的慢哈希 Bcrypt 算法）和签名（如 JWT 签名中的 HMAC SHA256 算法），但在严格意义上看，摘要与这两者是有本质的区别。 加密与签名加密与摘要的本质区别在于加密是可逆的，逆过程就是解密。 在经典密码学时代，加密的安全主要是依靠机密性来保证的，即依靠保护加密算法或算法的执行参数不被泄漏来保障信息的安全。而现代密码学不依靠机密性，加解密算法都是完全公开的，安全建立在特定问题的计算复杂度之上，具体是指算法根据输入端计算输出结果耗费的算力资源很小，但根据输出端的结果反过来推算原本的输入，耗费的算力就极其庞大。 根据加密与解密是否采用同一个密钥，现代密码学算法可分为对称加密算法和非对称加密两大类型 对称加密算法 定义：对称加密算法使用相同的密钥进行加密和解密操作。 特点： 加密速度快，适合大量数据的加密。 密钥管理较为复杂，因为通信双方需要安全地共享同一个密钥。 常见算法： AES（高级加密标准） DES（数据加密标准） 3DES（三重数据加密标准） 非对称加密算法 定义：非对称加密算法使用一对密钥，即公钥和私钥。公钥用于加密，私钥用于解密；或者私钥用于签名，公钥用于验证签名。 特点： 安全性高，因为私钥不需要传输，只有持有者知道。 加密速度相对较慢，适合小量数据的加密或用于密钥交换。 常见算法： RSA（Rivest-Shamir-Adleman） ECC（椭圆曲线密码学） DSA（数字签名算法","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%9E%B6%E6%9E%84%E5%AE%89%E5%85%A8%E6%80%A7/%E4%BC%A0%E8%BE%93/"},{"title":"凭证（Credentials）","text":"凭证（Credentials）系统如何保证它与用户之间的承诺是双方当时真实意图的体现，是准确、完整且不可抵赖的？ Cookie-SessionHTTP 协议是一种无状态的传输协议，我们希望 HTTP 能有一种手段，让服务器至少有办法能够区分出发送请求的用户是谁。 为了实现这个目的，RFC 6265规范定义了 HTTP 的状态管理机制，在 HTTP 协议中增加了 Set-Cookie 指令，该指令的含义是以键值对的方式向客户端发送一组信息，此信息将在此后一段时间内的每次 HTTP 请求中，以名为 Cookie 的 Header 附带着重新发回给服务端，以便服务端区分来自不同客户端的请求。一个典型的 Set-Cookie 指令如下所示： 1Set-Cookie: id=icyfenix; Expires=Wed, 21 Feb 2020 07:28:00 GMT; Secure; HttpOnly 收到该指令以后，客户端再对同一个域的请求中就会自动附带有键值对信息id=icyfenix，譬如以下代码所示： 123GET /index.html HTTP/2.0Host: icyfenix.cnCookie: id=icyfenix 根据每次请求传到服务端的 Cookie，服务器就能分辨出请求来自于哪一个用户。由于 Cookie 是放在请求头上的，属于额外的传输负担，不应该携带过多的内容，而且放在 Cookie 中传输也并不安全，容易被中间人窃取或被篡改，所以通常是不会像例子中设置id=icyfenix这样的明文信息。一般来说，系统会把状态信息保存在服务端，在 Cookie 里只传输的是一个无字面意义的、不重复的字符串，习惯上以sessionid或者jsessionid为名，服务器拿这个字符串为 Key，在内存中开辟一块空间，以 Key/Entity 的结构存储每一个在线用户的上下文状态，再辅以一些超时自动清理之类的管理措施。这种服务端的状态管理机制就是今天大家非常熟悉的 Session，Cookie-Session 也是最传统但今天依然广泛应用于大量系统中的，由服务端与客户端联动来完成的状态管理机制。 Cookie-Session 方案在“安全性”上其实是有一定先天优势的：状态信息都存储于服务器，只要依靠客户端的同源策略和 HTTPS 的传输层安全，保证 Cookie 中的键值不被窃取而出现被冒认身份的情况，就能完全规避掉上下文信息在传输过程中被泄漏和篡改的风险。 Cookie-Session 方案的另一大优点是服务端有主动的状态管理能力，可根据自己的意愿随时修改、清除任意上下文信息，譬如很轻易就能实现强制某用户下线的这样功能。 Session-Cookie 在单节点的单体服务环境中是最合适的方案，但当需要水平扩展服务能力，要部署集群时就开始面临麻烦了，由于 Session 存储在服务器的内存中，当服务器水平拓展成多节点时，设计者必须在以下三种方案中选择其一： 牺牲集群的一致性（Consistency），让均衡器采用亲和式的负载均衡算法，譬如根据用户 IP 或者 Session 来分配节点，每一个特定用户发出的所有请求都一直被分配到其中某一个节点来提供服务，每个节点都不重复地保存着一部分用户的状态，如果这个节点崩溃了，里面的用户状态便完全丢失。 牺牲集群的可用性（Availability），让各个节点之间采用复制式的 Session，每一个节点中的 Session 变动都会发送到组播地址的其他服务器上，这样某个节点崩溃了，不会中断对某个用户的服务，但 Session 之间组播复制的同步代价高昂，节点越多时，同步成本越高。 牺牲集群的分区容忍性（Partition Tolerance），让普通的服务节点中不再保留状态，将上下文集中放在一个所有服务节点都能访问到的数据节点中进行存储。此时的矛盾是数据节点就成为了单点，一旦数据节点损坏或出现网络分区，整个集群都不再能提供服务。 JWTJWT（JSON Web Token）定义于RFC 7519标准之中，是目前广泛使用的一种令牌格式，尤其经常与 OAuth2 配合应用于分布式的、涉及多方的应用系统中。 左边的字符串呈现了 JWT 令牌的本体。它最常见的使用方式是附在名为 Authorization 的 Header 发送给服务端， 1234GET /restful/products/1 HTTP/1.1Host: icyfenix.cnConnection: keep-aliveAuthorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX25hbWUiOiJpY3lmZW5peCIsInNjb3BlIjpbIkFMTCJdLCJleHAiOjE1ODQ5NDg5NDcsImF1dGhvcml0aWVzIjpbIlJPTEVfVVNFUiIsIlJPTEVfQURNSU4iXSwianRpIjoiOWQ3NzU4NmEtM2Y0Zi00Y2JiLTk5MjQtZmUyZjc3ZGZhMzNkIiwiY2xpZW50X2lkIjoiYm9va3N0b3JlX2Zyb250ZW5kIiwidXNlcm5hbWUiOiJpY3lmZW5peCJ9.539WMzbjv63wBtx4ytYYw_Fo1ECG_9vsgAn8bheflL8 右边的状态信息是对令牌使用 Base64URL 转码后得到的明文，请特别注意是明文，JWT 只解决防篡改的问题，并不解决防泄漏的问题，因此令牌默认是不加密的。令牌的组成部分： 第一部分是令牌头（Header) 它描述了令牌的类型（统一为 typ:JWT）以及令牌签名的算法，示例中 HS256 为 HMAC SHA256 算法的缩写，其他各种系统支持的签名算法可以参考https://jwt.io/网站所列。 1234{ &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;} 令牌的第二部分是负载（Payload） 这是令牌真正需要向服务端传递的信息。针对认证问题，负载至少应该包含能够告知服务端“这个用户是谁”的信息，针对授权问题，令牌至少应该包含能够告知服务端“这个用户拥有什么角色/权限”的信息。JWT 的负载部分是可以完全自定义的，根据具体要解决的问题不同，设计自己所需要的信息，只是总容量不能太大，毕竟要受到 HTTP Header 大小的限制。 12345678910111213{ &quot;username&quot;: &quot;icyfenix&quot;, &quot;authorities&quot;: [ &quot;ROLE_USER&quot;, &quot;ROLE_ADMIN&quot; ], &quot;scope&quot;: [ &quot;ALL&quot; ], &quot;exp&quot;: 1584948947, &quot;jti&quot;: &quot;9d77586a-3f4f-4cbb-9924-fe2f77dfa33d&quot;, &quot;client_id&quot;: &quot;bookstore_frontend&quot;} JWT 在 RFC 7519 中推荐（非强制约束）了七项声明名称（Claim Name），如有需要用到这些内容，建议字段名与官方的保持一致： iss（Issuer）：签发人。 exp（Expiration Time）：令牌过期时间。 sub（Subject）：主题。 aud （Audience）：令牌受众。 nbf （Not Before）：令牌生效时间。 iat （Issued At）：令牌签发时间。 jti （JWT ID）：令牌编号。 令牌的第三部分是签名（Signature） 令牌的第三部分是签名（Signature），签名的意思是：使用在对象头中公开的特定签名算法，通过特定的密钥（Secret，由服务器进行保密，不能公开）对前面两部分内容进行加密计算 1HMACSHA256(base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload) , secret) 签名的意义在于确保负载中的信息是可信的、没有被篡改的，也没有在传输过程中丢失任何信息。因为被签名的内容哪怕发生了一个字节的变动，也会导致整个签名发生显著变化。此外，由于签名这件事情只能由认证授权服务器完成（只有它知道 Secret），任何人都无法在篡改后重新计算出合法的签名值，所以服务端才能够完全信任客户端传上来的 JWT 中的负载信息。 JWT 令牌是多方系统中一种优秀的凭证载体，它不需要任何一个服务节点保留任何一点状态信息，就能够保障认证服务与用户之间的承诺是双方当时真实意图的体现，是准确、完整、不可篡改、且不可抵赖的。同时，由于 JWT 本身可以携带少量信息，这十分有利于 RESTful API 的设计，能够较容易地做成无状态服务，在做水平扩展时就不需要像前面 Cookie-Session 方案那样考虑如何部署的问题。现实中也确实有一些项目直接采用 JWT 来承载上下文来实现完全无状态的服务端，这能获得任意加入或移除服务节点的巨大便利，天然具有完美的水平扩缩能力。尽管大型系统中只使用 JWT 来维护上下文状态，服务端完全不持有状态是不太现实的，不过将热点的服务单独抽离出来做成无状态，仍是一种有效提升系统吞吐能力的架构技巧。但是，JWT 也并非没有缺点的完美方案，它存在着以下几个经常被提及的缺点： 令牌难以主动失效：JWT 令牌一旦签发，理论上就和认证服务器再没有什么瓜葛了，在到期之前就会始终有效，除非服务器部署额外的逻辑去处理失效问题，这对某些管理功能的实现是很不利的。譬如一种颇为常见的需求是：要求一个用户只能在一台设备上登录，在 B 设备登录后，之前已经登录过的 A 设备就应该自动退出。如果采用 JWT，就必须设计一个“黑名单”的额外的逻辑，用来把要主动失效的令牌集中存储起来，而无论这个黑名单是实现在 Session、Redis 或者数据库中，都会让服务退化成有状态服务，降低了 JWT 本身的价值，但黑名单在使用 JWT 时依然是很常见的做法，需要维护的黑名单一般是很小的状态量，许多场景中还是有存在价值的 相对更容易遭受重放攻击：首先说明 Cookie-Session 也是有重放攻击问题的，只是因为 Session 中的数据控制在服务端手上，应对重放攻击会相对主动一些。要在 JWT 层面解决重放攻击需要付出比较大的代价，无论是加入全局序列号（HTTPS 协议的思路）、Nonce 字符串（HTTP Digest 验证的思路）、挑战应答码（当下网银动态令牌的思路）、还是缩短令牌有效期强制频繁刷新令牌，在真正应用起来时都很麻烦。真要处理重放攻击，建议的解决方案是在信道层次（譬如启用 HTTPS）上解决，而不提倡在服务层次（譬如在令牌或接口其他参数上增加额外逻辑）上解决。 只能携带相当有限的数据：HTTP 协议并没有强制约束 Header 的最大长度，但是，各种服务器、浏览器都会有自己的约束，譬如 Tomcat 就要求 Header 最大不超过 8KB，而在 Nginx 中则默认为 4KB，因此在令牌中存储过多的数据不仅耗费传输带宽，还有额外的出错风险。 必须考虑令牌在客户端如何存储：严谨地说，这个并不是 JWT 的问题而是系统设计的问题。如果授权之后，操作完关掉浏览器就结束了，那把令牌放到内存里面，压根不考虑持久化才是最理想的方案。但并不是谁都能忍受一个网站关闭之后下次就一定强制要重新登录的。这样的话，想想客户端该把令牌存放到哪里？Cookie？localStorage？Indexed DB？它们都有泄漏的可能，而令牌一旦泄漏，别人就可以冒充用户的身份做任何事情。 无状态也不总是好的：这个其实不也是 JWT 的问题。如果不能想像无状态会有什么不好的话，我给你提个需求：请基于无状态 JWT 的方案，做一个在线用户实时统计功能。兄弟，难搞哦。","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%9E%B6%E6%9E%84%E5%AE%89%E5%85%A8%E6%80%A7/%E5%87%AD%E8%AF%81/"},{"title":"授权（ Authorization）","text":"授权（ Authorization）系统如何控制一个用户该看到哪些数据、能操作哪些功能？ 确保授权的过程可靠：对于单一系统来说，授权的过程是比较容易做到可控的，以前很多语境上提到授权，实质上讲的都是访问控制，理论上两者是应该分开的。而在涉及多方的系统中，授权过程则是一个比较困难却必须严肃对待的问题：如何既让第三方系统能够访问到所需的资源，又能保证其不泄露用户的敏感数据呢？常用的多方授权协议主要有 OAuth2 和 SAML 2.0（两个协议涵盖的功能并不是直接对等的）。 确保授权的结果可控：授权的结果用于对程序功能或者资源的访问控制（Access Control），成理论体系的权限控制模型有很多，譬如自主访问控制（Discretionary Access Control，DAC）、强制访问控制（Mandatory Access Control，MAC）、基于属性的访问控制（Attribute-Based Access Control，ABAC），还有最为常用的基于角色的访问控制（Role-Based Access Control，RBAC）。 RBAC所有的访问控制模型，实质上都是在解决同一个问题：“谁（User）拥有什么权限（Authority）去操作（Operation）哪些资源（Resource）”。 RBAC-1 模型的角色权限继承关系。譬如描述开发经理应该和开发人员一样具有代码提交的权限，描述开发人员都应该和任何公司员工一样具有食堂就餐的权限，就可以直接将食堂就餐赋予公司员工的角色上，把代码提交赋予到开发人员的角色上，再让开发人员的角色从公司员工派生，开发经理的角色从开发人员中派生即可。 RBAC-2 模型的角色职责分离关系。互斥性要求权限被赋予角色时，或角色被赋予用户时应遵循的强制性职责分离规定。举个例子，角色的互斥约束可限制同一用户只能分配到一组互斥角色集合中至多一个角色，譬如不能让同一名员工既当会计，也当出纳，否则资金安全无法保证。角色的基数约束可限制某一个用户拥有的最大角色数目，譬如不能让同一名员工从产品、设计、开发、测试全部包揽，否则产品质量无法保证。 OAuth2 OAuth2 是面向于解决第三方应用（Third-Party Application）的认证授权协议。如果你的系统并不涉及第三方，那引入 OAuth2 其实并无必要。 OAuth2 给出了多种解决办法，这些办法的共同特征是以令牌（Token）代替用户密码作为授权的凭证。有了令牌之后，哪怕令牌被泄漏，也不会导致密码的泄漏；令牌上可以设定访问资源的范围以及时效性；每个应用都持有独立的令牌，哪个失效都不会波及其他。这样上面提出的三个问题就都解决了。有了一层令牌之后，整个授权的流程如图 第三方应用（Third-Party Application）：需要得到授权访问我资源的那个应用 授权服务器（Authorization Server）：能够根据我的意愿提供授权（授权之前肯定已经进行了必要的认证过程，但它与授权可以没有直接关系）的服务器。 资源服务器（Resource Server）：能够提供第三方应用所需资源的服务器，它与认证服务可以是相同的服务器，也可以是不同的服务器 资源所有者（Resource Owner）： 拥有授权权限的人，即此场景中的“我”。 操作代理（User Agent）：指用户用来访问服务器的工具，对于人类用户来说，这个通常是指浏览器，但在微服务中一个服务经常会作为另一个服务的用户，此时指的可能就是 HttpClient、RPCClient 或者其他访问途径。 用令牌代替密码”确实是解决问题的好方法，但这充其量只能算个思路，距离可实施的步骤还是不够具体的，时序图中的“要求/同意授权”、“要求/同意发放令牌”、“要求/同意开放资源”几个服务请求、响应该如何设计，这就是执行步骤的关键了。对此，OAuth2 一共提出了四种不同的授权方式（这也是 OAuth2 复杂烦琐的主要原因），分别为： 授权码模式（Authorization Code） 隐式授权模式（Implicit） 密码模式（Resource Owner Password Credentials） 客户端模式（Client Credentials） 授权码模式授权码模式是四种模式中最严（luō）谨（suō）的，它考虑到了几乎所有敏感信息泄漏的预防和后果。具体步骤的时序如图 开始进行授权过程以前，第三方应用先要到授权服务器上进行注册，所谓注册，是指向认证服务器提供一个域名地址，然后从授权服务器中获取 ClientID 和 ClientSecret，以便能够顺利完成如下授权过程： 第三方应用将资源所有者（用户）导向授权服务器的授权页面，并向授权服务器提供 ClientID 及用户同意授权后的回调 URI，这是一次客户端页面转向。 授权服务器根据 ClientID 确认第三方应用的身份，用户在授权服务器中决定是否同意向该身份的应用进行授权，用户认证的过程未定义在此步骤中，在此之前应该已经完成。 如果用户同意授权，授权服务器将转向第三方应用在第 1 步调用中提供的回调 URI，并附带上一个授权码和获取令牌的地址作为参数，这是第二次客户端页面转向。 第三方应用通过回调地址收到授权码，然后将授权码与自己的 ClientSecret 一起作为参数，通过服务器向授权服务器提供的获取令牌的服务地址发起请求，换取令牌。该服务器的地址应与注册时提供的域名处于同一个域中。 授权服务器核对授权码和 ClientSecret，确认无误后，向第三方应用授予令牌。令牌可以是一个或者两个，其中必定要有的是访问令牌（Access Token），可选的是刷新令牌（Refresh Token）。访问令牌用于到资源服务器获取资源，有效期较短，刷新令牌用于在访问令牌失效后重新获取，有效期较长。 资源服务器根据访问令牌所允许的权限，向第三方应用提供资源。 这个过程设计，已经考虑到了几乎所有合理的意外情况，再举几个最容易遇到的意外状况，以便能够更好地理解为何要这样设计 OAuth2。 会不会有其他应用冒充第三方应用骗取授权？ ClientID 代表一个第三方应用的“用户名”，这项信息是可以完全公开的。但 ClientSecret 应当只有应用自己才知道，这个代表了第三方应用的“密码”。在第 5 步发放令牌时，调用者必须能够提供 ClientSecret 才能成功完成。只要第三方应用妥善保管好 ClientSecret，就没有人能够冒充它。 为什么要先发放授权码，再用授权码换令牌？ 这是因为客户端转向（通常就是一次 HTTP 302 重定向）对于用户是可见的，换而言之，授权码可能会暴露给用户以及用户机器上的其他程序，但由于用户并没有 ClientSecret，光有授权码也是无法换取到令牌的，所以避免了令牌在传输转向过程中被泄漏的风险。 为什么要设计一个时限较长的刷新令牌和时限较短的访问令牌？不能直接把访问令牌的时间调长吗？ 这是为了缓解 OAuth2 在实际应用中的一个主要缺陷，通常访问令牌一旦发放，除非超过了令牌中的有效期，否则很难（需要付出较大代价）有其他方式让它失效，所以访问令牌的时效性一般设计的比较短，譬如几个小时，如果还需要继续用，那就定期用刷新令牌去更新，授权服务器就可以在更新过程中决定是否还要继续给予授权。 隐式授权模式隐式授权省略掉了通过授权码换取令牌的步骤，整个授权过程都不需要服务端支持，一步到位。代价是在隐式授权中，授权服务器不会再去验证第三方应用的身份，因为已经没有应用服务器了，ClientSecret 没有人保管，就没有存在的意义了。但其实还是会限制第三方应用的回调 URI 地址必须与注册时提供的域名一致，尽管有可能被 DNS 污染之类的攻击所攻破，但仍算是尽可能努力一下。同样的原因，也不能避免令牌暴露给资源所有者，不能避免用户机器上可能意图不轨的其他程序、HTTP 的中间人攻击等风险了。 在时序图所示的交互过程里，隐式模式与授权码模式的显著区别是授权服务器在得到用户授权后，直接返回了访问令牌，这显著地降低了安全性，但 OAuth2 仍然努力尽可能地做到相对安全，譬如在前面提到的隐式授权中，尽管不需要用到服务端，但仍然需要在注册时提供回调域名，此时会要求该域名与接受令牌的服务处于同一个域内。此外，同样基于安全考虑，在隐式模式中明确禁止发放刷新令牌。 密码模式授权码模式和隐私模式属于纯粹的授权模式，它们与认证没有直接的联系，如何认证用户的真实身份是与进行授权互相独立的过程。但在密码模式里，认证和授权就被整合成了同一个过程了。 密码模式原本的设计意图是仅限于用户对第三方应用是高度可信任的场景中使用，因为用户需要把密码明文提供给第三方应用，第三方以此向授权服务器获取令牌。这种高度可信的第三方是极为较罕见的，尽管介绍 OAuth2 的材料中，经常举的例子是“操作系统作为第三方应用向授权服务器申请资源”，但真实应用中极少遇到这样的情况，合理性依然十分有限。 如果要采用密码模式，那“第三方”属性就必须弱化，把“第三方”视作是系统中与授权服务器相对独立的子模块，在物理上独立于授权服务器部署，但是在逻辑上与授权服务器仍同属一个系统，这样将认证和授权一并完成的密码模式才会有合理的应用场景。 客户端模式 客户端模式是指第三方应用（行文一致考虑，还是继续沿用这个称呼）以自己的名义，向授权服务器申请资源许可。此模式通常用于管理操作或者自动处理类型的场景中。 微服务架构并不提倡同一个系统的各服务间有默认的信任关系，所以服务之间调用也需要先进行认证授权，然后才能通信。此时，客户端模式便是一种常用的服务间认证授权的解决方案。","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%9E%B6%E6%9E%84%E5%AE%89%E5%85%A8%E6%80%A7/%E6%8E%88%E6%9D%83/"},{"title":"认证（Authentication）","text":"认证（Authentication）系统如何正确分辨出操作用户的真实身份？ 认证”是解决“你是谁？”的问题，但这里的“你”并不一定是指人（真不是在骂你），也可能是指外部的代码，即第三方的类库或者服务。 认证的标准主流的三种认证方式，具体含义和应用场景列举如下: 通信信道上的认证：你和我建立通信连接之前，要先证明你是谁。在网络传输（Network）场景中的典型是基于 SSL/TLS 传输安全层的认证。 通信协议上的认证：你请求获取我的资源之前，要先证明你是谁。在互联网（Internet）场景中的典型是基于 HTTP 协议的认证。 通信内容上的认证：你使用我提供的服务之前，要先证明你是谁。在万维网（World Wide Web）场景中的典型是基于 Web 内容的认证。 HTTP 认证IETF 在RFC 7235中定义了 HTTP 协议的通用认证框架，要求所有支持 HTTP 协议的服务器，在未授权的用户意图访问服务端保护区域的资源时，应返回 401 Unauthorized 的状态码，同时应在响应报文头里附带以下两个分别代表网页认证和代理认证的 Header 之一，告知客户端应该采取何种方式产生能代表访问者身份的凭证信息： 12WWW-Authenticate: &lt;认证方案&gt; realm=&lt;保护区域的描述信息&gt;Proxy-Authenticate: &lt;认证方案&gt; realm=&lt;保护区域的描述信息&gt; 接收到该响应后，客户端必须遵循服务端指定的认证方案，在请求资源的报文头中加入身份凭证信息，由服务端核实通过后才会允许该请求正常返回，否则将返回 403 Forbidden 错误。请求头报文应包含以下 Header 项之一： 12Authorization: &lt;认证方案&gt; &lt;凭证内容&gt;Proxy-Authorization: &lt;认证方案&gt; &lt;凭证内容&gt; HTTP 认证框架提出认证方案是希望能把认证“要产生身份凭证”的目的与“具体如何产生凭证”的实现分离开来，无论客户端通过生物信息（指纹、人脸）、用户密码、数字证书抑或其他方式来生成凭证，都属于是如何生成凭证的具体实现，都可以包容在 HTTP 协议预设的框架之内。 HTTP Basic Auth 原理：客户端在请求头中添加 Authorization 字段，格式为 Basic ，其中 是用户名和密码拼接后经过 Base64 编码的字符串。 优点：简单易实现。 缺点：安全性较低，因为 Base64 编码不是加密，且用户名和密码在网络上传输时容易被截获。建议与 HTTPS 一起使用。 HTTP Digest Auth 原理：客户端发送未认证的请求，服务器返回 401 状态码并要求提供凭证。客户端再次发送带有 Authorization 头的请求，其中包含经过哈希处理的凭证。 优点：比 Basic Auth 更安全，因为凭证是通过哈希算法处理的。 缺点：仍然存在一些安全漏洞，并且实现较为复杂。 Bearer Token (OAuth 2.0) 原理：客户端获取一个令牌（token），并在每次请求时将该令牌放入 Authorization 头中，格式为 Bearer 。 优点：高度灵活，支持多种授权类型（如授权码、隐式、客户端凭据等），广泛用于 API 安全。 缺点：需要额外的基础设施来管理令牌的发放和验证。 Mutual TLS (mTLS) 原理：不仅服务器对客户端进行身份验证，客户端也对服务器进行身份验证。双方都使用数字证书来进行双向认证。 优点：非常安全，适用于高安全需求的环境。 缺点：配置和管理复杂，涉及证书颁发机构（CA）和证书管理。 API Key 原理：客户端在请求头或查询参数中附加一个唯一的 API 密钥。 优点：简单易用，适合轻量级应用。 缺点：安全性较低，密钥容易泄露，建议与 IP 白名单等其他安全措施结合使用。 Session-Based Authentication 原理：用户登录成功后，服务器生成一个会话 ID 并存储在服务器端，客户端通过 Cookie 或请求头传递该会话 ID。 优点：适合 Web 应用，用户体验好。 缺点：需要管理会话状态，不适合无状态的 RESTful API。 Web 认证依靠内容而不是传输协议来实现的认证方式，在万维网里被称为“Web 认证”，由于实现形式上登录表单占了绝对的主流，因此通常也被称为“表单认证”（Form Authentication）。 表单认证与 HTTP 认证不见得是完全对立的，两者有不同的关注点，可以结合使用。但认证的整个交互过程遵循 OAuth 2 规范的密码模式。 WebAuthn 规范涵盖了“注册”与“认证”两大流程，先来介绍注册流程，它大致可以分为以下步骤： 用户进入系统的注册页面，这个页面的格式、内容和用户注册时需要填写的信息均不包含在 WebAuthn 标准的定义范围内。 当用户填写完信息，点击“提交注册信息”的按钮后，服务端先暂存用户提交的数据，生成一个随机字符串（规范中称为 Challenge）和用户的 UserID（在规范中称作凭证 ID），返回给客户端。 客户端的 WebAuthn API 接收到 Challenge 和 UserID，把这些信息发送给验证器（Authenticator），验证器可理解为用户设备上 TouchID、FaceID、实体密钥等认证设备的统一接口。 验证器提示用户进行验证，如果支持多种认证设备，还会提示用户选择一个想要使用的设备。验证的结果是生成一个密钥对（公钥和私钥），由验证器存储私钥、用户信息以及当前的域名。然后使用私钥对 Challenge 进行签名，并将签名结果、UserID 和公钥一起返回客户端。 浏览器将验证器返回的结果转发给服务器。 服务器核验信息，检查 UserID 与之前发送的是否一致，并用公钥解密后得到的结果与之前发送的 Challenge 相比较，一致即表明注册通过，由服务端存储该 UserID 对应的公钥。 登录流程与注册流程类似： 用户访问登录页面，填入用户名后即可点击登录按钮。 服务器返回随机字符串 Challenge、用户 UserID。 浏览器将 Challenge 和 UserID 转发给验证器。 验证器提示用户进行认证操作。由于在注册阶段验证器已经存储了该域名的私钥和用户信息，所以如果域名和用户都相同的话，就不需要生成密钥对了，直接以存储的私钥加密 Challenge，然后返回给浏览器。 服务端接收到浏览器转发来的被私钥加密的 Challenge，以此前注册时存储的公钥进行解密，如果解密成功则宣告登录成功。 WebAuthn 采用非对称加密的公钥、私钥替代传统的密码，这是非常理想的认证方案，私钥是保密的，只有验证器需要知道它，连用户本人都不需要知道，也就没有人为泄漏的可能；公钥是公开的，可以被任何人看到或存储。公钥可用于验证私钥生成的签名，但不能用来签名，除了得知私钥外，没有其他途径能够生成可被公钥验证为有效的签名，这样服务器就可以通过公钥是否能够解密来判断最终用户的身份是否合法。 WebAuthn 还一揽子地解决了传统密码在网络传输上的风险，无论密码是否客户端进行加密、如何加密，对防御中间人攻击来说都是没有意义的。更值得夸赞的是 WebAuthn 为登录过程带来极大的便捷性，不仅注册和验证的用户体验十分优秀，而且彻底避免了用户在一个网站上泄漏密码，所有使用相同密码的网站都受到攻击的问题，这个优点使得用户无须再为每个网站想不同的密码。","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%9E%B6%E6%9E%84%E5%AE%89%E5%85%A8%E6%80%A7/%E8%AE%A4%E8%AF%81/"},{"title":"CAP理论","text":"CAP 定理（Consistency、Availability、Partition Tolerance Theorem）,这个定理里描述了一个分布式的系统中，涉及共享数据问题时，以下三个特性最多只能同时满足其中两个： 一致性（Consistency）：代表数据在任何时刻、任何分布式节点中所看到的都是符合预期的。 可用性（Availability）：代表系统不间断地提供服务的能力，理解可用性要先理解与其密切相关两个指标：可靠性（Reliability）和可维护性（Serviceability）。可靠性使用平均无故障时间（Mean Time Between Failure，MTBF）来度量；可维护性使用平均可修复时间（Mean Time To Repair，MTTR）来度量。可用性衡量系统可以正常使用的时间与总时间之比，其表征为：A=MTBF/（MTBF+MTTR），即可用性是由可靠性和可维护性计算得出的比例值，譬如 99.9999%可用，即代表平均年故障修复时间为 32 秒。 分区容忍性（Partition Tolerance）：代表分布式环境中部分节点因网络原因而彼此失联后，即与其他节点形成“网络分区”时，系统仍能正确地提供服务的能力。 例如一个来自最终用户的交易请求，将交由账号、商家和仓库服务集群中某一个节点来完成响应，每一个单独的服务节点都有自己的数据库： 如果该变动信息没有及时同步给其他账号节点，将导致有可能发生用户购买另一商品时，被分配给到另一个节点处理，由于看到账号上有不正确的余额而错误地发生了原本无法进行的交易，此为一致性问题。 如果由于要把该变动信息同步给其他账号节点，必须暂时停止对该用户的交易服务，直至数据同步一致后再重新恢复，将可能导致用户在下一次购买商品时，因系统暂时无法提供服务而被拒绝交易，此为可用性问题。 如果由于账号服务集群中某一部分节点，因出现网络问题，无法正常与另一部分节点交换账号变动信息，此时服务集群中无论哪一部分节点对外提供的服务都可能是不正确的，整个集群能否承受由于部分节点之间的连接中断而仍然能够正确地提供服务，此为分区容忍性。 舍弃 C、A、P 时所带来的不同影响： 如果放弃分区容忍性（CA without P），意味着我们将假设节点之间通信永远是可靠的。永远可靠的通信在分布式系统中必定不成立的，这不是你想不想的问题，而是只要用到网络来共享数据，分区现象就会始终存在。在现实中，最容易找到放弃分区容忍性的例子便是传统的关系数据库集群，这样的集群虽然依然采用由网络连接的多个节点来协同工作，但数据却不是通过网络来实现共享的。以 Oracle 的 RAC 集群为例，它的每一个节点均有自己独立的 SGA、重做日志、回滚日志等部件，但各个节点是通过共享存储中的同一份数据文件和控制文件来获取数据的，通过共享磁盘的方式来避免出现网络分区。因而 Oracle RAC 虽然也是由多个实例组成的数据库，但它并不能称作是分布式数据库。 如果放弃可用性（CP without A），意味着我们将假设一旦网络发生分区，节点之间的信息同步时间可以无限制地延长，此时，问题相当于退化到前面“全局事务”中讨论的一个系统使用多个数据源的场景之中，我们可以通过 2PC/3PC 等手段，同时获得分区容忍性和一致性。在现实中，选择放弃可用性的 CP 系统情况一般用于对数据质量要求很高的场合中，除了 DTP 模型的分布式数据库事务外，著名的 HBase 也是属于 CP 系统，以 HBase 集群为例，假如某个 RegionServer 宕机了，这个 RegionServer 持有的所有键值范围都将离线，直到数据恢复过程完成为止，这个过程要消耗的时间是无法预先估计的。 如果放弃一致性（AP without C），意味着我们将假设一旦发生分区，节点之间所提供的数据可能不一致。选择放弃一致性的 AP 系统目前是设计分布式系统的主流选择，因为 P 是分布式网络的天然属性，你再不想要也无法丢弃；而 A 通常是建设分布式的目的，如果可用性随着节点数量增加反而降低的话，很多分布式系统可能就失去了存在的价值，除非银行、证券这些涉及金钱交易的服务，宁可中断也不能出错，否则多数系统是不能容忍节点越多可用性反而越低的。目前大多数 NoSQL 库和支持分布式的缓存框架都是 AP 系统，以 Redis 集群为例，如果某个 Redis 节点出现网络分区，那仍不妨碍各个节点以自己本地存储的数据对外提供缓存服务，但这时有可能出现请求分配到不同节点时返回给客户端的是不一致的数据。","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/CAP%E7%90%86%E8%AE%BA%20/"},{"title":"共享事务","text":"共享事务（Share Transaction）是指多个服务共用同一个数据源。 为了实现共享事务，就必须新增一个“交易服务器”的中间角色，无论是用户服务、商家服务还是仓库服务，它们都通过同一台交易服务器来与数据库打交道。如果将交易服务器的对外接口按照 JDBC 规范来实现的话，那它完全可以视为是一个独立于各个服务的远程数据库连接池，或者直接作为数据库代理来看待。此时三个服务所发出的交易请求就有可能做到交由交易服务器上的同一个数据库连接，通过本地事务的方式完成。 交易服务器根据不同服务节点传来的同一个事务 ID，使用同一个数据库连接来处理跨越多个服务的交易事务","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/%E5%85%B1%E4%BA%AB%E4%BA%8B%E5%8A%A1/"},{"title":"分布式事务","text":"分布式事务（Distributed Transaction）特指多个服务同时访问多个数据源的事务处理机制。 分布式事务中没有一揽子包治百病的解决办法，因地制宜地选用合适的事务处理方案才是唯一有效的做法。 我们在 CAP、ACID 中讨论的一致性称为“强一致性”（Strong Consistency），有时也称为“线性一致性”（Linearizability，通常是在讨论共识算法的场景中），而把牺牲了 C 的 AP 系统又要尽可能获得正确的结果的行为被称为“最终一致性”（Eventual Consistency），它是指：如果数据在一段时间之内没有被另外的操作所更改，那它最终将会达到与强一致性过程相同的结果，有时候面向最终一致性的算法也被称为“乐观复制算法”。 把使用 ACID 的事务称为“刚性事务”，下面将要介绍几种分布式事务的常见做法统称为“柔性事务”。 可靠事件队列 （最大努力交付） 最终用户向 Fenix’s Bookstore 发送交易请求：购买一本价值 100 元的《深入理解 Java 虚拟机》。 Fenix’s Bookstore 首先应对用户账号扣款、商家账号收款、库存商品出库这三个操作有一个出错概率的先验评估，根据出错概率的大小来安排它们的操作顺序，这种评估一般直接体现在程序代码中，有一些大型系统也可能会实现动态排序。譬如，根据统计，最有可能的出现的交易异常是用户购买了商品，但是不同意扣款，或者账号余额不足；其次是仓库发现商品库存不够，无法发货；风险最低的是收款，如果到了商家收款环节，一般就不会出什么意外了。那顺序就应该安排成最容易出错的最先进行，即：账号扣款 → 仓库出库 → 商家收款。 账号服务进行扣款业务，如扣款成功，则在自己的数据库建立一张消息表，里面存入一条消息：“事务 ID：某 UUID，扣款：100 元（状态：已完成），仓库出库《深入理解 Java 虚拟机》：1 本（状态：进行中），某商家收款：100 元（状态：进行中）”，注意，这个步骤中“扣款业务”和“写入消息”是使用同一个本地事务写入账号服务自己的数据库的。 在系统中建立一个消息服务，定时轮询消息表，将状态是“进行中”的消息同时发送到库存和商家服务节点中去（也可以串行地发，即一个成功后再发送另一个，但在我们讨论的场景中没必要）。这时候可能产生以下几种情况。 商家和仓库服务都成功完成了收款和出库工作，向用户账号服务器返回执行结果，用户账号服务把消息状态从“进行中”更新为“已完成”。整个事务宣告顺利结束，达到最终一致性的状态。 商家或仓库服务中至少一个因网络原因，未能收到来自用户账号服务的消息。此时，由于用户账号服务器中存储的消息状态一直处于“进行中”，所以消息服务器将在每次轮询的时候持续地向未响应的服务重复发送消息。这个步骤的可重复性决定了所有被消息服务器发送的消息都必须具备幂等性，通常的设计是让消息带上一个唯一的事务 ID，以保证一个事务中的出库、收款动作会且只会被处理一次。 商家或仓库服务有某个或全部无法完成工作，譬如仓库发现《深入理解 Java 虚拟机》没有库存了，此时，仍然是持续自动重发消息，直至操作成功（譬如补充了新库存），或者被人工介入为止。由此可见，可靠事件队列只要第一步业务完成了，后续就没有失败回滚的概念，只许成功，不许失败。 商家和仓库服务成功完成了收款和出库工作，但回复的应答消息因网络原因丢失，此时，用户账号服务仍会重新发出下一条消息，但因操作具备幂等性，所以不会导致重复出库和收款，只会导致商家、仓库服务器重新发送一条应答消息，此过程重复直至双方网络通信恢复正常。 也有一些支持分布式事务的消息框架，如 RocketMQ，原生就支持分布式事务操作，这时候上述情况 2、4 也可以交由消息框架来保障。 可靠事件队列还有一种更普通的形式，被称为“最大努力一次提交”（Best-Effort 1PC），指的就是将最有可能出错的业务以本地事务的方式完成后，采用不断重试的方式（不限于消息系统）来促使同一个分布式事务中的其他关联业务全部完成。 可靠消息队列虽然能保证最终的结果是相对可靠的，过程也足够简单（相对于 TCC 来说），但整个过程完全没有任何隔离性可言，有一些业务中隔离性是无关紧要的，但有一些业务中缺乏隔离性就会带来许多麻烦。譬如在本章的场景事例中，缺乏隔离性会带来的一个显而易见的问题便是“超售”：完全有可能两个客户在短时间内都成功购买了同一件商品，而且他们各自购买的数量都不超过目前的库存，但他们购买的数量之和却超过了库存。 TCC 事务 （Try-Confirm-Cancel）TCC 较为烦琐，它是一种业务侵入式较强的事务方案，要求业务处理过程必须拆分为“预留业务资源”和“确认/释放消费资源”两个子过程。 Try：尝试执行阶段，完成所有业务可执行性的检查（保障一致性），并且预留好全部需用到的业务资源（保障隔离性）。 Confirm：确认执行阶段，不进行任何业务检查，直接使用 Try 阶段准备的资源来完成业务处理。Confirm 阶段可能会重复执行，因此本阶段所执行的操作需要具备幂等性。 Cancel：取消执行阶段，释放 Try 阶段预留的业务资源。Cancel 阶段可能会重复执行，也需要满足幂等性。 最终用户向 Fenix’s Bookstore 发送交易请求：购买一本价值 100 元的《深入理解 Java 虚拟机》。 创建事务，生成事务 ID，记录在活动日志中，进入 Try 阶段： 用户服务：检查业务可行性，可行的话，将该用户的 100 元设置为“冻结”状态，通知下一步进入 Confirm 阶段；不可行的话，通知下一步进入 Cancel 阶段。 仓库服务：检查业务可行性，可行的话，将该仓库的 1 本《深入理解 Java 虚拟机》设置为“冻结”状态，通知下一步进入 Confirm 阶段；不可行的话，通知下一步进入 Cancel 阶段。 商家服务：检查业务可行性，不需要冻结资源。 如果第 2 步所有业务均反馈业务可行，将活动日志中的状态记录为 Confirm，进入 Confirm 阶段： 用户服务：完成业务操作（扣减那被冻结的 100 元）。 仓库服务：完成业务操作（标记那 1 本冻结的书为出库状态，扣减相应库存）。 商家服务：完成业务操作（收款 100 元）。 第 3 步如果全部完成，事务宣告正常结束，如果第 3 步中任何一方出现异常，不论是业务异常或者网络异常，都将根据活动日志中的记录，重复执行该服务的 Confirm 操作，即进行最大努力交付。 如果第 2 步有任意一方反馈业务不可行，或任意一方超时，将活动日志的状态记录为 Cancel，进入 Cancel 阶段： 用户服务：取消业务操作（释放被冻结的 100 元）。 仓库服务：取消业务操作（释放被冻结的 1 本书）。 商家服务：取消业务操作（大哭一场后安慰商家谋生不易）。 第 5 步如果全部完成，事务宣告以失败回滚结束，如果第 5 步中任何一方出现异常，不论是业务异常或者网络异常，都将根据活动日志中的记录，重复执行该服务的 Cancel 操作，即进行最大努力交付。 由上述操作过程可见，TCC 其实有点类似 2PC 的准备阶段和提交阶段，但 TCC 是位于用户代码层面，而不是在基础设施层面，这为它的实现带来了较高的灵活性，可以根据需要设计资源锁定的粒度。TCC 在业务执行时只操作预留资源，几乎不会涉及锁和资源的争用，具有很高的性能潜力。但是 TCC 并非纯粹只有好处，它也带来了更高的开发成本和业务侵入性，意味着有更高的开发成本和更换事务实现方案的替换成本，所以，通常我们并不会完全靠裸编码来实现 TCC，而是基于某些分布式事务中间件（譬如阿里开源的Seata）去完成，尽量减轻一些编码工作量。 TCC 事务具有较强的隔离性，避免了“超售”的问题，而且其性能一般来说是本篇提及的几种柔性事务模式中最高的，但它仍不能满足所有的场景。TCC 的最主要限制是它的业务侵入性很强，譬如，把我们的场景事例修改如下：由于中国网络支付日益盛行，现在用户和商家在书店系统中可以选择不再开设充值账号，至少不会强求一定要先从银行充值到系统中才能进行消费，允许直接在购物时通过 U 盾或扫码支付，在银行账号中划转货款。这个需求完全符合国内网络支付盛行的现状，却给系统的事务设计增加了额外的限制：如果用户、商家的账号余额由银行管理的话，其操作权限和数据结构就不可能再随心所欲地自行定义，通常也就无法完成冻结款项、解冻、扣减这样的操作，因为银行一般不会配合你的操作。所以 TCC 中的第一步 Try 阶段往往无法施行。 SAGA模式SAGA 事务。SAGA 在英文中是“长篇故事、长篇记叙、一长串事件”的意思。 原本 SAGA 的目的是避免大事务长时间锁定数据库的资源，后来才发展成将一个分布式环境中的大事务分解为一系列本地事务的设计模式。SAGA 由两部分操作组成。 大事务拆分若干个小事务，将整个分布式事务 T 分解为 n 个子事务，命名为 T1，T2，…，Ti，…，Tn。每个子事务都应该是或者能被视为是原子行为。如果分布式事务能够正常提交，其对数据的影响（最终一致性）应与连续按顺序成功提交 Ti等价。 为每一个子事务设计对应的补偿动作，命名为 C1，C2，…，Ci，…，Cn。Ti与 Ci必须满足以下条件： Ti与 Ci都具备幂等性。 Ti与 Ci满足交换律（Commutative），即先执行 Ti还是先执行 Ci，其效果都是一样的。 Ci必须能成功提交，即不考虑 Ci本身提交失败被回滚的情形，如出现就必须持续重试直至成功，或者要人工介入。 正向恢复（Forward Recovery）：如果 Ti事务提交失败，则一直对 Ti进行重试，直至成功为止（最大努力交付）。这种恢复方式不需要补偿，适用于事务最终都要成功的场景，譬如在别人的银行账号中扣了款，就一定要给别人发货。正向恢复的执行模式为：T1，T2，…，Ti（失败），Ti（重试）…，Ti+1，…，Tn。 反向恢复（Backward Recovery）：如果 Ti事务提交失败，则一直执行 Ci对 Ti进行补偿，直至成功为止（最大努力交付）。这里要求 Ci必须（在持续重试后）执行成功。反向恢复的执行模式为：T1，T2，…，Ti（失败），Ci（补偿），…，C2，C1。 与 TCC 相比，SAGA 不需要为资源设计冻结状态和撤销冻结的操作，补偿操作往往要比冻结操作容易实现得多。譬如，前面提到的账号余额直接在银行维护的场景，从银行划转货款到 Fenix’s Bookstore 系统中，这步是经由用户支付操作（扫码或 U 盾）来促使银行提供服务；如果后续业务操作失败，尽管我们无法要求银行撤销掉之前的用户转账操作，但是由 Fenix’s Bookstore 系统将货款转回到用户账上作为补偿措施却是完全可行的。 SAGA 必须保证所有子事务都得以提交或者补偿，但 SAGA 系统本身也有可能会崩溃，所以它必须设计成与数据库类似的日志机制（被称为 SAGA Log）以保证系统恢复后可以追踪到子事务的执行情况，譬如执行至哪一步或者补偿至哪一步了。另外，尽管补偿操作通常比冻结/撤销容易实现，但保证正向、反向恢复过程的能严谨地进行也需要花费不少的工夫，譬如通过服务编排、可靠事件队列等方式完成，所以，SAGA 事务通常也不会直接靠裸编码来实现，一般也是在事务中间件的基础上完成，前面提到的 Seata 就同样支持 SAGA 事务模式。 AT 事务模式譬如阿里的 GTS（Global Transaction Service，Seata 由 GTS 开源而来） 从整体上看是 AT 事务是参照了 XA 两段提交协议实现的，但针对 XA 2PC 的缺陷，即在准备阶段必须等待所有数据源都返回成功后，协调者才能统一发出 Commit 命令而导致的木桶效应（所有涉及的锁和资源都需要等待到最慢的事务完成后才能统一释放），设计了针对性的解决方案。 大致的做法是在业务数据提交时自动拦截所有 SQL，将 SQL 对数据修改前、修改后的结果分别保存快照，生成行锁，通过本地事务一起提交到操作的数据源中，相当于自动记录了重做和回滚日志。如果分布式事务成功提交，那后续清理每个数据源中对应的日志数据即可；如果分布式事务需要回滚，就根据日志数据自动产生用于补偿的“逆向 SQL”。 基于这种补偿方式，分布式事务中所涉及的每一个数据源都可以单独提交，然后立刻释放锁和资源。这种异步提交的模式，相比起 2PC 极大地提升了系统的吞吐量水平。而代价就是大幅度地牺牲了隔离性，甚至直接影响到了原子性。因为在缺乏隔离性的前提下，以补偿代替回滚并不一定是总能成功的。 譬如，当本地事务提交之后、分布式事务完成之前，该数据被补偿之前又被其他操作修改过，即出现了脏写（Dirty Write），这时候一旦出现分布式事务需要回滚，就不可能再通过自动的逆向 SQL 来实现补偿，只能由人工介入处理了。 通常来说，脏写是一定要避免的，所有传统关系数据库在最低的隔离级别上都仍然要加锁以避免脏写，因为脏写情况一旦发生，人工其实也很难进行有效处理。所以 GTS 增加了一个“全局锁”（Global Lock）的机制来实现写隔离，要求本地事务提交之前，一定要先拿到针对修改记录的全局锁后才允许提交，没有获得全局锁之前就必须一直等待，这种设计以牺牲一定性能为代价，避免了有两个分布式事务中包含的本地事务修改了同一个数据，从而避免脏写。 在读隔离方面，AT 事务默认的隔离级别是读未提交（Read Uncommitted），这意味着可能产生脏读（Dirty Read）。也可以采用全局锁的方案解决读隔离问题，但直接阻塞读取的话，代价就非常大了，一般不会这样做","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"title":"本地事务与全局事务","text":"本地事务本地事务是最基础的一种事务解决方案，只适用于单个服务使用单个数据源的场景。从应用角度看，它是直接依赖于数据源本身提供的事务能力来工作的，在程序代码层面，最多只能对事务接口做一层标准化的包装（如 JDBC 接口），并不能深入参与到事务的运作过程当中，事务的开启、终止、提交、回滚、嵌套、设置隔离级别，乃至与应用代码贴近的事务传播方式，全部都要依赖底层数据源的支持才能工作。 全局事务全局事务被限定为一种适用于单个服务使用多个数据源场景的事务解决方案。 本地事务的问题1234567891011121314151617public void buyBook(PaymentBill bill) { userTransaction.begin(); warehouseTransaction.begin(); businessTransaction.begin(); try { userAccountService.pay(bill.getMoney()); warehouseService.deliver(bill.getItems()); businessAccountService.receipt(bill.getMoney()); userTransaction.commit(); warehouseTransaction.commit(); businessTransaction.commit(); } catch(Exception e) { userTransaction.rollback(); warehouseTransaction.rollback(); businessTransaction.rollback(); }} 从代码上可看出，程序的目的是要做三次事务提交，但实际上代码并不能这样写，试想一下，如果在businessTransaction.commit()中出现错误，代码转到catch块中执行，此时userTransaction和warehouseTransaction已经完成提交，再去调用rollback()方法已经无济于事，这将导致一部分数据被提交，另一部分被回滚，整个事务的一致性也就无法保证了。 XA 协议1991 年，为了解决分布式事务的一致性问题，X/Open组织（后来并入了The Open Group）提出了一套名为X/Open XA（XA 是 eXtended Architecture 的缩写）的处理事务架 其核心内容是定义了全局的事务管理器（Transaction Manager，用于协调全局事务）和局部的资源管理器（Resource Manager，用于驱动本地事务）之间的通信接口。XA 接口是双向的，能在一个事务管理器和多个资源管理器（Resource Manager）之间形成通信桥梁，通过协调多个数据源的一致动作，实现全局事务的统一提交或者统一回滚. 基于 XA 模式在 Java 语言中的实现了全局事务处理的标准，这也就是我们现在所熟知的 JTA。JTA 最主要的两个接口是： 事务管理器的接口：javax.transaction.TransactionManager。这套接口是给 Java EE 服务器提供容器事务（由容器自动负责事务管理）使用的，还提供了另外一套javax.transaction.UserTransaction接口，用于通过程序代码手动开启、提交和回滚事务。 资源定义接口：javax.transaction.xa.XAResource，任何资源（JDBC、JMS 等等）如果想要支持 JTA，只要实现 XAResource 接口中的方法即可。 两阶段提交 （2 Phase Commit，2PC）协议 准备阶段：又叫作投票阶段 在这一阶段，协调者询问事务的所有参与者是否准备好提交，参与者如果已经准备好提交则回复 Prepared，否则回复 Non-Prepared。这里所说的准备操作跟人类语言中通常理解的准备并不相同，对于数据库来说，准备操作是在重做日志中记录全部事务提交操作所要做的内容，它与本地事务中真正提交的区别只是暂不写入最后一条 Commit Record 而已，这意味着在做完数据持久化后并不立即释放隔离性，即仍继续持有锁，维持数据对其他非事务内观察者的隔离状态。 提交阶段：又叫作执行阶段 协调者如果在上一阶段收到所有事务参与者回复的 Prepared 消息，则先自己在本地持久化事务状态为 Commit，在此操作完成后向所有参与者发送 Commit 指令，所有参与者立即执行提交操作；否则，任意一个参与者回复了 Non-Prepared 消息，或任意一个参与者超时未回复，协调者将自己的事务状态持久化为 Abort 之后，向所有参与者发送 Abort 指令，参与者立即执行回滚操作。对于数据库来说，这个阶段的提交操作应是很轻量的，仅仅是持久化一条 Commit Record 而已，通常能够快速完成，只有收到 Abort 指令时，才需要根据回滚日志清理已提交的数据，这可能是相对重负载的操作。 能够成功保证一致性还需要一些其他前提条件 必须假设网络在提交阶段的短时间内是可靠的，即提交阶段不会丢失消息。同时也假设网络通信在全过程都不会出现误差，即可以丢失消息，但不会传递错误的消息，XA 的设计目标并不是解决诸如拜占庭将军一类的问题。两段式提交中投票阶段失败了可以补救（回滚），而提交阶段失败了无法补救（不再改变提交或回滚的结果，只能等崩溃的节点重新恢复），因而此阶段耗时应尽可能短，这也是为了尽量控制网络风险的考虑。 必须假设因为网络分区、机器崩溃或者其他原因而导致失联的节点最终能够恢复，不会永久性地处于失联状态。由于在准备阶段已经写入了完整的重做日志，所以当失联机器一旦恢复，就能够从日志中找出已准备妥当但并未提交的事务数据，并向协调者查询该事务的状态，确定下一步应该进行提交还是回滚操作。 上面所说的协调者、参与者都是可以由数据库自己来扮演的，不需要应用程序介入。协调者一般是在参与者之间选举产生的，而应用程序相对于数据库来说只扮演客户端的角色。 2pc 存在的问题 单点问题：协调者在两段提交中具有举足轻重的作用，协调者等待参与者回复时可以有超时机制，允许参与者宕机，但参与者等待协调者指令时无法做超时处理。一旦宕机的不是其中某个参与者，而是协调者的话，所有参与者都会受到影响。如果协调者一直没有恢复，没有正常发送 Commit 或者 Rollback 的指令，那所有参与者都必须一直等待。 性能问题：两段提交过程中，所有参与者相当于被绑定成为一个统一调度的整体，期间要经过两次远程服务调用，三次数据持久化（准备阶段写重做日志，协调者做状态持久化，提交阶段在日志写入 Commit Record），整个过程将持续到参与者集群中最慢的那一个处理操作结束为止，这决定了两段式提交的性能通常都较差。 一致性风险：前面已经提到，两段式提交的成立是有前提条件的，当网络稳定性和宕机恢复能力的假设不成立时，仍可能出现一致性问题。宕机恢复能力这一点不必多谈，1985 年 Fischer、Lynch、Paterson 提出了“FLP 不可能原理”，证明了如果宕机最后不能恢复，那就不存在任何一种分布式协议可以正确地达成一致性结果。该原理在分布式中是与“CAP 不可兼得原理“齐名的理论。而网络稳定性带来的一致性风险是指：尽管提交阶段时间很短，但这仍是一段明确存在的危险期，如果协调者在发出准备指令后，根据收到各个参与者发回的信息确定事务状态是可以提交的，协调者会先持久化事务状态，并提交自己的事务，如果这时候网络忽然被断开，无法再通过网络向所有参与者发出 Commit 指令的话，就会导致部分数据（协调者的）已提交，但部分数据（参与者的）既未提交，也没有办法回滚，产生了数据不一致的问题。 三段式提交（3 Phase Commit，3PC）协议三段式提交把原本的两段式提交的准备阶段再细分为两个阶段，分别称为 CanCommit、PreCommit，把提交阶段改称为 DoCommit 阶段。 CanCommit 是一个询问阶段，协调者让每个参与的数据库根据自身状态，评估该事务是否有可能顺利完成。 将准备阶段一分为二的理由是这个阶段是重负载的操作，一旦协调者发出开始准备的消息，每个参与者都将马上开始写重做日志，它们所涉及的数据资源即被锁住，如果此时某一个参与者宣告无法完成提交，相当于大家都白做了一轮无用功。 所以，增加一轮询问阶段，如果都得到了正面的响应，那事务能够成功提交的把握就比较大了，这也意味着因某个参与者提交时发生崩溃而导致大家全部回滚的风险相对变小。因此，在事务需要回滚的场景中，三段式的性能通常是要比两段式好很多的，但在事务能够正常提交的场景中，两者的性能都依然很差，甚至三段式因为多了一次询问，还要稍微更差一些。 同样也是由于事务失败回滚概率变小的原因，在三段式提交中，如果在 PreCommit 阶段之后发生了协调者宕机，即参与者没有能等到 DoCommit 的消息的话，默认的操作策略将是提交事务而不是回滚事务或者持续等待，这就相当于避免了协调者单点问题的风险。 三段式提交对单点问题和回滚时的性能问题有所改善，但是它对一致性风险问题并未有任何改进，在这方面它面临的风险甚至反而是略有增加了的。譬如，进入 PreCommit 阶段之后，协调者发出的指令不是 Ack 而是 Abort，而此时因网络问题，有部分参与者直至超时都未能收到协调者的 Abort 指令的话，这些参与者将会错误地提交事务，这就产生了不同参与者之间数据不一致的问题。","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/%E6%9C%AC%E5%9C%B0%E4%BA%8B%E5%8A%A1%E4%B8%8E%E5%85%A8%E5%B1%80%E4%BA%8B%E5%8A%A1/"},{"title":"自动装箱、拆箱与遍历循环","text":"自动装箱、拆箱与遍历循环 12345678public static void main(String[] args) { List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4); int sum = 0; for (int i : list) { sum += i; } System.out.println(sum);} 自动装箱、拆箱与遍历循环编译之后 12345678910111213 public static void main(String[] args) { List list = Arrays.asList( new Integer[] { Integer.valueOf(1), Integer.valueOf(2), Integer.valueOf(3), Integer.valueOf(4) }); int sum = 0; for (Iterator localIterator = list.iterator(); localIterator.hasNext(); ) { int i = ((Integer)localIterator.next()).intValue(); sum += i; } System.out.println(sum); } 自动装箱、拆箱在编译之后被转化 成了对应的包装和还原方法，如本例中的Integer.valueOf()与Integer.intValue()方法，而遍历循环则是把代码还原成了迭代器的实现，这也是为何遍历循环需要被遍历的类实现Iterable接口的原因。最后再看看变长参数，它在调用的时候变成了一个数组类型的参数，在变长参数出现之前，程序员的确也就是使用数组来完成类似功能的。 自动装箱的陷阱123456789101112131415public static void main(String[] args) { Integer a = 1; Integer b = 2; Integer c = 3; Integer d = 3; Integer e = 321; Integer f = 321; Long g = 3L; System.out.println(c == d); System.out.println(e == f); System.out.println(c == (a + b)); System.out.println(c.equals(a + b)); System.out.println(g == (a + b)); System.out.println(g.equals(a + b));} truefalsetruetruetruefalse","link":"/2024/09/01/java/jvm/%E8%87%AA%E5%8A%A8%E8%A3%85%E7%AE%B1%E3%80%81%E6%8B%86%E7%AE%B1%E4%B8%8E%E9%81%8D%E5%8E%86%E5%BE%AA%E7%8E%AF/"},{"title":"Java模块化系统","text":"在JDK 9中引入的Java模块化系统（Java Platform Module System，JPMS）是对Java技术的一次重要升级，为了能够实现模块化的关键目标——可配置的封装隔离机制，Java虚拟机对类加载架构也做 出了相应的变动调整，才使模块化系统得以顺利地运作。JDK 9的模块不仅仅像之前的JAR包那样只是简单地充当代码的容器，除了代码外，Java的模块定义还包含以下内容： 依赖其他模块的列表。 导出的包列表，即其他模块可以使用的列表。 开放的包列表，即其他模块可反射访问模块的列表。 使用的服务列表。 提供服务的实现列表。 可配置的封装隔离机制首先要解决JDK9之前基于类路径（ClassPath）来查找依赖的可靠性问题。此前，如果类路径中缺失了运行时依赖的类型，那就只能等程序运行到发生该类型的加载、链接 时才会报出运行的异常。而在JDK 9以后，如果启用了模块化进行封装，模块就可以声明对其他模块的显式依赖，这样Java虚拟机就能够在启动时验证应用程序开发阶段设定好的依赖关系在运行期是否完备，如有缺失那就直接启动失败，从而避免了很大一部分由于类型依赖而引发的运行时异常。 可配置的封装隔离机制还解决了原来类路径上跨JAR文件的public类型的可访问性问题。JDK9中的 ublic类型不再意味着程序的所有地方的代码都可以随意访问到它们，模块提供了更精细的可访问性控制，必须明确声明其中哪一些public的类型可以被其他哪一些模块访问，这种访问控制也主要是在类载过程中完成的。 解决的问题Java 9 引入的模块化系统（Module System）主要解决了以下几个关键问题： 类路径地狱（ClassPath Hell） 在传统的 Java 应用程序中，类路径（ClassPath）管理非常复杂，尤其是在大型项目中。类路径地狱指的是由于类路径配置不当导致的类加载问题，例如： 重复的类：多个JAR文件中包含同名类，导致类加载冲突。 缺失的类：类路径中缺少必要的类文件，导致 ClassNotFoundException 或 NoClassDefFoundError。 版本冲突：不同库依赖于不同版本的相同类，导致兼容性问题。 模块化系统通过明确声明模块之间的依赖关系，避免了这些问题。每个模块都有自己的命名空间，类加载器会根据模块声明的依赖关系来加载类，从而减少了类路径配置的复杂性和错误。 更好的封装 在传统的 Java 应用程序中，所有的公共类（public 类）都对其他类可见，这导致了封装的不足。模块化系统通过 module-info.java 文件提供了更强的封装机制： * 默认封装：模块内部的包默认对其他模块不可见，除非显式导出。 * 导出包：模块可以通过 exports 关键字导出特定的包，使其对其他模块可见。 * 开放包：模块可以通过 opens 关键字开放特定的包，允许其他模块通过反射访问这些包中的类。 这种封装机制提高了代码的安全性和可维护性，减少了不必要的依赖和潜在的副作用。 依赖管理 模块化系统允许明确声明模块之间的依赖关系，通过 requires 关键字指定模块依赖的其他模块。这使得依赖管理更加清晰和可控： * 显式依赖：模块必须显式声明其依赖的其他模块，避免了隐式依赖带来的问题。 * 版本管理：模块化系统支持版本管理，可以指定依赖的具体版本，减少版本冲突。 性能优化 模块化系统通过以下方式优化了性能： * 按需加载：JVM 只加载应用程序实际使用的模块，减少了类加载的时间和内存占用。 * 模块缓存：JVM 可以缓存已加载的模块，加快后续启动时间。 * 类加载优化：模块化系统提供了更高效的类加载机制，减少了类加载的开销。 可维护性和可扩展性 模块化系统使得大型应用程序的维护和扩展更加容易： * 独立开发：每个模块可以独立开发和测试，减少了模块间的耦合。 * 模块替换：可以轻松地替换或升级单个模块，而不需要重新构建整个应用程序。 * 模块隔离：模块之间的隔离机制减少了模块间的干扰，提高了系统的稳定性和可靠性。 标准化的平台模块 Java 9 将 JDK 本身也模块化，将其拆分为多个标准模块，如 java.base、java.logging、java.sql 等。这使得开发者可以根据需要选择加载特定的模块，减少了 JDK 的体积和启动时间。 注意事项 模块路径：确保模块路径正确设置，否则 JVM 无法找到模块。 导出和开放：模块内部的包默认对其他模块不可见，需要显式导出或开放。 反射访问：模块化系统限制了反射访问，需要使用 –add-opens 或 –add-exports 参数来开放访问。 类加载器的变动为了保证兼容性，JDK 9并没有从根本上动摇从JDK 1.2以来运行了二十年之久的三层类加载器架 构以及双亲委派模型。但是为了模块化系统的顺利施行，模块化下的类加载器仍然发生了一些应该被注意到变动，主要包括以下几个方面。 扩展类加载器（Extension Class Loader）被平台类加载器（Platform Class Loader）取代。 既然整个JDK都基于模块化进行构建（原来的rt.jar和tools.jar被拆分成数十个JMOD文件），其中的Java类库就已天然地满足了可扩展的需求，那自然无须再保留 \\lib\\ext目录，此前使用这个目录或者java.ext.dirs系统变量来扩展JDK功能的机制已经没有继续存在的价值了，用来加载这部分类库的扩展类加载器也完成了它的历史使命。 平台类加载器和应用程序类加载器都不再派生自java.net.URLClassLoader 现在启动类加载器、平台类加载器、应用程序类加载器全都继承于 jdk.internal.loader.BuiltinClassLoader，在BuiltinClassLoader中实现了新的模块化架构下类如何从模块中加载的逻辑，以及模块中资源可访问性的处理。 启动类加载器现在是在Java虚拟机 内部和Java类库共同协作实现的类加载器，尽管有了BootClassLoader这样的Java类，但为了与之前的代码保持兼容，所有在获取启动类加载器的场景（譬如Object.class.getClassLoader()）中仍然会返回null来代替，而不会得到BootClassLoader的实例。 JDK9中虽然仍然维持着三层类加载器和双亲委派的架构，但类加载的委派关系也发生了变动。当平台及应用程序类加载器收到类加载请求，在委派给父加载器加载前，要先判断该类是否能够归属到某一个系统模块中，如果可以找到这样的归属关系，就要优先委派给负责那个模块的加载器完成加载。","link":"/2024/09/01/java/jvm/Java%E6%A8%A1%E5%9D%97%E5%8C%96%E7%B3%BB%E7%BB%9F/"},{"title":"泛型","text":"泛型的本质是参数化类型（Parameterized Type）或者参数化多态（Parametric Poly morphism）的应用，即可以将操作的数据类型指定为方法签名中的一种特殊参数，这种参数类型能够用在类、接口和方法的创建中，分别构成泛型类、泛型接口和泛型方法。 Java选择的泛型实现方式叫作“类型擦除式泛型”（Type Erasure Generics），而C#选择的泛型实现方式是“具现化式泛型”（Reified Generics）。具现化和特化、偏特化这些名词最初都是源于C++模版语法中的概念，C#里面泛型无论在程序源码里面、编译后的中间语言表示（Intermediate Language，这时候泛型是一个占位符）里面，抑或是运行期的CLR里面都是切实存在的，List与 List就是两个不同的类型，它们由系统在运行期生成，有着自己独立的虚方法表和类型数据。 而Java语言中的泛型则不同，它只在程序源码中存在，在编译后的字节码文件中，全部泛型都被替换 为原来的裸类型（Raw Type）了，并且在相应的地方插入了强制转型代码，因此对于运行期的Java语言来说，Array ist与ArrayList其实是同一个类型，由此读者可以想象“类型擦除”这个名字的含义和来源。 1234567891011public class TypeErasureGenerics&lt;E&gt; { public void doSomething(Object item) { if (item instanceof E) { // 不合法，无法对泛型进行实例判断 ... } E newItem = new E(); // 不合法，无法使用泛型创建对象 E[] itemArray = new E[10]; // 不合法，无法使用泛型创建数组 } } 类型擦除要让所有需要泛型化的已有类型，譬如ArrayList，原地泛型化后变成了Array List，而且保证以前直接用ArrayList的代码在泛型新版本里必须还能继续用这同一个容器，这就必须让所有泛型化的实例类型，譬如ArrayList、Array List这些全部自动成为ArrayList的子类型才能可以，否则类型转换就是不安全的。由此就引出了“裸类型”（Raw Type）的概 ，裸类型应被视为所有该类型泛型化实例的共同父类型（Super Type），只有这样代码中的赋值才是被系统允许的从子类到父类的安全转型。 123456ArrayList&lt;Integer&gt; ilist = new ArrayList&lt;Integer&gt;(); ArrayList&lt;String&gt; slist = new ArrayList&lt;String&gt;(); ArrayList list; // 裸类型list = ilist; list = slist; 泛型擦除前的例子 1234567public static void main(String[] args) { Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put(&quot;hello&quot;, &quot;你好&quot;); map.put(&quot;how are you?&quot;, &quot;吃了没？&quot;); System.out.println(map.get(&quot;hello&quot;)); System.out.println(map.get(&quot;how are you?&quot;));} 把这段Java代码编译成Class文件，然后再用字节码反编译工具进行反编译后，将会发现泛型都不见了，程序又变回了Java泛型出现之前的写法，泛型类型都变回了裸类型，只在元素访问时插入了从Object到String的强制转型代码。 泛型擦除后的例子 1234567public static void main(String[] args) { Map map = new HashMap(); map.put(&quot;hello&quot;, &quot;你好&quot;); map.put(&quot;how are you?&quot;, &quot;吃了没？&quot;); System.out.println((String) map.get(&quot;hello&quot;)); System.out.println((String) map.get(&quot;how are you?&quot;));} 类型擦除带来的缺陷 擦除法实现泛型直接导致了对原始类型（Primitive Types）无法支持泛型，因为一旦把泛型信息擦除后，到要插入强制转型代码的地方就没办法往下做了，因为不支持int、long与Object之间的强制转型。 运行期无法取到泛型类型信息。如下代码不得不加入的类型参数 123public static &lt;T&gt; T[] convert(List&lt;T&gt; list, Class&lt;T&gt; componentType) { T[] array = (T[])Array.newInstance(componentType, list.size());} 泛型遇见重载以下代码是不能被编译的，因为参数List和List编译之后都被擦除了，变成了同一种的裸类型List， 类型擦除导致这两个方法的特征签名变得一模一样。 1234567891011public class GenericTypes { public static void method(List&lt;String&gt; list) { System.out.println(&quot;invoke method(List&lt;String&gt; list)&quot;); } public static void method(List&lt;Integer&gt; list) { System.out.println(&quot;invoke method(List&lt;Integer&gt; list)&quot;); } } 以下代码运行结果为： invoke method(List list)invoke method(List list) 1234567891011121314public class GenericTypes { public static String method(List&lt;String&gt; list) { System.out.println(&quot;invoke method(List&lt;String&gt; list)&quot;); return &quot;&quot;; } public static int method(List&lt;Integer&gt; list) { System.out.println(&quot;invoke method(List&lt;Integer&gt; list)&quot;); return 1; } public static void main(String[] args) { method(new ArrayList&lt;String&gt;()); method(new ArrayList&lt;Integer&gt;()); }} 这两段代码的差别是因为下面代码的两个method()方法添加了不同的返回值，由于这两个返 回值的加入，方法重载居然成功了，即这段代码可以被编译和执行了。之所以这次能编译和执行成功，是因为两个method()方法加入了不同的返回值后才能共存在一个Class文件之中，方法重载要求方法具备不同的特征签名，返回值并不包含在方法的特征签名中，所以返回值不参与重载选择，但是在Class文件格式之中，只要描述符不是完全一致的两个方法就可以共存。也就是说两个方法如果有相同的名称和特征签名，但返回值不同，那它们也是可以合法地共存于一个Class文件中的。 从上面的例子中可以看到擦除法对实际编码带来的不良影响，由于List和List擦除后是同一个类型，我们只能添加两个并不需要实际使用到的返回值才能完成重载，这是一种毫无优 和美感可言的解决方案，并且存在一定语意上的混乱，必须用JDK6的Javac才能编译成功，其他版本或者是ECJ编译器都有可能拒绝编译。","link":"/2024/09/01/java/jvm/%E6%B3%9B%E5%9E%8B/"},{"title":"主内存与工作内存","text":"Java内存模型规定了所有的变量都存储在主内存（Main Memory ）中，每条线程还有自己的工作内存（Working Memory ），线程的工作内存中保存了被该线程使用的变量的主内存副本，线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的数据。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成，线程、主内存、工作内存三者的交互关系如图： 从变量、主内存、工作内存的定义来看，主内存主要对应于Java堆中的对象实例数据部分，而工作内存则对应于虚拟机栈中的部分区域。从更基础的层次上说，主内存直接对应于物理硬件的内存，而为了获取更好的运行速度，虚拟机（或者是硬件、操作系统本身的优化措施）可能会让工作内存优先存储于寄存器和高速缓存中，因为程序运行时主要访问的是工作内存。","link":"/2024/09/01/java/jvm/%E4%B8%BB%E5%86%85%E5%AD%98%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%86%85%E5%AD%98/"},{"title":"运行时栈帧结构","text":"方法是Java虚拟机最基本的执行单元，“栈帧”（Stack Frame）则是用于支持虚拟机进行方法调用和方法执行的数据结构，它也是虚拟机运行时数据区中的虚拟机栈（Virtual M achine Stack）的栈元素。栈帧存储了方法的局部变量表、操作数栈、动态连接和方法返回地址等信息， 每一个方法从调用开始至执行结束的过程，都对应着一个栈帧在虚拟机栈里面从入栈到出栈的过程。 局部变量表局部变量表（Local Variables Table）是一组变量值的存储空间，用于存放方法参数和方法内部定义 的局部变量。 局部变量表的容量以变量槽（Variable Slot）为最小单位，《Java虚拟机规范》中并没有明确指出一个变量槽应占用的内存空间大小，只是很有导向性地说到每个变量槽都应该能存放一个boolean、 byte、char、short、int、float、reference或returnAddress类型的数据。 一个变量槽可以存放一个 2位以内的数据类型，Java中占用不超过32位存储空间的数据类型有boolean、by te、char、short、int、 float、reference和returnAddress这8种类型。 reference类型表示对一个对象实例的引用，《Java虚拟机规范》既没有说明它的长度，也没有明确指出这种引用应有怎样的结构。但是一般来说，虚拟机实现至少都应当能通过这个引用做到两件事情，一是从根据引用直接或间接地查找到对象在Java堆中的数据存放的起始地址或索引，二是根据引用直接或间接地查找到对象所属数据类型在方法区中的存储的类型信息，否则将无法实现《Java语言规范》中定义的语法约定。 returnAddress类型目前已经很少见了，它是为字节码指令jsr、jsr_w和ret服务的，指向了一条字节码指令的地址，某些很古老的Java虚拟机曾经使用这几条指令来实现异常处理时的跳转，但现在也已经全部改为采用异常表来代替了。 操作数栈操作数栈（Operand Stack）也常被称为操作栈，它是一个后入先出（Last In First Out，LIFO）栈。 当一个方法刚刚开始执行的时候，这个方法的操作数栈是空的，在方法的执行过程中，会有各种字节码指令往操作数栈中写入和提取内容，也就是出栈和入栈操作。譬如在做算术运算的时候是通过将运算涉及的操作数栈压入栈顶后调用运算指令来进行的，又譬如在调用其他方法的时候是通过操作数栈来进行方法参数的传递。 两个不同栈帧作为不同方法的虚拟机栈的元素，是完全相互独立的。但是在大多虚拟机的实现里都会进行一些优化处理，令两个栈帧出现一部分重叠。让下面栈帧的部分操作数栈与上面栈帧的部分局部变量表重叠在一起，这样做不仅节约了一些空间，更重要的是在进行方法调 用时就可以直接共用一部分数据，无须进行额外的参数复制传递了。 动态连接每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接（Dynamic Linking）。Class文件的常量池中存有大量的符号引用，字节码中的方法调用指令就以常量池里指向方法的符号引用作为参数。这些符号 引用一部分会在类加载阶段或者第一次使用的时候就被转化为直接引用，这种转化被称为静态解析。 另外一部分将在每一次运行期间都转化为直接引用，这部分就称为动态连接。 方法返回地址当一个方法开始执行后，只有两种方式退出这个方法 执行引擎遇到任意一个方法返回的字节码指令，这时候可能会有返回值传递给上层的方法调用者（调用当前方法的方法称为调用者或者主调方法，方法是否有返回值以及返回值的类型将根据遇到何种方法返回指令来决定，这种退出方法的方式称为“正常调用完成”（Normal Method Invocation Completion）。 在方法执行的过程中遇到了异常，并且这个异常没有在方法体内得到妥善处理。无论是Java虚拟机内部产生的异常，还是代码中使用athrow字节码指令产生的异常，只要在本方法的异常表中没有搜索到匹配的异常处理器，就会导致方法退出，这种退出方法的方式称为“异常调用完成（Abrupt Method Invocation Completion）”。一个方法使用异常完成出口的方式退出，是不会给它的上层调用者提供任何返回值的。 无论采用何种退出方式，在方法退出之后，都必须返回到最初方法被调用时的位置，程序才能 续执行，方法返回时可能需要在栈帧中保存一些信息，用来帮助恢复它的上层主调方法的执行状态。 一般来说，方法正常退出时，主调方法的PC计数器的值就可以作为返回地址，栈帧中很可能会保存这个计数器值。而方法异常退出时，返回地址是要通过异常处理器表来确定的，栈帧中就一般不会保存这部分信息。 方法退出的过程实际上等同于把当前栈帧出栈，因此退出时可能执行的操作有：恢复上层方法的局部变量表和操作数栈，把返回值（如果有的话）压入调用者栈帧的操作数栈中，调整PC计数器的值以指向方法调用指令后面的一条指令等。笔者这里写的“可能”是由于这是基于概念模型的讨论，只有具体到某一款Java虚拟机实现，会执行哪些操作才能确定下来。 附加信息《Java虚拟机规范》允许虚拟机实现增加一些规范里没有描述的信息到栈帧之中，例如与调试、 性能收集相关的信息，这部分信息完全取决于具体的虚拟机实现，这里不再详述。在讨论概念时，一般会把动态连接、方法返回地址与其他附加信息全部归为一类，称为栈帧信息。","link":"/2024/09/01/java/jvm/%E8%BF%90%E8%A1%8C%E6%97%B6%E6%A0%88%E5%B8%A7%E7%BB%93%E6%9E%84/"},{"title":"传输链路优化","text":"传输链路优化（Transmission Optimization）今天的传输链路优化原则，在若干年后的未来再回头看它们时，其中多数已经成了奇技淫巧，有些甚至成了反模式。 https://icyfenix.cn/architect-perspective/general-architecture/diversion-system/transmission-optimization.html 连接数优化传输压缩快速 UDP 网络连接","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E9%80%8F%E6%98%8E%E5%A4%9A%E7%BA%A7%E5%88%86%E6%B5%81%E7%B3%BB%E7%BB%9F/%E4%BC%A0%E8%BE%93%E9%93%BE%E8%B7%AF%E4%BC%98%E5%8C%96/"},{"title":"内容分发网络（CDN）","text":"Content Distribution Network 如果把某个互联网系统比喻为一家企业，那内容分发网络就是它遍布世界各地的分支销售机构，现在有客户要买一块 CPU，那么订机票飞到美国加州 Intel 总部肯定是不合适的，到本地电脑城找个装机铺才是通常的做法，在此场景中，内容分发网络就相当于电脑城里的本地经销商。 仅从网络传输的角度看，一个互联网系统的速度取决于以下四点因素： 网站服务器接入网络运营商的链路所能提供的出口带宽。 用户客户端接入网络运营商的链路所能提供的入口带宽。 从网站到用户之间经过的不同运营商之间互联节点的带宽，一般来说两个运营商之间只有固定的若干个点是互通的，所有跨运营商之间的交互都要经过这些点。 从网站到用户之间的物理链路传输时延。爱打游戏的同学应该都清楚，延迟（Ping 值）比带宽更重要。 以上四个网络问题，除了第二个只能通过换一个更好的宽带才能解决之外，其余三个都能通过内容分发网络来显著改善。一个运作良好的内容分发网络，能为互联网系统解决跨运营商、跨地域物理距离所导致的时延问题，能为网站流量带宽起到分流、减负的作用。 内容分发网络的工作过程，主要涉及路由解析、内容分发、负载均衡和所能支持的 CDN 应用内容四个方面。 路由解析内容分发网络将用户请求路由到它的资源服务器上就是依靠 DNS 服务器来实现的。 CDN 路由解析的具体工作过程是： 架设好“icyfenix.cn”的服务器后，将服务器的 IP 地址在你的 CDN 服务商上注册为“源站”，注册后你会得到一个 CNAME，即本例中的“icyfenix.cn.cdn.dnsv1.com.”。 将得到的 CNAME 在你购买域名的 DNS 服务商上注册为一条 CNAME 记录。 当第一位用户来访你的站点时，将首先发生一次未命中缓存的 DNS 查询，域名服务商解析出 CNAME 后，返回给本地 DNS，至此之后链路解析的主导权就开始由内容分发网络的调度服务接管了。 本地 DNS 查询 CNAME 时，由于能解析该 CNAME 的权威服务器只有 CDN 服务商所架设的权威 DNS，这个 DNS 服务将根据一定的均衡策略和参数，如拓扑结构、容量、时延等，在全国各地能提供服务的 CDN 缓存节点中挑选一个最适合的，将它的 IP 代替源站的 IP 地址，返回给本地 DNS。 浏览器从本地 DNS 拿到 IP 地址，将该 IP 当作源站服务器来进行访问，此时该 IP 的 CDN 节点上可能有，也可能没有缓存过源站的资源。 经过内容分发后的 CDN 节点，就有能力代替源站向用户提供所请求的资源。 内容分发在 DNS 服务器的协助下，无论是对用户还是服务器，内容分发网络都可以是完全透明的，在两者都不知情的情况下，由 CDN 的缓存节点接管了用户向服务器发出的资源请求。后面随之而来的问题是缓存节点中必须有用户想要请求的资源副本，才可能代替源站来响应用户请求。这里面又包括了两个子问题：“如何获取源站资源” 和 “如何管理（更新）资源”。 CDN 获取源站资源的过程被称为“内容分发”，目前主要有以下两种主流的内容分发方式： 主动分发（Push）：分发由源站主动发起，将内容从源站或者其他资源库推送到用户边缘的各个 CDN 缓存节点上。这个推送的操作没有什么业界标准可循，可以采用任何传输方式（HTTP、FTP、P2P，等等）、任何推送策略（满足特定条件、定时、人工，等等）、任何推送时间，只要与后面说的更新策略相匹配即可。由于主动分发通常需要源站、CDN 服务双方提供程序 API 接口层面的配合，所以它对源站并不是透明的，只对用户一侧单向透明。主动分发一般用于网站要预载大量资源的场景。譬如双十一之前一段时间内，淘宝、京东等各个网络商城就会开始把未来活动中所需用到的资源推送到 CDN 缓存节点中，特别常用的资源甚至会直接缓存到你的手机 APP 的存储空间或者浏览器的localStorage上。 被动回源（Pull）：被动回源由用户访问所触发全自动、双向透明的资源缓存过程。当某个资源首次被用户请求的时候，CDN 缓存节点发现自己没有该资源，就会实时从源站中获取，这时资源的响应时间可粗略认为是资源从源站到 CDN 缓存节点的时间，再加上资源从 CDN 发送到用户的时间之和。因此，被动回源的首次访问通常是比较慢的（但由于 CDN 的网络条件一般远高于普通用户，并不一定就会比用户直接访问源站更慢），不适合应用于数据量较大的资源。被动回源的优点是可以做到完全的双向透明，不需要源站在程序上做任何的配合，使用起来非常方便。这种分发方式是小型站点使用 CDN 服务的主流选择，如果不是自建 CDN，而是购买阿里云、腾讯云的 CDN 服务的站点，多数采用的就是这种方式。 CDN 应用 加速静态资源：这是 CDN 本职工作。 安全防御：CDN 在广义上可以视作网站的堡垒机，源站只对 CDN 提供服务，由 CDN 来对外界其他用户服务，这样恶意攻击者就不容易直接威胁源站。CDN 对某些攻击手段的防御，如对DDoS 攻击的防御尤其有效。但需注意，将安全都寄托在 CDN 上本身是不安全的，一旦源站真实 IP 被泄漏，就会面临很高的风险。 协议升级：不少 CDN 提供商都同时对接（代售 CA 的）SSL 证书服务，可以实现源站是 HTTP 协议的，而对外开放的网站是基于 HTTPS 的。同理，可以实现源站到 CDN 是 HTTP/1.x 协议，CDN 提供的外部服务是 HTTP/2 或 HTTP/3 协议、实现源站是基于 IPv4 网络的，CDN 提供的外部服务支持 IPv6 网络，等等。 状态缓存：第一节介绍客户端缓存时简要提到了状态缓存，CDN 不仅可以缓存源站的资源，还可以缓存源站的状态，譬如源站的 301/302 转向就可以缓存起来让客户端直接跳转、还可以通过 CDN 开启HSTS、可以通过 CDN 进行OCSP 装订加速 SSL 证书访问，等等。有一些情况下甚至可以配置 CDN 对任意状态码（譬如 404）进行一定时间的缓存，以减轻源站压力，但这个操作应当慎重，在网站状态发生改变时去及时刷新缓存。 修改资源：CDN 可以在返回资源给用户的时候修改它的任何内容，以实现不同的目的。譬如，可以对源站未压缩的资源自动压缩并修改 Content-Encoding，以节省用户的网络带宽消耗、可以对源站未启用客户端缓存的内容加上缓存 Header，自动启用客户端缓存，可以修改CORS的相关 Header，将源站不支持跨域的资源提供跨域能力，等等。 访问控制：CDN 可以实现 IP 黑/白名单功能，根据不同的来访 IP 提供不同的响应结果，根据 IP 的访问流量来实现 QoS 控制、根据 HTTP 的 Referer 来实现防盗链，等等。 注入功能：CDN 可以在不修改源站代码的前提下，为源站注入各种功能","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E9%80%8F%E6%98%8E%E5%A4%9A%E7%BA%A7%E5%88%86%E6%B5%81%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AE%B9%E5%88%86%E5%8F%91%E7%BD%91%E7%BB%9C%EF%BC%88CDN%EF%BC%89/"},{"title":"域名缓存（DNS Lookup）","text":"域名缓存（DNS Lookup）DNS 也许是全世界最大、使用最频繁的信息查询系统，如果没有适当的分流机制，DNS 将会成为整个网络的瓶颈。 DNS 的作用是将便于人类理解的域名地址转换为便于计算机处理的 IP 地址 无论是使用浏览器抑或是在程序代码中访问某个网址域名，譬如以www.icyfenix.com.cn为例，如果没有缓存的话，都会先经过 DNS 服务器的解析翻译，找到域名对应的 IP 地址才能开始通信，这项操作是操作系统自动完成的，一般不需要用户程序的介入。不过，DNS 服务器并不是一次性地将“www.icyfenix.com.cn”直接解析成 IP 地址，需要经历一个递归的过程。首先 DNS 会将域名还原为“www.icyfenix.com.cn.”，注意最后多了一个点“.”，它是“.root”的含义。早期的域名必须带有这个点才能被 DNS 正确解析，如今几乎所有的操作系统、DNS 服务器都可以自动补上结尾的点号，然后开始如下解析步骤： 客户端先检查本地的 DNS 缓存，查看是否存在并且是存活着的该域名的地址记录。DNS 是以存活时间（Time to Live，TTL）来衡量缓存的有效情况的，所以，如果某个域名改变了 IP 地址，DNS 服务器并没有任何机制去通知缓存了该地址的机器去更新或者失效掉缓存，只能依靠 TTL 超期后的重新获取来保证一致性。后续每一级 DNS 查询的过程都会有类似的缓存查询操作 客户端将地址发送给本机操作系统中配置的本地 DNS（Local DNS），这个本地 DNS 服务器可以由用户手工设置，也可以在 DHCP 分配时或者在拨号时从 PPP 服务器中自动获取到。 本地 DNS 收到查询请求后，会按照“是否有www.icyfenix.com.cn的权威服务器”→“是否有icyfenix.com.cn的权威服务器”→“是否有com.cn的权威服务器”→“是否有cn的权威服务器”的顺序，依次查询自己的地址记录，如果都没有查询到，就会一直找到最后点号代表的根域名服务器为止。这个步骤里涉及了两个重要名词： 权威域名服务器（Authoritative DNS）：是指负责翻译特定域名的 DNS 服务器，“权威”意味着这个域名应该翻译出怎样的结果是由它来决定的。DNS 翻译域名时无需像查电话本一样刻板地一对一翻译，根据来访机器、网络链路、服务内容等各种信息，可以玩出很多花样，权威 DNS 的灵活应用，在后面的内容分发网络、服务发现等章节都还会有所涉及。 根域名服务器（Root DNS）是指固定的、无需查询的顶级域名（Top-Level Domain）服务器，可以默认为它们已内置在操作系统代码之中。全世界一共有 13 组根域名服务器（注意并不是 13 台，每一组根域名都通过任播的方式建立了一大群镜像，根据维基百科的数据，迄今已经超过 1000 台根域名服务器的镜像了）。13 这个数字是由于 DNS 主要采用 UDP 传输协议（在需要稳定性保证的时候也可以采用 TCP）来进行数据交换，未分片的 UDP 数据包在 IPv4 下最大有效值为 512 字节，最多可以存放 13 组地址记录，由此而来的限制。 现在假设本地 DNS 是全新的，上面不存在任何域名的权威服务器记录，所以当 DNS 查询请求按步骤 3 的顺序一直查到根域名服务器之后，它将会得到“cn的权威服务器”的地址记录，然后通过“cn的权威服务器”，得到“com.cn的权威服务器”的地址记录，以此类推，最后找到能够解释www.icyfenix.com.cn的权威服务器地址。 通过“www.icyfenix.com.cn的权威服务器”，查询www.icyfenix.com.cn的地址记录，地址记录并不一定就是指 IP 地址，在 RFC 规范中有定义的地址记录类型已经多达数十种，譬如 IPv4 下的 IP 地址为 A 记录，IPv6 下的 AAAA 记录、主机别名 CNAME 记录，等等。","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E9%80%8F%E6%98%8E%E5%A4%9A%E7%BA%A7%E5%88%86%E6%B5%81%E7%B3%BB%E7%BB%9F/%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90/"},{"title":"客户端缓存（Client Cache）","text":"客户端缓存（Client Cache）HTTP 协议的无状态性决定了它必须依靠客户端缓存来解决网络传输效率上的缺陷。 状态缓存HTTP 缓存中，状态缓存是指不经过服务器，客户端直接根据缓存信息对目标网站的状态判断，以前只有 301/Moved Permanently（永久重定向）这一种；后来在RFC6797中增加了HSTS（HTTP Strict Transport Security）机制，用于避免依赖 301/302 跳转 HTTPS 时可能产生的降级中间人劫持。 强制缓存HTTP 的强制缓存对一致性处理的策略就如它的名字一样，十分直接：假设在某个时点到来以前，譬如收到响应后的 10 分钟内，资源的内容和状态一定不会被改变，因此客户端可以无须经过任何请求，在该时点前一直持有和使用该资源的本地缓存副本。 根据约定，强制缓存在浏览器的地址输入、页面链接跳转、新开窗口、前进和后退中均可生效，但在用户主动刷新页面时应当自动失效。 HTTP 协议中设有以下两类 Header 实现强制缓存。 Expires：Expires 是 HTTP/1.0 协议中开始提供的 Header，后面跟随一个截至时间参数。当服务器返回某个资源时带有该 Header 的话，意味着服务器承诺截止时间之前资源不会发生变动，浏览器可直接缓存该数据，不再重新发请求。 示例： 12HTTP/1.1 200 OKExpires: Wed, 8 Apr 2020 07:28:00 GMT Expires 是 HTTP 协议最初版本中提供的缓存机制，设计非常直观易懂，但考虑得并不够周全，它至少存在以下显而易见的问题： 受限于客户端的本地时间。譬如，在收到响应后，客户端修改了本地时间，将时间前后调整几分钟，就可能会造成缓存提前失效或超期持有。 无法处理涉及到用户身份的私有资源，譬如，某些资源被登录用户缓存在自己的浏览器上是合理的，但如果被代理服务器或者内容分发网络缓存起来，则可能被其他未认证的用户所获取。 无法描述“不缓存”的语义。譬如，浏览器为了提高性能，往往会自动在当次会话中缓存某些 MIME 类型的资源，在 HTTP/1.0 的服务器中就缺乏手段强制浏览器不允许缓存某个资源。以前为了实现这类功能，通常不得不使用脚本，或者手工在资源后面增加时间戳（譬如如“xx.js?t=1586359920”、“xx.jpg?t=1586359350”）来保证每次资源都会重新获取。 关于“不缓存”的语义，在 HTTP/1.0 中其实预留了“Pragma: no-cache”来表达，但 Pragma 参数在 HTTP/1.0 中并没有确切描述其具体行为，随后就被 HTTP/1.1 中出现过的 Cache-Control 所替代，现在，尽管主流浏览器通常都会支持 Pragma，但行为仍然是不确定的，实际并没有什么使用价值。 Cache-Control：Cache-Control 是 HTTP/1.1 协议中定义的强制缓存 Header，它的语义比起 Expires 来说就丰富了很多，如果 Cache-Control 和 Expires 同时存在，并且语义存在冲突（譬如 Expires 与 max-age / s-maxage 冲突）的话，规定必须以 Cache-Control 为准。Cache-Control 的使用示例如下： 12HTTP/1.1 200 OKCache-Control: max-age=600 Cache-Control 在客户端的请求 Header 或服务器的响应 Header 中都可以存在，它定义了一系列的参数，且允许自行扩展（即不在标准 RFC 协议中，由浏览器自行支持的参数），其标准的参数主要包括有： max-age和s-maxage：max-age 后面跟随一个以秒为单位的数字，表明相对于请求时间（在 Date Header 中会注明请求时间）多少秒以内缓存是有效的，资源不需要重新从服务器中获取。相对时间避免了 Expires 中采用的绝对时间可能受客户端时钟影响的问题。s-maxage 中的“s”是“Share”的缩写，意味“共享缓存”的有效时间，即允许被 CDN、代理等持有的缓存有效时间，用于提示 CDN 这类服务器应在何时让缓存失效。 public和private：指明是否涉及到用户身份的私有资源，如果是 public，则可以被代理、CDN 等缓存，如果是 private，则只能由用户的客户端进行私有缓存。 no-cache和no-store：no-cache 指明该资源不应该被缓存，哪怕是同一个会话中对同一个 URL 地址的请求，也必须从服务端获取，令强制缓存完全失效，但此时下一节中的协商缓存机制依然是生效的；no-store 不强制会话中相同 URL 资源的重复获取，但禁止浏览器、CDN 等以任何形式保存该资源。 no-transform：禁止资源被任何形式地修改。譬如，某些 CDN、透明代理支持自动 GZip 压缩图片或文本，以提升网络性能，而 no-transform 就禁止了这样的行为，它要求 Content-Encoding、Content-Range、Content-Type 均不允许进行任何形式的修改。 min-fresh和only-if-cached：这两个参数是仅用于客户端的请求 Header。min-fresh 后续跟随一个以秒为单位的数字，用于建议服务器能返回一个不少于该时间的缓存资源（即包含 max-age 且不少于 min-fresh 的数字）。only-if-cached 表示客户端要求不必给它发送资源的具体内容，此时客户端就仅能使用事先缓存的资源来进行响应，若缓存不能命中，就直接返回 503/Service Unavailable 错误。 must-revalidate和proxy-revalidate：must-revalidate 表示在资源过期后，一定需要从服务器中进行获取，即超过了 max-age 的时间后，就等同于 no-cache 的行为，proxy-revalidate 用于提示代理、CDN 等设备资源过期后的缓存行为，除对象不同外，语义与 must-revalidate 完全一致。 协商缓存强制缓存是基于时效性的，但无论是人还是服务器，其实多数情况下都并没有什么把握去承诺某项资源多久不会发生变化。另外一种基于变化检测的缓存机制，在一致性上会有比强制缓存更好的表现，但需要一次变化检测的交互开销，性能上就会略差一些，这种基于检测的缓存机制，通常被称为“协商缓存”。另外，应注意在 HTTP 中协商缓存与强制缓存并没有互斥性，这两套机制是并行工作的，譬如，当强制缓存存在时，直接从强制缓存中返回资源，无须进行变动检查；而当强制缓存超过时效，或者被禁止（no-cache / must-revalidate），协商缓存仍可以正常地工作。 协商缓存有两种变动检查机制，分别是根据资源的修改时间进行检查，以及根据资源唯一标识是否发生变化来进行检查，它们都是靠一组成对出现的请求、响应 Header 来实现的： Last-Modified 和 If-Modified-Since：Last-Modified 是服务器的响应 Header，用于告诉客户端这个资源的最后修改时间。对于带有这个 Header 的资源，当客户端需要再次请求时，会通过 If-Modified-Since 把之前收到的资源最后修改时间发送回服务端。 如果此时服务端发现资源在该时间后没有被修改过，就只要返回一个 304/Not Modified 的响应即可，无须附带消息体，达到节省流量的目的，如下所示： 123HTTP/1.1 304 Not ModifiedCache-Control: public, max-age=600Last-Modified: Wed, 8 Apr 2020 15:31:30 GMT 如果此时服务端发现资源在该时间之后有变动，就会返回 200/OK 的完整响应，在消息体中包含最新的资源，如下所示： 1234HTTP/1.1 200 OKCache-Control: public, max-age=600Last-Modified: Wed, 8 Apr 2020 15:31:30 GMTContent Etag 和 If-None-Match：Etag 是服务器的响应 Header，用于告诉客户端这个资源的唯一标识。HTTP 服务器可以根据自己的意愿来选择如何生成这个标识，譬如 Apache 服务器的 Etag 值默认是对文件的索引节点（INode），大小和最后修改时间进行哈希计算后得到的。对于带有这个 Header 的资源，当客户端需要再次请求时，会通过 If-None-Match 把之前收到的资源唯一标识发送回服务端。 如果此时服务端计算后发现资源的唯一标识与上传回来的一致，说明资源没有被修改过，就只要返回一个 304/Not Modified 的响应即可，无须附带消息体，达到节省流量的目的，如下所示： 123HTTP/1.1 304 Not ModifiedCache-Control: public, max-age=600ETag: &quot;28c3f612-ceb0-4ddc-ae35-791ca840c5fa&quot; 如果此时服务端发现资源的唯一标识有变动，就会返回 200/OK 的完整响应，在消息体中包含最新的资源，如下所示： 1234HTTP/1.1 200 OKCache-Control: public, max-age=600ETag: &quot;28c3f612-ceb0-4ddc-ae35-791ca840c5fa&quot;Content Etag 是 HTTP 中一致性最强的缓存机制，譬如，Last-Modified 标注的最后修改只能精确到秒级，如果某些文件在 1 秒钟以内，被修改多次的话，它将不能准确标注文件的修改时间；又或者如果某些文件会被定期生成，可能内容并没有任何变化，但 Last-Modified 却改变了，导致文件无法有效使用缓存，这些情况 Last-Modified 都有可能产生资源一致性问题，只能使用 Etag 解决。 Etag 却又是 HTTP 中性能最差的缓存机制，体现在每次请求时，服务端都必须对资源进行哈希计算，这比起简单获取一下修改时间，开销要大了很多。Etag 和 Last-Modified 是允许一起使用的，服务器会优先验证 Etag，在 Etag 一致的情况下，再去对比 Last-Modified，这是为了防止有一些 HTTP 服务器未将文件修改日期纳入哈希范围内。 到这里为止，HTTP 的协商缓存机制已经能很好地处理通过 URL 获取单个资源的场景，为什么要强调“单个资源”呢？在 HTTP 协议的设计中，一个 URL 地址是有可能能够提供多份不同版本的资源，譬如，一段文字的不同语言版本，一个文件的不同编码格式版本，一份数据的不同压缩方式版本，等等。因此针对请求的缓存机制，也必须能够提供对应的支持。为此，HTTP 协议设计了以 Accept*（Accept、Accept-Language、Accept-Charset、Accept-Encoding）开头的一套请求 Header 和对应的以 Content-*（Content-Language、Content-Type、Content-Encoding）开头的响应 Header，这些 Headers 被称为 HTTP 的内容协商机制。与之对应的，对于一个 URL 能够获取多个资源的场景中，缓存也同样也需要有明确的标识来获知根据什么内容来对同一个 URL 返回给用户正确的资源。这个就是 Vary Header 的作用，Vary 后面应该跟随一组其他 Header 的名字，譬如： 12HTTP/1.1 200 OKVary: Accept, User-Agent以上响应的含义是应该根据 MIME 类型和浏览器类型来缓存资源，获取资源时也需要根据请求 Header 中对应的字段来筛选出适合的资源版本。 根据约定，协商缓存不仅在浏览器的地址输入、页面链接跳转、新开窗口、前进、后退中生效，而且在用户主动刷新页面（F5）时也同样是生效的，只有用户强制刷新（Ctrl+F5）或者明确禁用缓存（譬如在 DevTools 中设定）时才会失效，此时客户端向服务端发出的请求会自动带有“Cache-Control: no-cache”。","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E9%80%8F%E6%98%8E%E5%A4%9A%E7%BA%A7%E5%88%86%E6%B5%81%E7%B3%BB%E7%BB%9F/%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%BC%93%E5%AD%98/"},{"title":"服务端缓存","text":"服务端缓存引入缓存的理由，总结起来无外乎以下两种： 为缓解 CPU 压力而做缓存：譬如把方法运行结果存储起来、把原本要实时计算的内容提前算好、把一些公用的数据进行复用，这可以节省 CPU 算力，顺带提升响应性能。 为缓解 I/O 压力而做缓存：譬如把原本对网络、磁盘等较慢介质的读写访问变为对内存等较快介质的访问，将原本对单点部件（如数据库）的读写访问变为到可扩缩部件（如缓存中间件）的访问，顺带提升响应性能。 缓存出发点是缓解 CPU 和 I/O 资源在峰值流量下的压力，“顺带”而非“专门”地提升响应性能。这里的言外之意是如果可以通过增强 CPU、I/O 本身的性能（譬如扩展服务器的数量）来满足需要的话，那升级硬件往往是更好的解决方案，即使需要一些额外的投入成本，也通常要优于引入缓存后可能带来的风险。 缓存属性通常，我们设计或者选择缓存至少会考虑以下四个维度的属性： 吞吐量：缓存的吞吐量使用 OPS 值（每秒操作数，Operations per Second，ops/s）来衡量，反映了对缓存进行并发读、写操作的效率，即缓存本身的工作效率高低。 命中率：缓存的命中率即成功从缓存中返回结果次数与总请求次数的比值，反映了引入缓存的价值高低，命中率越低，引入缓存的收益越小，价值越低。 扩展功能：缓存除了基本读写功能外，还提供哪些额外的管理功能，譬如最大容量、失效时间、失效事件、命中率统计，等等。 分布式支持：缓存可分为“进程内缓存”和“分布式缓存”两大类，前者只为节点本身提供服务，无网络访问操作，速度快但缓存的数据不能在各个服务节点中共享，后者则相反。 吞吐量并发读写的场景中，吞吐量受多方面因素的共同影响，譬如，怎样设计数据结构以尽可能避免数据竞争，存在竞争风险时怎样处理同步（主要有使用锁实现的悲观同步和使用CAS实现的乐观同步）、如何避免伪共享现象（False Sharing，这也算是典型缓存提升开发复杂度的例子）发生，等等。其中第一点尽可能避免竞争是最关键的，无论如何实现同步都不会比直接无须同步更快。 命中率与淘汰策略有限的物理存储决定了任何缓存的容量都不可能是无限的，所以缓存需要在消耗空间与节约时间之间取得平衡，这要求缓存必须能够自动或者由人工淘汰掉缓存中的低价值数据，由人工管理的缓存淘汰主要取决于开发者如何编码，不能一概而论，这里只讨论由缓存自动进行淘汰的情况。“缓存如何自动地实现淘汰低价值目标”，被称为缓存的淘汰策略，也常称作替换策略或者清理策略。 最基础的淘汰策略实现方案有以下三种： FIFO（First In First Out）：优先淘汰最早进入被缓存的数据。FIFO 实现十分简单，但一般来说它并不是优秀的淘汰策略，越是频繁被用到的数据，往往会越早被存入缓存之中。如果采用这种淘汰策略，很可能会大幅降低缓存的命中率。 LRU（Least Recent Used）：优先淘汰最久未被使用访问过的数据。LRU 通常会采用 HashMap 加 LinkedList 双重结构（如 LinkedHashMap）来实现，以 HashMap 来提供访问接口，保证常量时间复杂度的读取性能，以 LinkedList 的链表元素顺序来表示数据的时间顺序，每次缓存命中时把返回对象调整到 LinkedList 开头，每次缓存淘汰时从链表末端开始清理数据。对大多数的缓存场景来说，LRU 都明显要比 FIFO 策略合理，尤其适合用来处理短时间内频繁访问的热点对象。但相反，它的问题是如果一些热点数据在系统中经常被频繁访问，但最近一段时间因为某种原因未被访问过，此时这些热点数据依然要面临淘汰的命运，LRU 依然可能错误淘汰价值更高的数据。 LFU（Least Frequently Used）：优先淘汰最不经常使用的数据。LFU 会给每个数据添加一个访问计数器，每访问一次就加 1，需要淘汰时就清理计数器数值最小的那批数据。LFU 可以解决上面 LRU 中热点数据间隔一段时间不访问就被淘汰的问题，但同时它又引入了两个新的问题，首先是需要对每个缓存的数据专门去维护一个计数器，每次访问都要更新，这样做会带来高昂的维护开销；另一个问题是不便于处理随时间变化的热度变化，譬如某个曾经频繁访问的数据现在不需要了，它也很难自动被清理出缓存。 以 LFU 分支为例，针对它存在的两个问题，近年来提出的 TinyLFU 和 W-TinyLFU 算法： TinyLFU（Tiny Least Frequently Used）：TinyLFU 是 LFU 的改进版本。为了缓解 LFU 每次访问都要修改计数器所带来的性能负担，TinyLFU 会首先采用 Sketch 对访问数据进行分析，所谓 Sketch 是统计学上的概念，指用少量的样本数据来估计全体数据的特征，这种做法显然牺牲了一定程度的准确性，但是只要样本数据与全体数据具有相同的概率分布，Sketch 得出的结论仍不失为一种高效与准确之间权衡的有效结论。借助Count–Min Sketch算法（可视为布隆过滤器的一种等价变种结构），TinyLFU 可以用相对小得多的记录频率和空间来近似地找出缓存中的低价值数据。为了解决 LFU 不便于处理随时间变化的热度变化问题，TinyLFU 采用了基于“滑动时间窗”（在“流量控制”中我们会更详细地分析这种算法）的热度衰减算法，简单理解就是每隔一段时间，便会把计数器的数值减半，以此解决“旧热点”数据难以清除的问题。 W-TinyLFU（Windows-TinyLFU）：W-TinyLFU 又是 TinyLFU 的改进版本。TinyLFU 在实现减少计数器维护频率的同时，也带来了无法很好地应对稀疏突发访问的问题，所谓稀疏突发访问是指有一些绝对频率较小，但突发访问频率很高的数据，譬如某些运维性质的任务，也许一天、一周只会在特定时间运行一次，其余时间都不会用到，此时 TinyLFU 就很难让这类元素通过 Sketch 的过滤，因为它们无法在运行期间积累到足够高的频率。应对短时间的突发访问是 LRU 的强项，W-TinyLFU 就结合了 LRU 和 LFU 两者的优点，从整体上看是 LFU 策略，从局部实现上看又是 LRU 策略。具体做法是将新记录暂时放入一个名为 Window Cache 的前端 LRU 缓存里面，让这些对象可以在 Window Cache 中累积热度，如果能通过 TinyLFU 的过滤器，再进入名为 Main Cache 的主缓存中存储，主缓存根据数据的访问频繁程度分为不同的段（LFU 策略，实际上 W-TinyLFU 只分了两段），但单独某一段局部来看又是基于 LRU 策略去实现的（称为 Segmented LRU）。每当前一段缓存满了之后，会将低价值数据淘汰到后一段中去存储，直至最后一段也满了之后，该数据就彻底清理出缓存。 扩展功能一般来说，一套标准的 Map 接口（或者来自JSR 107的 javax.cache.Cache 接口）就可以满足缓存访问的基本需要，不过在“访问”之外，专业的缓存往往还会提供很多额外的功能。笔者简要列举如下： 加载器：许多缓存都有“CacheLoader”之类的设计，加载器可以让缓存从只能被动存储外部放入的数据，变为能够主动通过加载器去加载指定 Key 值的数据，加载器也是实现自动刷新功能的基础前提。 淘汰策略：有的缓存淘汰策略是固定的，也有一些缓存能够支持用户自己根据需要选择不同的淘汰策略。 失效策略：要求缓存的数据在一定时间后自动失效（移除出缓存）或者自动刷新（使用加载器重新加载）。 事件通知：缓存可能会提供一些事件监听器，让你在数据状态变动（如失效、刷新、移除）时进行一些额外操作。有的缓存还提供了对缓存数据本身的监视能力（Watch 功能）。 并发级别：对于通过分段加锁来实现的缓存（以 Guava Cache 为代表），往往会提供并发级别的设置。可以简单将其理解为缓存内部是使用多个 Map 来分段存储数据的，并发级别就用于计算出使用 Map 的数量。如果将这个参数设置过大，会引入更多的 Map，需要额外维护这些 Map 而导致更大的时间和空间上的开销；如果设置过小，又会导致在访问时产生线程阻塞，因为多个线程更新同一个 ConcurrentMap 的同一个值时会产生锁竞争。 容量控制：缓存通常都支持指定初始容量和最大容量，初始容量目的是减少扩容频率，这与 Map 接口本身的初始容量含义是一致的。最大容量类似于控制 Java 堆的-Xmx 参数，当缓存接近最大容量时，会自动清理掉低价值的数据。 引用方式：支持将数据设置为软引用或者弱引用，提供引用方式的设置是为了将缓存与 Java 虚拟机的垃圾收集机制联系起来。 统计信息：提供诸如缓存命中率、平均加载时间、自动回收计数等统计。 持久化：支持将缓存的内容存储到数据库或者磁盘中，进程内缓存提供持久化功能的作用不是太大，但分布式缓存大多都会考虑提供持久化功能。 分布式缓存比起缓存数据在进程内存中读写的速度，一旦涉及网络访问，由网络传输、数据复制、序列化和反序列化等操作所导致的延迟要比内存访问高得多，所以对分布式缓存来说，处理与网络有相关的操作是对吞吐量影响更大的因素，往往也是比淘汰策略、扩展功能更重要的关注点。 从访问的角度来说，对于甚少更新但频繁读取的数据，理论上更适合做复制式缓存；对于更新和读取都较为频繁的数据，理论上就更适合做集中式缓存。 复制式缓存：复制式缓存可以看作是“能够支持分布式的进程内缓存”，它的工作原理与 Session 复制类似。缓存中所有数据在分布式集群的每个节点里面都存在有一份副本，读取数据时无须网络访问，直接从当前节点的进程内存中返回，理论上可以做到与进程内缓存一样高的读取性能；当数据发生变化时，就必须遵循复制协议，将变更同步到集群的每个节点中，复制性能随着节点的增加呈现平方级下降，变更数据的代价十分高昂。 复制式缓存的代表是JBossCache，这是 JBoss 针对企业级集群设计的缓存方案，支持 JTA 事务，依靠 JGroup 进行集群节点间数据同步。以 JBossCache 为典型的复制式缓存曾有一段短暂的兴盛期，但今天基本上已经很难再见到使用这种缓存形式的大型信息系统了，JBossCache 被淘汰的主要原因是写入性能实在差到不堪入目的程度，它在小规模集群中同步数据尚算差强人意，但在大规模集群下，很容易就因网络同步的速度跟不上写入速度，进而导致在内存中累计大量待重发对象，最终引发 OutOfMemory 崩溃。如果对 JBossCache 没有足够了解的话，稍有不慎就要被埋进坑里。 为了缓解复制式同步的写入效率问题，JBossCache 的继任者Infinispan提供了另一种分布式同步模式（这种同步模式的名字就叫做“分布式”），允许用户配置数据需要复制的副本数量，譬如集群中有八个节点，可以要求每个数据只保存四份副本，此时，缓存的总容量相当于是传统复制模式的一倍，如果要访问的数据在本地缓存中没有存储，Infinispan 完全有能力感知网络的拓扑结构，知道应该到哪些节点中寻找数据。 集中式缓存：*集中式缓存是目前分布式缓存的主流形式，集中式缓存的读、写都需要网络访问，其好处是不会随着集群节点数量的增加而产生额外的负担，其坏处自然是读、写都不再可能达到进程内缓存那样的高性能。 集中式缓存还有一个必须提到的关键特点，它与使用缓存的应用分处在独立的进程空间中，其好处是它能够为异构语言提供服务，譬如用 C 语言编写的Memcached完全可以毫无障碍地为 Java 语言编写的应用提供缓存服务；但其坏处是如果要缓存对象等复杂类型的话，基本上就只能靠序列化来支撑具体语言的类型系统（支持 Hash 类型的缓存，可以部分模拟对象类型），不仅有序列化的成本，还很容易导致传输成本也显著增加。举个例子，假设某个有 100 个字段的大对象变更了其中 1 个字段的值，通常缓存也不得不把整个对象所有内容重新序列化传输出去才能实现更新，因此，一般集中式缓存更提倡直接缓存原始数据类型而不是对象。相比之下，JBossCache 通过它的字节码自审（Introspection）功能和树状存储结构（TreeCache），做到了自动跟踪、处理对象的部分变动，用户修改了对象中哪些字段的数据，缓存就只会同步对象中真正变更那部分数据。 如今Redis广为流行，基本上已经打败了 Memcached 及其他集中式缓存框架，成为集中式缓存的首选，甚至可以说成为了分布式缓存的实质上的首选，几乎到了不必管读取、写入哪种操作更频繁，都可以无脑上 Redis 的程度。尽管 Redis 最初设计的本意是 NoSQL 数据库而不是专门用来做缓存的，可今天它确实已经成为许多分布式系统中无可或缺的基础设施，广泛用作缓存的实现方案。 从数据一致性角度说，缓存本身也有集群部署的需求，理论上你应该认真考虑一下是否能接受不同节点取到的缓存数据有可能存在差异。譬如刚刚放入缓存中的数据，另外一个节点马上访问发现未能读到；刚刚更新缓存中的数据，另外一个节点访问在短时间内读取到的仍是旧的数据，等等。根据分布式缓存集群是否能保证数据一致性，可以将它分为 AP 和 CP 两种类型。此处又一次出现了“理论上”，是因为我们实际开发中通常不太会把追求强一致性的数据使用缓存来处理，可以这样做，但是没必要（可类比 MESI 等缓存一致性协议）。譬如，Redis 集群就是典型的 AP 式，有着高性能高可用等特点，却并不保证强一致性。而能够保证强一致性的 ZooKeeper、Doozerd、Etcd 等分布式协调框架，通常不会有人将它们当为“缓存框架”来使用，这些分布式协调框架的吞吐量相对 Redis 来说是非常有限的。不过 ZooKeeper、Doozerd、Etcd 倒是常与 Redis 和其他分布式缓存搭配工作，用来实现其中的通知、协调、队列、分布式锁等功能。 缓存风险缓存穿透 （key 不存在， 数据库也不存在）缓存的目的是为了缓解 CPU 或者 I/O 的压力，譬如对数据库做缓存，大部分流量都从缓存中直接返回，只有缓存未能命中的数据请求才会流到数据库中，这样数据库压力自然就减小了。但是如果查询的数据在数据库中根本不存在的话，缓存里自然也不会有，这类请求的流量每次都不会命中，每次都会触及到末端的数据库，缓存就起不到缓解压力的作用了，这种查询不存在数据的现象被称为缓存穿透。 缓存穿透有可能是业务逻辑本身就存在的固有问题，也有可能是被恶意攻击的所导致，为了解决缓存穿透，通常会采取下面两种办法： 对于业务逻辑本身就不能避免的缓存穿透，可以约定在一定时间内对返回为空的 Key 值依然进行缓存（注意是正常返回但是结果为空，不应把抛异常的也当作空值来缓存了），使得在一段时间内缓存最多被穿透一次。如果后续业务在数据库中对该 Key 值插入了新记录，那应当在插入之后主动清理掉缓存的 Key 值。如果业务时效性允许的话，也可以将对缓存设置一个较短的超时时间来自动处理。 对于恶意攻击导致的缓存穿透，通常会在缓存之前设置一个布隆过滤器来解决。所谓恶意攻击是指请求者刻意构造数据库中肯定不存在的 Key 值，然后发送大量请求进行查询。布隆过滤器是用最小的代价来判断某个元素是否存在于某个集合的办法。如果布隆过滤器给出的判定结果是请求的数据不存在，那就直接返回即可，连缓存都不必去查。虽然维护布隆过滤器本身需要一定的成本，但比起攻击造成的资源损耗仍然是值得的。 缓存击穿 （key 失效， 数据库存在）我们都知道缓存的基本工作原理是首次从真实数据源加载数据，完成加载后回填入缓存，以后其他相同的请求就从缓存中获取数据，缓解数据源的压力。如果缓存中某些热点数据忽然因某种原因失效了，譬如典型地由于超期而失效，此时又有多个针对该数据的请求同时发送过来，这些请求将全部未能命中缓存，都到达真实数据源中去，导致其压力剧增，这种现象被称为缓存击穿。要避免缓存击穿问题，通常会采取下面的两种办法： 加锁同步，以请求该数据的 Key 值为锁，使得只有第一个请求可以流入到真实的数据源中，其他线程采取阻塞或重试策略。如果是进程内缓存出现问题，施加普通互斥锁即可，如果是分布式缓存中出现的问题，就施加分布式锁，这样数据源就不会同时收到大量针对同一个数据的请求了。 热点数据由代码来手动管理，缓存击穿是仅针对热点数据被自动失效才引发的问题，对于这类数据，可以直接由开发者通过代码来有计划地完成更新、失效，避免由缓存的策略自动管理。 缓存雪崩缓存击穿是针对单个热点数据失效，由大量请求击穿缓存而给真实数据源带来压力。有另一种可能是更普遍的情况，不需要是针对单个热点数据的大量请求，而是由于大批不同的数据在短时间内一起失效，导致了这些数据的请求都击穿了缓存到达数据源，同样令数据源在短时间内压力剧增。 出现这种情况，往往是系统有专门的缓存预热功能，也可能大量公共数据是由某一次冷操作加载的，这样都可能出现由此载入缓存的大批数据具有相同的过期时间，在同一时刻一起失效。还有一种情况是缓存服务由于某些原因崩溃后重启，此时也会造成大量数据同时失效，这种现象被称为缓存雪崩。要避免缓存雪崩问题，通常会采取下面的三种办法： 提升缓存系统可用性，建设分布式缓存的集群。 启用透明多级缓存，各个服务节点一级缓存中的数据通常会具有不一样的加载时间，也就分散了它们的过期时间。 将缓存的生存期从固定时间改为一个时间段内的随机时间，譬如原本是一个小时过期，那可以缓存不同数据时，设置生存期为 55 分钟到 65 分钟之间的某个随机时间。","link":"/2023/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E9%80%8F%E6%98%8E%E5%A4%9A%E7%BA%A7%E5%88%86%E6%B5%81%E7%B3%BB%E7%BB%9F/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%BC%93%E5%AD%98/"},{"title":"负载均衡（Load Balancing","text":"负载均衡（Load Balancing）调度后方的多台机器，以统一的接口对外提供服务，承担此职责的技术组件被称为“负载均衡”。 无论在网关内部建立了多少级的负载均衡，从形式上来说都可以分为两种：四层负载均衡和七层负载均衡。 四层负载均衡的优势是性能高，七层负载均衡的优势是功能强。 做多级混合负载均衡，通常应是低层的负载均衡在前，高层的负载均衡在后。 OSI 七层模型： 层 数据单元 功能 7 应用层 数据Data 提供为应用软件提供服务的接口，用于与其他应用软件之间的通信典型协议：HTTP、HTTPS、FTP、Telnet、SSH、SMTP、POP3 等 6 表达层 数据Data 把数据转换为能与接收者的系统格式兼容并适合传输的格式 5 会话层 数据Data 负责在数据传输中设置和维护计算机网络中两台计算机之间的通信连接 4 传输层 数据段Segments 把传输表头加至数据以形成数据包。传输表头包含了所使用的协议等发送信息典型协议：TCP、UDP、RDP、SCTP、FCP 等 3 网络层 数据包Packets 决定数据的传输路径选择和转发，将网络表头附加至数据段后以形成报文（即数据包）典型协议：IPv4/IPv6、IGMP、ICMP、EGP、RIP 等 2 数据链路层 数据帧Frame 负责点对点的网络寻址、错误侦测和纠错。当表头和表尾被附加至数据包后，就形成数据帧（Frame）典型协议：WiFi（802.11）、Ethernet（802.3）、PPP 等 1 物理层 比特流Bit 在物理网络上传送数据帧，它负责管理电脑通信设备和网络媒体之间的互通。包括了针脚、电压、线缆规范、集线器、中继器、网卡、主机接口卡等 所说的“四层负载均衡”其实是多种均衡器工作模式的统称，“四层”的意思是说这些工作模式的共同特点是维持着同一个 TCP 连接，而不是说它只工作在第四层。事实上，这些模式主要都是工作在二层（数据链路层，改写 MAC 地址）和三层（网络层，改写 IP 地址）上，单纯只处理第四层（传输层，可以改写 TCP、UDP 等协议的内容和端口）的数据无法做到负载均衡的转发，因为 OSI 的下三层是媒体层（Media Layers），上四层是主机层（Host Layers），既然流量都已经到达目标主机上了，也就谈不上什么流量转发，最多只能做代理了。但出于习惯和方便，现在几乎所有的资料都把它们统称为四层负载均衡，笔者也同样称呼它为四层负载均衡. 数据链路层负载均衡数据链路层负载均衡所做的工作，是修改请求的数据帧中的 MAC 目标地址，让用户原本是发送给负载均衡器的请求的数据帧，被二层交换机根据新的 MAC 目标地址转发到服务器集群中对应的服务器（后文称为“真实服务器”，Real Server）的网卡上，这样真实服务器就获得了一个原本目标并不是发送给它的数据帧。 由于二层负载均衡器在转发请求过程中只修改了帧的 MAC 目标地址，不涉及更上层协议（没有修改 Payload 的数据），所以在更上层（第三层）看来，所有数据都是未曾被改变过的。由于第三层的数据包，即 IP 数据包中包含了源（客户端）和目标（均衡器）的 IP 地址，只有真实服务器保证自己的 IP 地址与数据包中的目标 IP 地址一致，这个数据包才能被正确处理。因此，使用这种负载均衡模式时，需要把真实物理服务器集群所有机器的虚拟 IP 地址（Virtual IP Address，VIP）配置成与负载均衡器的虚拟 IP 一样，这样经均衡器转发后的数据包就能在真实服务器中顺利地使用。也正是因为实际处理请求的真实物理服务器 IP 和数据请求中的目的 IP 是一致的，所以响应结果就不再需要通过负载均衡服务器进行地址交换，可将响应结果的数据包直接从真实服务器返回给用户的客户端，避免负载均衡器网卡带宽成为瓶颈，因此数据链路层的负载均衡效率是相当高的。 上述只有请求经过负载均衡器，而服务的响应无须从负载均衡器原路返回的工作模式，整个请求、转发、响应的链路形成一个“三角关系”，所以这种负载均衡模式也常被很形象地称为“三角传输模式”（Direct Server Return，DSR），也有叫“单臂模式”（Single Legged Mode）或者“直接路由”（Direct Routing）。 虽然数据链路层负载均衡效率很高，但它并不能适用于所有的场合，除了那些需要感知应用层协议信息的负载均衡场景它无法胜任外（所有的四层负载均衡器都无法胜任，将在后续介绍七层均衡器时一并解释），它在网络一侧受到的约束也很大。二层负载均衡器直接改写目标 MAC 地址的工作原理决定了它与真实的服务器的通信必须是二层可达的，通俗地说就是必须位于同一个子网当中，无法跨 VLAN。优势（效率高）和劣势（不能跨子网）共同决定了数据链路层负载均衡最适合用来做数据中心的第一级均衡设备，用来连接其他的下级负载均衡器。 网络层负载均衡根据 OSI 七层模型，在第三层网络层传输的单位是分组数据包（Packets），这是一种在分组交换网络（Packet Switching Network，PSN）中传输的结构化数据单位。以 IP 协议为例，一个 IP 数据包由 Headers 和 Payload 两部分组成， Headers 长度最大为 60 Bytes，其中包括了 20 Bytes 的固定数据和最长不超过 40 Bytes 的可选的额外设置组成。 IP 分组数据包的 Headers 带有源和目标的 IP 地址，通过改变这里面的 IP 地址来实现数据包的转发。具体有两种常见的修改方式。 IP 隧道”（IP Tunnel）传输保持原来的数据包不变，新创建一个数据包，把原来数据包的 Headers 和 Payload 整体作为另一个新的数据包的 Payload，在这个新数据包的 Headers 中写入真实服务器的 IP 作为目标地址，然后把它发送出去。经过三层交换机的转发，真实服务器收到数据包后，必须在接收入口处设计一个针对性的拆包机制，把由负载均衡器自动添加的那层 Headers 扔掉，还原出原来的数据包来进行使用。这样，真实服务器就同样拿到了一个原本不是发给它（目标 IP 不是它）的数据包，达到了流量转发的目的。 IP 隧道的转发模式比起直接路由模式效率会有所下降，但由于并没有修改原有数据包中的任何信息，所以 IP 隧道的转发模式仍然具备三角传输的特性，即负载均衡器转发来的请求，可以由真实服务器去直接应答，无须在经过均衡器原路返回。而且由于 IP 隧道工作在网络层，所以可以跨越 VLAN，因此摆脱了直接路由模式中网络侧的约束。 NAT 模式直接把数据包 Headers 中的目标地址改掉，修改后原本由用户发给均衡器的数据包，也会被三层交换机转发送到真实服务器的网卡上，而且因为没有经过 IP 隧道的额外包装，也就无须再拆包了。但问题是这种模式是通过修改目标 IP 地址才到达真实服务器的，如果真实服务器直接将应答包返回客户端的话，这个应答数据包的源 IP 是真实服务器的 IP，也即均衡器修改以后的 IP 地址，客户端不可能认识该 IP，自然就无法再正常处理这个应答了。因此，只能让应答流量继续回到负载均衡，由负载均衡把应答包的源 IP 改回自己的 IP，再发给客户端，这样才能保证客户端与真实服务器之间的正常通信。 应用层负载均衡四层负载均衡工作模式都属于“转发”，即直接将承载着 TCP 报文的底层数据格式（IP 数据包或以太网帧）转发到真实服务器上，此时客户端到响应请求的真实服务器维持着同一条 TCP 通道。但工作在四层之后的负载均衡模式就无法再进行转发了，只能进行代理，此时真实服务器、负载均衡器、客户端三者之间由两条独立的 TCP 通道来维持通信。 “代理”这个词，根据“哪一方能感知到”的原则，可以分为“正向代理”、“反向代理”和“透明代理”三类。 正向代理：正向代理是为客户端服务的代理，它代表客户端去访问目标服务器。 反向代理：反向代理是为服务器服务的代理，它代表服务器接收来自客户端的请求。 透明代理：对双方都透明的，配置在网络中间设备上的代理服务，譬如，架设在路由器上的透明翻墙代理。 七层负载均衡器它就属于反向代理中的一种，它比四层均衡器至少多一轮 TCP 握手，有着跟 NAT 转发模式一样的带宽问题，而且通常要耗费更多的 CPU，但是可用的解析规则远比四层丰富。 均衡策略与实现 轮循均衡（Round Robin）：每一次来自网络的请求轮流分配给内部中的服务器，从 1 至 N 然后重新开始。此种均衡算法适合于集群中的所有服务器都有相同的软硬件配置并且平均服务请求相对均衡的情况。 权重轮循均衡（Weighted Round Robin）：根据服务器的不同处理能力，给每个服务器分配不同的权值，使其能够接受相应权值数的服务请求。譬如：服务器 A 的权值被设计成 1，B 的权值是 3，C 的权值是 6，则服务器 A、B、C 将分别接收到 10%、30％、60％的服务请求。此种均衡算法能确保高性能的服务器得到更多的使用率，避免低性能的服务器负载过重。 随机均衡（Random）：把来自客户端的请求随机分配给内部中的多个服务器，在数据足够大的场景下能达到相对均衡的分布。 权重随机均衡（Weighted Random）：此种均衡算法类似于权重轮循算法，不过在分配处理请求时是个随机选择的过程。 一致性哈希均衡（Consistency Hash）：根据请求中某一些数据（可以是 MAC、IP 地址，也可以是更上层协议中的某些参数信息）作为特征值来计算需要落在的节点上，算法一般会保证同一个特征值每次都一定落在相同的服务器上。一致性的意思是保证当服务集群某个真实服务器出现故障，只影响该服务器的哈希，而不会导致整个服务集群的哈希键值重新分布。 响应速度均衡（Response Time）：负载均衡设备对内部各服务器发出一个探测请求（例如 Ping），然后根据内部中各服务器对探测请求的最快响应时间来决定哪一台服务器来响应客户端的服务请求。此种均衡算法能较好的反映服务器的当前运行状态，但这最快响应时间仅仅指的是负载均衡设备与服务器间的最快响应时间，而不是客户端与服务器间的最快响应时间。 最少连接数均衡（Least Connection）：客户端的每一次请求服务在服务器停留的时间可能会有较大的差异，随着工作时间加长，如果采用简单的轮循或随机均衡算法，每一台服务器上的连接进程可能会产生极大的不平衡，并没有达到真正的负载均衡。最少连接数均衡算法对内部中需负载的每一台服务器都有一个数据记录，记录当前该服务器正在处理的连接数量，当有新的服务连接请求时，将把当前请求分配给连接数最少的服务器，使均衡更加符合实际情况，负载更加均衡。此种均衡策略适合长时处理的请求服务，如 FTP 传输。 从实现角度来看，负载均衡器的实现分为“软件均衡器”和“硬件均衡器”两类。在软件均衡器方面，又分为直接建设在操作系统内核的均衡器和应用程序形式的均衡器两种。前者的代表是 LVS（Linux Virtual Server），后者的代表有 Nginx、HAProxy、KeepAlived 等，前者性能会更好，因为无须在内核空间和应用空间中来回复制数据包；而后者的优势是选择广泛，使用方便，功能不受限于内核版本。","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E9%80%8F%E6%98%8E%E5%A4%9A%E7%BA%A7%E5%88%86%E6%B5%81%E7%B3%BB%E7%BB%9F/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"title":"java线程池的参数设置","text":"在说如何对线程池优化之前重复一下线程的7大参数 corePoolSize： 核心线程数，也是线程池中常驻的线程数，线程池初始化时默认是没有线程的，当任务来临时才开始创建线程去执行任务 maximumPoolSize： 最大线程数，在核心线程数的基础上可能会额外增加一些非核心线程，需要注意的是只有当workQueue队列填满时才会创建多于corePoolSize的线程(线程池总线程数不超过maxPoolSize) keepAliveTime： 非核心线程的空闲时间超过keepAliveTime就会被自动终止回收掉，注意当corePoolSize=maxPoolSize时，keepAliveTime参数也就不起作用了(因为不存在非核心线程)； unit： keepAliveTime的时间单位 workQueue： 用于保存任务的队列，可以为无界、有界、同步移交三种队列类型之一，当池子里的工作线程数大于corePoolSize时，这时新进来的任务会被放到队列中 threadFactory： 创建线程的工厂类，默认使用Executors.defaultThreadFactory()，也可以使用guava库的ThreadFactoryBuilder来创建 handler： 线程池无法继续接收任务(队列已满且线程数达到maximunPoolSize)时的饱和策略，取值有AbortPolicy、CallerRunsPolicy、DiscardOldestPolicy、DiscardPolicy 核心线程设置计算密集型当线程执行的是计算密集型的任务，那么要尽量防止线程的上下文切换，所以要设置的线程数要较少一点，一般设置为 线程数 = CPU核数+1，也可以设置成CPU核数*2 I/0密集型在 WEB 应用中 涉及到大量的 i/o传输 ， 一旦发生IO，线程就会处于等待状态，当IO结束，数据准备好后，线程才会继续执行。我们可以多设置一些线程池中线程的数量，这样就能让在等待IO的这段时间内，其他线程可以去做其它事，提高并发处理效率。对于IO密集型应用：线程数 = CPU核心数/(1-阻塞系数) 这个阻塞系数一般为0.8~0.9之间，也可以取0.8或者0.9。 其他参数设置 根据不同的场景选择不同适合的拒绝策略 ， 也可以实现RejectedExecutionHandler接口自定义拒绝策略 使用线程工厂创建线程时正确的对线程命名，便于排查问题 我们也可以调用shutdown来手动终止线程池。如果我们忘记调用shutdown，为了让线程资源被释放，我们还可以使用keepAliveTime 和 allowCoreThreadTimeOut来达到目的 ThreadPoolExecutor提供了protected类型可以被覆盖的钩子方法，我们可以使用beforeExecute和afterExecute来记录线程之前前和后的一些运行情况","link":"/2024/09/01/java/juc%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%9A%84%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/"},{"title":"ReentrantLock","text":"相对于 synchronized 可中断 可以设置超时时间 可以设置公平锁 （防止饥饿） 支持多个条件变量 （类似于支持多个 waitSet）与synchronized一样 ，都支持可重入 基本语法 12345678ReentrantLock reentrantLock = new ReentrantLock(); reentrantLock.lock(); try { //临界区 }finally { reentrantLock.unlock(); } 可重入锁可重入是指同一个线程如果首个获取了这把锁，那么因为他是这把锁的拥有者，因此有权利再次获取这把锁如果不是可重入锁，那么第二次获取锁，自己也会被挡住 123456789101112131415161718192021222324252627private static ReentrantLock lock = new ReentrantLock(); public static void main(String[] args) { lock.lock(); //lock加锁是不可打断的 try { log.info(&quot;main &quot;); m1(); }finally { lock.unlock(); } } static void m1(){ lock.lock(); try { log.info(&quot;m1 &quot;); m2(); }finally { lock.unlock(); } } static void m2(){ lock.lock(); try { log.info(&quot;m2&quot;); }finally { lock.unlock(); } } 可打断被动的被打断 避免死等 12345678910111213141516171819202122232425private static ReentrantLock lock = new ReentrantLock(); public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { // 没有竞争获取锁 //有竞争进入阻塞队列，可以被其他线程用 interruput 打断 try { log.info(&quot;尝试获得锁&quot;); lock.lockInterruptibly(); //可打断锁 防止无限制等待 ，可以防止死锁 } catch (InterruptedException e) { e.printStackTrace(); log.info(&quot;没有获得锁，返回&quot;); return; } try { log.info(&quot;获取到锁&quot;); } finally { lock.unlock(); } }, &quot;t1&quot;); lock.lock(); t1.start(); Thread.sleep(1000); log.info(&quot;打断t1&quot;); t1.interrupt(); } 锁超时123456789101112131415161718192021222324private static ReentrantLock lock = new ReentrantLock(); public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -&gt; { //if(!lock.tryLock()) //尝试获得锁 成功获得锁 失败不去阻塞队列等待 防止无限制等待 try { if(!lock.tryLock(2,TimeUnit.SECONDS)) //设置等待时间 也支持可打断的特性 { log.info(&quot;获取不到锁&quot;); return; } } catch (InterruptedException e) { e.printStackTrace(); log.info(&quot;没有获取锁，返回&quot;); return; } try { log.info(&quot;获取到锁&quot;); }finally { lock.unlock(); } }, &quot;t1&quot;); lock.lock(); t1.start(); } 公平锁RenntranLock 默认是不公平锁公平锁 可以解决饥饿问题 ， 设置公平锁会降低并发度 1public ReentrantLock(boolean fair) 构造函数传入是否是公平锁 条件变量synchronized 中也有条件变量，就是waitSet休息室，当条件不满足时进入waitSet等待 ReentranLock的条件变量比synchronized强大之处在于，支持多个条件变量 synchronized 时那些不满足条件的线程都在一间休息室等消息 而ReentranLock支持多间休息室，可以分类 使用流程 await 前需要获取锁 await 执行后，会释放锁，静茹conditionObject等待 await 的线程被唤醒（或打断，超时）重新竞争lock锁 竞争lock锁成功后，从await后继续执行 123456789101112static ReentrantLock lock = new ReentrantLock(); public static void main(String[] args) throws InterruptedException { //创建一个姓的条件变量 （休息室） Condition condition1 = lock.newCondition(); Condition condition2 = lock.newCondition(); lock.lock(); //进入休息室等待 condition1.await(); condition1.signal(); //唤醒等待的线程 condition2.signalAll(); // 唤醒所有等待的线程 } 同步线程之顺序控制 使用synchrinized wait（） notifyAll（） 12345678910111213141516171819202122232425262728static final Object lock = new Object();static boolean canRun = false;public static void main(String[] args) { Thread thread1 = new Thread(() -&gt; { synchronized (lock) { while (!canRun) //防止虚假唤醒 { try { lock.wait(); // 不允许打印时 等待 } catch (InterruptedException e) { e.printStackTrace(); } } log.info(&quot;1&quot;); } }, &quot;t1&quot;); Thread thread2 = new Thread(() -&gt; { synchronized (lock) { log.info(&quot;2&quot;); canRun = true; //设置允许打印 lock.notifyAll(); //唤醒所有等待线程 } }, &quot;t2&quot;); thread1.start(); thread2.start();} 使用RenntrinLock 123456789101112131415161718192021222324252627282930313233static final ReentrantLock lock = new ReentrantLock(); static Condition condition = lock.newCondition(); static boolean canRun = false; public static void main(String[] args) { Thread thread1 = new Thread(() -&gt; { try { while (!canRun &amp;&amp; lock.tryLock()) //尝试获得锁 { condition.await(); } } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } log.info(&quot;1&quot;); }, &quot;t1&quot;); Thread thread2 = new Thread(() -&gt; { lock.lock(); try { log.info(&quot;2&quot;); canRun = true; condition.signal(); }finally { lock.unlock(); } }, &quot;t2&quot;); thread1.start(); thread2.start(); } 使用park 和 unpark 1234567891011121314151617181920static boolean canRun = false; public static void main(String[] args) { Thread thread1 = new Thread(() -&gt; { while (!canRun) { LockSupport.park(); } log.info(&quot;1&quot;); }, &quot;t1&quot;); Thread thread2 = new Thread(() -&gt; { log.info(&quot;2&quot;); canRun = true; LockSupport.unpark(thread1); }, &quot;t2&quot;); thread1.start(); thread2.start(); } 同步线程之交替输出三个线程 t1输出 a t2输出 b t3输出c 让他们交替输出 abcabcabcabcabc 每个线程输出五次 使用synchrinized wait（） notifyAll（） 123456789101112131415161718192021222324252627282930313233343536373839404142434445 public static void main(String[] args) { WaitNatify waitNatify = new WaitNatify(1,5); Thread thread1 = new Thread(() -&gt; { waitNatify.print(&quot;a&quot;,1,2); }, &quot;t1&quot;); Thread thread2 = new Thread(() -&gt; { waitNatify.print(&quot;b&quot;,2,3); }, &quot;t2&quot;); Thread thread3 = new Thread(() -&gt; { waitNatify.print(&quot;c&quot;,3,1); }, &quot;t3&quot;); thread1.start(); thread2.start(); thread3.start(); }}class WaitNatify{ private int flag; private int loopNum; public WaitNatify(int flag, int loopNum) { this.flag = flag; this.loopNum = loopNum; } public void print(String str,int waitFlag,int nextFlag) { for (int i = 0; i &lt; loopNum; i++) { synchronized (this) { while (flag != waitFlag) { try { this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.printf(str); flag = nextFlag; this.notifyAll(); } } }} 使用ReentrinLock 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 public static void main(String[] args) throws InterruptedException { AwaitSignalAll awaitSignalAll = new AwaitSignalAll(5); Condition a = awaitSignalAll.newCondition(); Condition b = awaitSignalAll.newCondition(); Condition c = awaitSignalAll.newCondition(); Thread thread1 = new Thread(() -&gt; { awaitSignalAll.print(&quot;a&quot;, a,b); }, &quot;t1&quot;); Thread thread2 = new Thread(() -&gt; { awaitSignalAll.print(&quot;b&quot;, b,c); }, &quot;t2&quot;); Thread thread3 = new Thread(() -&gt; { awaitSignalAll.print(&quot;c&quot;, c,a); }, &quot;t3&quot;); thread1.start(); thread2.start(); thread3.start(); Thread.sleep(1000); awaitSignalAll.lock(); // 这里不获得锁 引起java.lang.IllegalMonitorStateException异常 （非法监视器状态异常） try{ a.signalAll(); //目前线程全部在休息室等待 ， 这里设置 第一次被唤醒的休息室的线程 }finally { awaitSignalAll.unlock(); } }}class AwaitSignalAll extends ReentrantLock { private int loopNum; public AwaitSignalAll(int loopNum) { this.loopNum = loopNum; } public void print(String str, Condition current,Condition next) { for (int i = 0; i &lt; loopNum; i++) { lock(); try { current.await(); //三个线程调用 全部进入自己的休息室等待 System.out.printf(str); //等待结束 打印str next.signalAll(); // 唤醒下一个应该执行的休息室内的线程 } catch (InterruptedException e) { e.printStackTrace(); } finally { unlock(); } } }} 使用park 和 unpark 12345678910111213141516171819202122232425262728293031323334 static Thread thread1; static Thread thread2; static Thread thread3; public static void main(String[] args) throws InterruptedException { ParkUnpark parkUnpark = new ParkUnpark(5); thread1 = new Thread(() -&gt; { parkUnpark.print(&quot;a&quot;, thread2); }, &quot;t1&quot;); thread2 = new Thread(() -&gt; { parkUnpark.print(&quot;b&quot;, thread3); }, &quot;t2&quot;); thread3 = new Thread(() -&gt; { parkUnpark.print(&quot;c&quot;, thread1); }, &quot;t3&quot;); thread1.start(); thread2.start(); thread3.start(); LockSupport.unpark(thread1); }}class ParkUnpark { private int loopNum; public ParkUnpark(int loopNum) { this.loopNum = loopNum; } public void print(String str, Thread next) { for (int i = 0; i &lt; loopNum; i++) { LockSupport.park(); System.out.printf(str); LockSupport.unpark(next); } }} AQS AbstractQueuedSynchronizer全称是 AbstractQueuedSynchronizer，是阻塞式锁和相关的同步器工具的框架 用 state 属性来表示资源的状态（分独占模式和共享模式），子类需要定义如何维护这个状态，控制如何获取锁和释放锁 getState - 获取 state 状态 setState - 设置 state 状态 compareAndSetState - cas 机制设置 state 状态 独占模式是只有一个线程能够访问资源，而共享模式可以允许多个线程访问资源 提供了基于 FIFO 的等待队列，类似于 Monitor 的 EntryList 条件变量来实现等待、唤醒机制，支持多个条件变量，类似于 Monitor 的 WaitSet 子类主要实现这样一些方法（默认抛出 UnsupportedOperationException) tryAcquire //获取锁 tryRelease // 释放锁 tryAcquireShared tryReleaseShared isHeldExclusively 1234// 如果获取锁失败if (!tryAcquire(arg)) { // 入队, 可以选择阻塞当前线程 实际用了 park unpark 方法} 1234// 如果释放锁成功if (tryRelease(arg)) { // 让阻塞线程恢复运行} 不可重入锁 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public static void main(String[] args) { Mylock mylock = new Mylock(); new Thread(() -&gt; { mylock.lock(); try { System.out.println(&quot;枷锁成功&quot;); TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } finally { mylock.unlock(); System.out.println(&quot;解锁成功&quot;); } }).start(); new Thread(() -&gt; { mylock.lock(); try { System.out.println(&quot;枷锁成功&quot;); }finally { mylock.unlock(); System.out.println(&quot;解锁成功&quot;); } }).start(); }}// 自定义不可重入锁class Mylock implements Lock { //同步器类 独占锁 class MySync extends AbstractQueuedLongSynchronizer{ @Override // 获取锁 protected boolean tryAcquire(long arg) { if(compareAndSetState(0,1)) { // 加锁成功 设置线程位当前线程 setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } @Override //释放锁 protected boolean tryRelease(long arg) { setExclusiveOwnerThread(null); setState(0); // state 是volatile 修饰，具有写屏障 保证之前的变量可见性 return true; } @Override //是否持有独占锁 protected boolean isHeldExclusively() { return getState() == 1; } // 创建条件变量 public Condition newCondition(){ return new ConditionObject(); } } private MySync sync = new MySync(); @Override // 加锁 （不成功进入等待队列 ） public void lock() { sync.acquire(1); } @Override // 可打断锁 public void lockInterruptibly() throws InterruptedException { sync.acquireInterruptibly(1); } @Override // 可打断锁 尝试一次 public boolean tryLock() { return sync.tryAcquire(1); } @Override // 尝试加锁 （带超时时间） public boolean tryLock(long time, TimeUnit unit) throws InterruptedException { return sync.tryAcquireNanos(1,unit.toNanos(time)); } @Override // 解锁 public void unlock() { sync.release(1); } @Override // 创建条件变量 public Condition newCondition() { return sync.newCondition(); }} ReentrinLock原理构造器 123public ReentrantLock() { sync = new NonfairSync(); // 非公平锁 } // 加锁方法 123456final void lock() { if (compareAndSetState(0, 1)) // 改变 锁状态为加锁 setExclusiveOwnerThread(Thread.currentThread()); //更改锁线程为当前线程 else acquire(1); // 加锁失败 } 123456public final void acquire(int arg) { // 尝试加锁 放入阻塞队列 addWaiter （） if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 尝试加锁acquireQueued 123456789101112131415161718192021final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { // 死循环 尝试获得锁 final Node p = node.predecessor(); //前驱节点 if (p == head &amp;&amp; tryAcquire(arg)) { // 如果前驱节点是头节点 说明头节点是第二的 才有资格尝试获得锁 setHead(node); p.next = null; // help GC failed = false; return interrupted; } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // 尝试获取锁失败 parkAndCheckInterrupt()) //阻塞 interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 释放锁","link":"/2024/09/01/java/juc%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/ReentrantLock/"},{"title":"透明多级分流系统","text":"在用户使用信息系统的过程中，请求从浏览器出发，在域名服务器的指引下找到系统的入口，经过网关、负载均衡器、缓存、服务集群等一系列设施，最后触及到末端存储于数据库服务器中的信息，然后逐级返回到用户的浏览器之中。这其中要经过很多技术部件。作为系统的设计者，我们应该意识到不同的设施、部件在系统中有各自不同的价值。 一些部件位于客户端或网络的边缘，能够迅速响应用户的请求，避免给后方的 I/O 与 CPU 带来压力，典型如本地缓存、内容分发网络、反向代理等。 一些部件的处理能力能够线性拓展，易于伸缩，可以使用较小的代价堆叠机器来获得与用户数量相匹配的并发性能，应尽量作为业务逻辑的主要载体，典型如集群中能够自动扩缩的服务节点。 一些部件稳定服务对系统运行有全局性的影响，要时刻保持着容错备份，维护着高可用性，典型如服务注册中心、配置中心。 一些设施是天生的单点部件，只能依靠升级机器本身的网络、存储和运算性能来提升处理能力，如位于系统入口的路由、网关或者负载均衡器（它们都可以做集群，但一次网络请求中无可避免至少有一个是单点的部件）、位于请求调用链末端的传统关系数据库等，都是典型的容易形成单点部件。 对系统进行流量规划时，我们应该充分理解这些部件的价值差异，有两条简单、普适的原则能指导我们进行设计： 第一条原则是尽可能减少单点部件，如果某些单点是无可避免的，则应尽最大限度减少到达单点部件的流量。在系统中往往会有多个部件能够处理、响应用户请求，譬如要获取一张存储在数据库的用户头像图片，浏览器缓存、内容分发网络、反向代理、Web 服务器、文件服务器、数据库都可能提供这张图片。恰如其分地引导请求分流至最合适的组件中，避免绝大多数流量汇集到单点部件（如数据库），同时依然能够在绝大多数时候保证处理结果的准确性，使单点系统在出现故障时自动而迅速地实施补救措施，这便是系统架构中多级分流的意义。 另一条更关键的原则是奥卡姆剃刀原则。作为一名架构设计者，你应对多级分流的手段有全面的理解与充分的准备，同时清晰地意识到这些设施并不是越多越好。在实际构建系统时，你应当在有明确需求、真正必要的时候再去考虑部署它们。不是每一个系统都要追求高并发、高可用的，根据系统的用户量、峰值流量和团队本身的技术与运维能力来考虑如何部署这些设施才是合理的做法，在能满足需求的前提下，最简单的系统就是最好的系统。","link":"/2024/09/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E9%80%8F%E6%98%8E%E5%A4%9A%E7%BA%A7%E5%88%86%E6%B5%81%E7%B3%BB%E7%BB%9F/%E9%80%8F%E6%98%8E%E5%A4%9A%E7%BA%A7%E5%88%86%E6%B5%81%E7%B3%BB%E7%BB%9F/"},{"title":"juc 之 CyclicBarrier、CountDownLatch、Semaphore的理解","text":"Semaphore在 jdk 的注释中 是这样描述他的 一个计数信号灯。从概念上讲，信号量维护一组许可证。如有必要，每个采集模块都会阻塞，直到获得许可证，然后再获取许可证。每次发布都会添加一个许可证，可能会释放一个阻塞收单机构。然而，没有使用实际的许可证对象；信号量只是保持可用数量的计数，并相应地进行操作。 信号量通常用于限制可以访问某些（物理或逻辑）资源的线程数。 Semaphore的使用老规矩先看代码 123456789101112131415161718192021222324252627282930313233343536373839static void semaphoreTest () { ExecutorService executorService = Executors.newFixedThreadPool(10); Semaphore semaphore = new Semaphore(3); for (int i = 0; i &lt; 10; i++) { executorService.submit(() -&gt; { try { semaphore.acquire(); System.out.println(System.currentTimeMillis()+&quot;:&quot;+Thread.currentThread().getName() +&quot;: 获得许可&quot;); TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { throw new RuntimeException(e); }finally { semaphore.release(); System.out.println(System.currentTimeMillis()+&quot;:&quot;+Thread.currentThread().getName() +&quot;: 释放许可&quot;); } }); } executorService.shutdown(); }1660359070454:pool-1-thread-1: 获得许可1660359070454:pool-1-thread-4: 获得许可1660359070455:pool-1-thread-2: 获得许可1660359072456:pool-1-thread-4: 释放许可1660359072456:pool-1-thread-1: 释放许可1660359072456:pool-1-thread-2: 释放许可1660359072456:pool-1-thread-3: 获得许可1660359072456:pool-1-thread-6: 获得许可1660359072456:pool-1-thread-5: 获得许可1660359074466:pool-1-thread-6: 释放许可1660359074466:pool-1-thread-8: 获得许可1660359074466:pool-1-thread-7: 获得许可1660359074466:pool-1-thread-5: 释放许可1660359074466:pool-1-thread-3: 释放许可1660359074466:pool-1-thread-9: 获得许可1660359076470:pool-1-thread-8: 释放许可1660359076470:pool-1-thread-10: 获得许可1660359076470:pool-1-thread-7: 释放许可1660359076470:pool-1-thread-9: 释放许可1660359078472:pool-1-thread-10: 释放许可 从上边的代码中可以看出信号量通常用于限制可以访问某些（物理或逻辑）资源的线程数。在上面的代码中可以看出 最多运行的资源数只有三个 Semaphore 应用场景Semaphore可以用于做流量控制，特别是公用资源有限的应用场景，比如数据库连接。假 如有一个需求，要读取几万个文件的数据，因为都是IO密集型任务，我们可以启动几十个线程 并发地读取，但是如果读到内存后，还需要存储到数据库中，而数据库的连接数只有10个，这 时我们必须控制只有10个线程同时获取数据库连接保存数据，否则会报错无法获取数据库连 接。这个时候，就可以使用Semaphore来做流量控制， CyclicBarrier在 jdk 的注释中 是这样描述他的 一种同步辅助工具，允许一组线程都等待对方到达一个公共障碍点。CyclicBarrier在涉及固定大小的线程组的程序中很有用，这些线程偶尔必须相互等待。该屏障被称为循环屏障，因为它可以在等待线程释放后重新使用。 它要做的事情就是让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行 但在使用中需要注意的是 线程的数量 和 CyclicBarrier 构造函数中 parties 的数量要保持一直，以为他只维护了一个计数器，如果说线程数量是 4 CyclicBarrier 构造函数 的数量是三，那个 第四个线程执行时 会一直 Await，同样 如果 Await 三次 4个线程同样可以执行完成，但是这样就无法保证不同线程 是否到达 同一个 障碍点。 CyclicBarrier 的使用 这里设置 三个线程数 同时 CyclicBarrier 允许的一组线程数为三 1234567891011121314151617181920212223242526272829303132static void cyclicBarrierTest () { ExecutorService executorService = Executors.newFixedThreadPool(3); CyclicBarrier cyclicBarrier = new CyclicBarrier(3); for (int i = 0; i &lt; 3; i++) { executorService.submit(() -&gt; { try { System.out.println(System.currentTimeMillis()+&quot;:&quot;+Thread.currentThread().getName() +&quot;: 正在准备&quot;); TimeUnit.SECONDS.sleep(2); cyclicBarrier.await(); System.out.println(System.currentTimeMillis()+&quot;:&quot;+Thread.currentThread().getName() +&quot;: 正在执行&quot;); TimeUnit.SECONDS.sleep(2); cyclicBarrier.await(); } catch (InterruptedException e) { throw new RuntimeException(e); } catch (BrokenBarrierException e) { throw new RuntimeException(e); } finally { System.out.println(System.currentTimeMillis()+&quot;:&quot;+Thread.currentThread().getName() +&quot;: 执行完成&quot;); } }); } executorService.shutdown(); }1660362890906:pool-1-thread-3: 正在准备1660362890906:pool-1-thread-2: 正在准备1660362890906:pool-1-thread-1: 正在准备1660362892916:pool-1-thread-3: 正在执行1660362892916:pool-1-thread-2: 正在执行1660362892916:pool-1-thread-1: 正在执行1660362894925:pool-1-thread-3: 执行完成1660362894925:pool-1-thread-1: 执行完成1660362894925:pool-1-thread-2: 执行完成 从上面的代码中可以看出 必须有足够 3 的线程到达障碍点 才能继续向下执行 ， 同时他可以设置多个障碍点 ** CyclicBarrier还提供一个更高级的构造函数CyclicBarrier（int parties，Runnable barrier- Action），用于在线程到达屏障时，优先执行barrierAction**在 barrierAction 中打印 优先执行了 A 1234567891011121314151617181920212223242526272829303132333435static void cyclicBarrierTest01 () { ExecutorService executorService = Executors.newFixedThreadPool(3); CyclicBarrier cyclicBarrier = new CyclicBarrier(3, () -&gt; { System.out.println(System.currentTimeMillis() + &quot;:&quot; + Thread.currentThread().getName() + &quot;: 优先执行了 A&quot;); }); for (int i = 0; i &lt; 3; i++) { executorService.submit(() -&gt; { try { System.out.println(System.currentTimeMillis() + &quot;:&quot; + Thread.currentThread().getName() + &quot;: 正在准备&quot;); TimeUnit.SECONDS.sleep(2); cyclicBarrier.await(); System.out.println(System.currentTimeMillis() + &quot;:&quot; + Thread.currentThread().getName() + &quot;: 正在执行&quot;); TimeUnit.SECONDS.sleep(2); cyclicBarrier.await(); } catch (InterruptedException e) { throw new RuntimeException(e); } catch (BrokenBarrierException e) { throw new RuntimeException(e); } finally { System.out.println(System.currentTimeMillis() + &quot;:&quot; + Thread.currentThread().getName() + &quot;: 执行完成&quot;); } }); } }1660363624132:pool-1-thread-1: 正在准备1660363624132:pool-1-thread-3: 正在准备1660363624132:pool-1-thread-2: 正在准备1660363626142:pool-1-thread-2: 优先执行了 A1660363626143:pool-1-thread-2: 正在执行1660363626143:pool-1-thread-3: 正在执行1660363626143:pool-1-thread-1: 正在执行1660363628149:pool-1-thread-3: 优先执行了 A1660363628149:pool-1-thread-3: 执行完成1660363628149:pool-1-thread-1: 执行完成1660363628149:pool-1-thread-2: 执行完成 CyclicBarrier 应用场景CyclicBarrier可以用于多线程计算数据，最后合并计算结果的场景。例如，用一个Excel保 存了用户所有银行流水，每个Sheet保存一个账户近一年的每笔银行流水，现在需要统计用户 的日均银行流水，先用多线程处理每个sheet里的银行流水，都执行完之后，得到每个sheet的日 均银行流水，最后，再用barrierAction用这些线程的计算结果，计算出整个Excel的日均银行流水， CountDownLatchCountDownLatch允许一个或多个线程等待其他线程完成操作。 一种同步辅助工具，允许一个或多个线程等待，直到在其他线程中执行的一组操作完成。使用给定计数初始化倒计时锁存器。由于调用倒计时方法，等待方法一直阻塞，直到当前计数达到零，然后释放所有等待线程，并立即返回任何后续的等待调用。这是一种一次性现象——计数无法重置。如果需要重置计数的版本，请考虑使用CyclicBarrier。 计数器必须大于等于0，只是等于0时候，计数器就是零，调用await方法时不会 阻塞当前线程。CountDownLatch不可能重新初始化或者修改CountDownLatch对象的内部计数 器的值。一个线程调用countDown方法happen-before，另外一个线程调用await方法。 CountDownLatch的使用123456789101112131415161718192021222324252627static void countDownLatchTest () { ExecutorService executorService = Executors.newFixedThreadPool(3); CountDownLatch countDownLatch = new CountDownLatch(3); for (int i = 0; i &lt; 3; i++) { executorService.submit(() -&gt; { try { System.out.println(System.currentTimeMillis()+&quot;:&quot;+Thread.currentThread().getName() +&quot;: 执行完成&quot;); TimeUnit.SECONDS.sleep(2); countDownLatch.countDown(); } catch (InterruptedException e) { throw new RuntimeException(e); }finally { } }); } try { countDownLatch.await(); System.out.println(&quot;线程全部执行完成&quot;); } catch (InterruptedException e) { }finally { executorService.shutdown(); } }1660364051584:pool-1-thread-3: 执行完成1660364051584:pool-1-thread-2: 执行完成1660364051584:pool-1-thread-1: 执行完成线程全部执行完成 CountDownLatch 应用场景假如有这样一个需求：我们需要解析一个Excel里多个sheet的数据，此时可以考虑使用多 线程，每个线程解析一个sheet里的数据，等到所有的sheet都解析完之后，程序需要提示解析完 成。在这个需求中，要实现主线程等待所有线程完成sheet的解析操作 CyclicBarrier 和 CountDownLatch 的区别 CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()方法重置。所以CyclicBarrier能处理更为复杂的业务场景。例如，如果计算发生错误，可以重置计数 器，并让线程重新执行一次。 CyclicBarrier还提供其他有用的方法，比如getNumberWaiting方法可以获得Cyclic-Barrier 阻塞的线程数量。isBroken()方法用来了解阻塞的线程是否被中断。","link":"/2024/09/01/java/juc%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/juc%20%E4%B9%8B%20CyclicBarrier%E3%80%81CountDownLatch%E3%80%81Semaphore%E7%9A%84%E7%90%86%E8%A7%A3/"},{"title":"java线程运行原理","text":"基础解释 1.栈帧（存在于java栈） 1234567891011121314public static void main(String[] args) { method1(3); } private static void method1(int x) { int y = x + 1; Object m =method2(); System.out.println(y); } private static Object method2() { Object obj = new Object(); return obj; } idea debug 图解 线程运行原理 线程上下文切换因为以下一些原因导致CPU不再执行当前线程，转而执行另一个线程的代码 叫做上下文切换 原因 线程的cpu时间片用完了 垃圾回收 优先级高的线程需要运行 ==主动：== 线程自己调用了 sleep、yield、wait、join、park、synchronized、lock等方法 当Context Switch发生时，需要保存当前线程的状态，并恢复另一个线程，对应jiava中的程序计数器（PC register），他的作用是记住下一条jvm指令地址 频繁切换会影响性能 线程中常见方法 方法名 static 功能说明 注意 start() 启动一个新线程，在新的线程运行 run 方法中的代码 start 方法只是让线程进入就绪，里面代码不一定立刻运行（CPU 的时间片还没分给它）。每个线程对象的start方法只能调用一次，如果调用了多次会出IllegalThreadStateException run() 新线程启动后会调用的方法 如果在构造 Thread 对象时传递了 Runnable 参数，则线程启动后会调用 Runnable 中的 run 方法，否则默认不执行任何操作。但可以创建 Thread 的子类对象，来覆盖默认行为 join() 等待线程运行结束 join(long n) 等待线程运行结束,最多等待 n 毫秒 getId() 获取线程长整型的 id id 唯一 getName() 获取线程名 setName(String) 修改线程名 getPriority() 获取线程优先级 setPriority(int) 修改线程优先级 java中规定线程优先级是1~10 的整数，较大的优先级能提高该线程被 CPU 调度的机率 getState() 获取线程状态 Java 中线程状态是用 6 个 enum 表示，分别为：NEW, RUNNABLE, BLOCKED, WAITING,TIMED_WAITING, TERMINATED isInterrupted() 判断是否被打断， 不会清除 打断标记 isAlive() 线程是否存活（还没有运行完毕） interrupt() 打断线程 如果被打断线程正在 sleep，wait，join 会导致被打断的线程抛出 InterruptedException，==并清除打断标记== ；如果打断的正在运行的线程，则会设置==打断标记== ；park 的线程被打断，也会设置 ==打断标记== interrupted static 判断当前线程是否被打断 会清除==打断标记== currentThread() static 获取当前正在执行的线程 sleep(long n) static 让当前执行的线程休眠n毫秒休眠时让出 cpu 的时间片给其它线程 yield() static 提示线程调度器让出当前线程对CPU的使用 主要是为了测试和调试 1. run 和 start 重写run（）方法写的是线程需要执行的代码 start（）是前程启动的方法 2. sleep 与 yield（让步;放弃）2.1 sleep sleep 执行 线程由 Running 进入 Timed_Waiting（阻塞）12345678910Thread thread = new Thread(new Runnable() { @SneakyThrows @Override public void run() { Thread.sleep(2000); } }, &quot;t1&quot;); thread.start(); Thread.sleep(500); System.out.println(&quot;线程的状态：&quot;+thread.getState()); 1线程的状态：TIMED_WAITING 线程可以使用interrupt方法打断正在睡眠的线程，这时 sleep方法会抛出 InterruptedException异常123456789101112131415Thread thread = new Thread(new Runnable() { @Override public void run() { log.info(&quot;sleep....&quot;); try { Thread.sleep(2000); } catch (InterruptedException e) { log.info(&quot;wake up ....&quot;); } } }, &quot;t1&quot;); thread.start(); Thread.sleep(1000); log.info(&quot;interrupt....&quot;); thread.interrupt(); //叫醒 1234567819:46:22 [t1] c.Test1 - sleep....19:46:23 [main] c.Test1 - interrupt....19:46:23 [t1] c.Test1 - wake up ....java.lang.InterruptedException at java.lang.Thread.sleep(Native Method) at java.lang.Thread.sleep(Thread.java:953) at com.zclvct.juc.test.ThreadTest1$1.run(ThreadTest1.java:85) at java.lang.Thread.run(Thread.java:823) 睡眠结束后的线程未必立刻执行 建议用TimeUnit的类进行睡眠 12345678910@Override public void run() { log.info(&quot;sleep....&quot;); try { TimeUnit.SECONDS.sleep(2); log.info(&quot;sleep end ....&quot;); } catch (InterruptedException e) { e.printStackTrace(); } } 1219:52:11 [t1] c.Test1 - sleep....19:52:13 [t1] c.Test1 - sleep end .... 2.1 yield （让出、谦让） 调用yield（）会让当前线程让当前线程从Running 进入 Runnable 状态 ，然后调度其他线程 具体实现依赖于操作系统的调度器。== 就绪状态（Runnable）会被调度器调度 而 阻塞状态（Timed_Waiting）不会== 123456789101112131415161718192021Runnable task1 = () -&gt; { int count = 0; for(;;) { System.out.println(&quot;======1 &quot;+count++); } }; Runnable task2 = () -&gt; { int count = 0; for(;;) { // Thread.yield(); System.out.println(&quot;=============2 &quot;+count++); } }; Thread thread1 = new Thread(task1); Thread thread2 = new Thread(task2); thread1.setPriority(Thread.MAX_PRIORITY); thread2.setPriority(Thread.MIN_PRIORITY); thread1.start(); thread2.start(); 通过yeild（）或设置优先级 在一定条件下 count的差值会变大2.3 线程优先级 setPriority（） getPriority（） 优先级会提示调度器优先调度的线程，当时调度器课以忽略他 ， 他仅仅是提示作用 如果cpu防盲，那么优先级高的线程会获取更多的时间片，但cpu闲时几乎没有作用 2. join 等待调用的线程运行结束 1.join 同步应用 123456789101112131415161718192021static int i = 0; public static void main(String[] args) throws InterruptedException { test1(); } private static void test1() throws InterruptedException { log.info(&quot;开始。。。&quot;); Thread thread = new Thread(new Runnable() { @SneakyThrows @Override public void run() { log.info(&quot;开始。。。&quot;); TimeUnit.SECONDS.sleep(1); log.info(&quot;结束。。。&quot;); i = 10; } },&quot;th1&quot;); thread.start(); thread.join(); log.info(&quot;结果为：{}&quot;,i); log.info(&quot;结束。。。&quot;); } 1234520:27:30 [main] c.Test1 - 开始。。。20:27:30 [th1] c.Test1 - 开始。。。20:27:31 [th1] c.Test1 - 结束。。。20:27:31 [main] c.Test1 - 结果为：1020:27:31 [main] c.Test1 - 结束。。。· 1.join 等待最大时间 thread.join（1000） 设置等待最大时间 ，到时间则结束不继续等待线程执行完毕 2. interrupt （打断） 打断 sleep 、 wait 、 join的线程 打断sleep线程，==会清空打断状态、会抛出异常== 1234567891011121314151617Thread thread1 = new Thread(new Runnable() { @Override public void run() { log.info(&quot;sleep....&quot;); try { TimeUnit.SECONDS.sleep(2); // wait ， join ,sleep 被打断会清除打断标记 } catch (InterruptedException e) { log.info(&quot;wake up ....&quot;); e.printStackTrace(); } } }, &quot;t1&quot;); thread1.start(); TimeUnit.SECONDS.sleep(1); log.info(&quot;interrupt....&quot;); thread1.interrupt(); //打断 log.debug(&quot;打断标记 {}&quot;,thread1.isInterrupted()); 12345678920:55:00 [t1] c.Test1 - sleep....20:55:01 [main] c.Test1 - interrupt....20:55:01 [t1] c.Test1 - wake up ....20:55:01 [main] c.Test1 - 打断标记 falsejava.lang.InterruptedException at java.lang.Thread.sleep(Native Method) at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386) at com.zclvct.juc.test.ThreadTest1$1.run(ThreadTest1.java:183) at java.lang.Thread.run(Thread.java:823) 打断正常运行的线程线程 打断正常运行的线程线程 ，==不会清空打断标记 ，不会抛出异常== isInterrupted（） 不会清除打断标记 static interrupted （） 会清除打断标记 两者都是判断打断标记的放法 12345678910111213141516171819Thread thread1 = new Thread(new Runnable() { @Override public void run() { while (true) { boolean interrupted = Thread.currentThread().isInterrupted(); if(interrupted) { log.info(&quot;被打断退出循环&quot;); break; } } } }, &quot;t1&quot;); thread1.start(); TimeUnit.SECONDS.sleep(1); log.info(&quot;interrupt....&quot;); thread1.interrupt(); //打断 log.debug(&quot;打断标记 {}&quot;,thread1.isInterrupted()); 12321:00:54 [main] c.Test1 - interrupt....21:00:54 [t1] c.Test1 - 被打断退出循环21:00:54 [main] c.Test1 - 打断标记 true 两阶段终止模式 1234567891011121314151617181920212223242526272829303132333435363738static class TwoPhaseTermination { private Thread monitor; //监控线程 // 启动监控线程 public void start() { monitor = new Thread(() -&gt; { while (true) { Thread thread = Thread.currentThread(); boolean interrupted = thread.isInterrupted(); if(interrupted) { log.info(&quot;打断我 ，我选择认输&quot;); break; } try { TimeUnit.SECONDS.sleep(1); log.info(&quot;执行监控功能&quot;); } catch (InterruptedException e) { log.info(&quot;正在睡眠打断我，设置打断标记&quot;); thread.interrupt(); e.printStackTrace(); } } }); monitor.start(); } // 停止监控线程 public void stop() { monitor.interrupt(); } }public static void main(String[] args) throws InterruptedException { TwoPhaseTermination twoPhaseTermination = new TwoPhaseTermination(); twoPhaseTermination.start(); TimeUnit.SECONDS.sleep(3); twoPhaseTermination.stop(); } 123421:23:20 [Thread-3] c.Test1 - 执行监控功能21:23:21 [Thread-3] c.Test1 - 执行监控功能21:23:22 [Thread-3] c.Test1 - 正在睡眠打断我，设置打断标记21:23:22 [Thread-3] c.Test1 - 打断我 ，我选择认输 打断park线程 打断park线程，不会清空打断状态 打断状态为真时再次park（）不会生效 可以利用 static interrupted清除打断状态1234567891011Thread th1 = new Thread(new Runnable() { @Override public void run() { log.info(&quot;park。。&quot;); LockSupport.park(); log.info(&quot;unpark....&quot;); log.info(&quot;打断状态 {}&quot;,Thread.currentThread().isInterrupted()); } },&quot;th1&quot;); th1.start(); th1.interrupt(); 12313:21:48 [th1] c.Test1 - park。。13:21:48 [th1] c.Test1 - unpark....13:21:48 [th1] c.Test1 - 打断状态 true 不推荐的方法 会破坏同步代码块，造成线程死锁 方法名 static 功能说明 stop（） 停止线程运行 suspend（） 挂起（暂停）线程运行 resume（） 恢复线程运行 主线程与守护线程默认情况下，Java进程需要等待所有线程都运行结束，才会结束。有一种特殊的线程叫做守护线程，只要其他非守护线程运行结束了，即使守护线程的代码没有执行完，也会立即结束。 1234567891011121314151617Thread th1 = new Thread(new Runnable() { @Override public void run() { while (true) { if(Thread.currentThread().isInterrupted()) { break; } } log.info(Thread.currentThread().getName() +&quot; 运行结束&quot;); } },&quot;th1&quot;); th1.setDaemon(true); //设置线程为守护线程 th1.start(); TimeUnit.SECONDS.sleep(1); log.info(&quot;主线程结束。。。&quot;); 113:44:57 [main] c.Test1 - 主线程结束。。。 垃圾回收线程就是守护线程 Tomcat中的Acceptor 和 Poller线程都是守护线程 线程的五种状态 （操作系统层面） 线程的六种状态（java层面） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576Thread t1 = new Thread(&quot;t1&quot;) { @Override public void run() { log.debug(&quot;running...&quot;); } }; Thread t2 = new Thread(&quot;t2&quot;) { @Override public void run() { while(true) { // runnable } } }; t2.start(); Thread t3 = new Thread(&quot;t3&quot;) { @Override public void run() { log.debug(&quot;running...&quot;); } }; t3.start(); Thread t4 = new Thread(&quot;t4&quot;) { @Override public void run() { synchronized (TestState.class) { try { Thread.sleep(1000000); // timed_waiting } catch (InterruptedException e) { e.printStackTrace(); } } } }; t4.start(); Thread t5 = new Thread(&quot;t5&quot;) { @Override public void run() { try { t2.join(); // waiting } catch (InterruptedException e) { e.printStackTrace(); } } }; t5.start(); Thread t6 = new Thread(&quot;t6&quot;) { @Override public void run() { synchronized (TestState.class) { // blocked 同一个对象加锁 他拿不到锁 try { Thread.sleep(1000000); } catch (InterruptedException e) { e.printStackTrace(); } } } }; t6.start(); try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } log.debug(&quot;t1 state {}&quot;, t1.getState()); log.debug(&quot;t2 state {}&quot;, t2.getState()); log.debug(&quot;t3 state {}&quot;, t3.getState()); log.debug(&quot;t4 state {}&quot;, t4.getState()); log.debug(&quot;t5 state {}&quot;, t5.getState()); log.debug(&quot;t6 state {}&quot;, t6.getState()); System.in.read(); 123456714:30:36 [t3] c.TestState - running...14:30:37 [main] c.TestState - t1 state NEW14:30:37 [main] c.TestState - t2 state RUNNABLE14:30:37 [main] c.TestState - t3 state TERMINATED terminated14:30:37 [main] c.TestState - t4 state TIMED_WAITING14:30:37 [main] c.TestState - t5 state WAITING14:30:37 [main] c.TestState - t6 state BLOCKED","link":"/2024/09/01/java/juc%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/java%E7%BA%BF%E7%A8%8B%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/"},{"title":"AbstractQueuedSynchronizer 的实现","text":"AbstractQueuedSynchronizer翻译过来是 AbstractQueuedSynchronizer ， 也是我们常说的 AQS 他是很多同步器的基础框架，比如 ReentrantLock、CountDownLatch 和 Semaphore 等都是基于 AQS 实现的。除此之外，我们还可以基于 AQS，定制出我们所需要的同步器。同步器的设计是基于模板方法模式的，也就是说，使用者需要继承同步器并重写指定的 方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些 模板方法将会调用使用者重写的方法。 原理概述6d7fb03256929308724c2416d09eaca5.png) state 表示同步器的状态 head 和 tail 组成同步器 的 CLH（虚拟的双向队列）双向队列 ConditionObject 实现了 Condition （也称为条件队列或条件变量） 是每个对象具有了等待集 ， 如果锁取代了同步方法和语句的使用，则条件取代了对象监视器方法的使用。 unsafe Java中的Unsafe类为我们提供了类似C++手动管理内存的能力。在AQS 执行CAS操作 重写同步器重写同步器指定的方法时，需要使用同步器提供的如下3个方法来访问或修改同步状态 getState()：获取当前同步状态。 setState(int newState)：设置当前同步状态。 compareAndSetState(int expect,int update)：使用CAS设置当前状态，该方法能够保证状态 设置的原子性 同步器可重写的方法 方法名称 描 述 protected boolean tryAcquire(int arg) 独占式获取同步状态，实现该方法需要查询当前状态并频段同步状态是否符合预期，然后再进行CAS设置同步状态 protected boolean tryRelease(int arg) 独占式释放同步状态，等待获取同步状态的线程将有机会获取同步状态 protected int tryAcquireShared 共享式获取同步状态，返回大于等于 0 的值表示获取成功，反之获取失败 protected boolean tryReleaseShared(int arg) 共享式释放同步状态 protected boolean isHeldExclusively() 但钱同步器是否在独占模式下被线程占用，一般该方法表示是否被当前线程锁独占 public final void acquire(int arg) 独占式获取同步状态，如果单线线程同步状态获取成功，则有该方法返回，否则进入同步队列等待 public final void acquireInterruptibly(int arg) 与acquire(int arg) 相同，但是该方法显影中断，当前线程为获取到同步状态二进入同步队列中，如果当前线程被中断，则该方法会判处InterruptedException并返回 public final boolean tryAcquireNanos(int arg, long nanosTimeout) 在 acquireInterruptibly 基础上增加了超市限制，如果当前线程在超市时间内没有获取到同步状态，那么会返回false，如果获取到了返回 true public final void acquireShared(int arg) 共享式获取同步状态，如果当前线程为获取到同步状态，将会进入同步队列等待，与独占是获取的主要区别式在同一时刻可以又多个线程获取到同步状态 public final void acquireSharedInterruptibly(int arg) 与 acquireShared(int arg 相同，该方法响应中断 public final boolean tryAcquireSharedNanos(int arg, long nanosTimeout) 在acquireSharedInterruptibly(int arg)基础上增加了超时限制 public final boolean release(int arg) 独占式 释放同步状态，该方法在释放同步状态之后，将同步队列中第一个节点包含的线程唤醒 public final boolean releaseShared(int arg) 共享式释放同步状态 public final Collection getQueuedThreads() 获取等待在同步队列上的线程集合 同步器提供的方法可以分为三大类 独占式获取与释放同步状态、 共享式获取与释放 同步状态和查询同步队列中的等待线程情况 同步状态 stateAQS 中的 state 是 volatile 修饰的 保证了字段可见性 使用 compareAndSetState 对 state进行操作 保证了原子性AQS 获取、释放资源是否成功都是由state决定的 ReentrantLock的state用来表示是否有锁资源 ReentrantReadWriteLock的state高16位代表读锁状态，低16位代表写锁状态 Semaphore的state用来表示可用信号的个数 CountDownLatch的state用来表示计数器的值 同步队列node节点同步器依赖内部的同步队列（一个FIFO双向队列）来完成同步状态的管理，当前线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成为一个节点（Node）并将其 加入同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点中的线程唤醒，使其再 次尝试获取同步状态.同步队列中的节点（Node）用来保存获取同步状态失败的线程引用、等待状态以及前驱和后继节点，节点的属性类型与名称以及描述 12345678910111213141516171819202122232425static final class Node { // 用于指示节点正在共享模式下等待的标记 static final Node SHARED = new Node(); // 用于指示节点正在独占模式下等待的标记 static final Node EXCLUSIVE = null; /** waitStatus 表示当前节点以取消调度（超时或者被中断） */ static final int CANCELLED = 1; /** waitStatus 表示后继节点处于等待状态 */ static final int SIGNAL = -1; /** waitStatus 表示该节点在等待队列上，调用条件变量的 signal 方法 该节点会从等待队列转移到同步队列 */ static final int CONDITION = -2; /** waitStatus 表示下一次共享式同步状态获取将会无条件被传播下去*/ static final int PROPAGATE = -3; volatile int waitStatus; // 前驱节点 volatile Node prev; // 后继节点 volatile Node next; // 获取同步状态的线程 volatile Thread thread; // 等待队列的后继节点 Node nextWaiter; } 同步队列结构当一个线程成功地获取了同步状态（或者锁），其他线程将无法获取到同步状态，转 而被构造成为节点并加入到同步队列中，而这个加入队列的过程必须要保证线程安全，因此同步器提供了一个基于CAS的设置尾节点的方法：compareAndSetTail(Node expect,Node update)，它需要传递当前线程“认为”的尾节点和当前节点，只有设置成功后，当前节点才正式 与之前的尾节点建立关联。 独占式同步状态获取首先调用自定义同步器实现的tryAcquire(int arg)方法，该方法 保证线程安全的获取同步状态，如果同步状态获取失败，则构造同步节点（独占式 Node.EXCLUSIVE，同一时刻只能有一个线程成功获取同步状态）并通过addWaiter(Node node) 方法将该节点加入到同步队列的尾部，最后调用acquireQueued(Node node,int arg)方法，使得该 节点以“死循环”的方式获取同步状态。如果获取不到则阻塞节点中的线程，而被阻塞线程的 唤醒主要依靠前驱节点的出队或阻塞线程被中断来实现。 1234567public final void acquire(int arg) { // 尝试获取同步状态 if (!tryAcquire(arg) &amp;&amp; // 构建尾节点 以死循环的方式获取同步状态 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 123456789101112131415161718private Node addWaiter(Node mode) { // 创建 Node节点 Node node = new Node(Thread.currentThread(), mode); Node pred = tail; // 判断尾节点 是不是空 if (pred != null) { // 如果尾节点不等于null，把当前节点的前驱节点指向尾节点 node.prev = pred; // 替换尾节点 if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } //如果添加失败或队列不存在，执行end函数 enq(node); return node; } 在enq(final Node node)方法中，同步器通过“死循环”来保证节点的正确添加，在“死循环”中只有通过CAS将节点设置成为尾节点之后，当前线程才能从该方法返回，否则，当前线 程不断地尝试设置。可以看出，enq(final Node node)方法将并发添加节点的请求通过CAS变 得“串行化”了。 123456789101112131415161718192021private Node enq(final Node node) { for (;;) { //循环 //获取尾节点 Node t = tail; if (t == null) { //如果尾节点为空，创建哨兵节点，通过cas把头节点指向哨兵节点 if (compareAndSetHead(new Node())) //cas成功，尾节点指向哨兵节点 tail = head; } else { //当前节点的前驱节点设指向之前尾节点 node.prev = t; //cas设置把尾节点指向当前节点 if (compareAndSetTail(t, node)) { //cas成功，之前尾节点的下个节点指向当前节点 t.next = node; return t; } } } } 节点进入同步队列之后，就进入了一个自旋的过程，每个节点（或者说每个线程）都在自省地观察，当条件满足，获取到了同步状态，就可以从这个自旋过程中退出，否则依旧留在这 个自旋过程中，并会阻塞节点的线程 1234567891011121314151617181920212223242526272829final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { // 获取前驱节点 final Node p = node.predecessor(); // 如果前驱节点是头节点 ， 获取同步状态 if (p == head &amp;&amp; tryAcquire(arg)) { // 设置当前节点为头节点 也就是哨兵节点 setHead(node); // p.next = null; // help GC failed = false; return interrupted; } // 判断是否需要挂起当前线程 主要是 判断是否为 Node.SIGNAL 状态， 如果是执行parkAndCheckInterrupt 调用线程的 park 阻塞线程 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // 调用线程的 park 阻塞线程 线程在这里阻塞住 可能被唤醒 或者被打断 返回true 代表线程被打断 parkAndCheckInterrupt()) // 标记被打断 interrupted = true; } } finally { //如果出现异常或者出现中断，就会执行finally的取消线程的请求操作，核心代码是node.waitStatus = Node.CANCELLED;将线程的状态改为CANCELLED。 if (failed) cancelAcquire(node); } } 队列中各个节点的状态 独占式同步状态获取流程，也就是acquire(int arg)方法调用流程 前驱节点为头节点且能够获取同步状态的判断条件和线程进入等待状态是获取同步状态的自旋过程。当同步状态获取成功之后，当前线程从acquire(int arg)方法返回，如果 对于锁这种并发组件而言，代表着当前线程获取了锁。 独占式同步状态释放当前线程获取同步状态并执行了相应逻辑之后，就需要释放同步状态，使得后续节点能够继续获取同步状态。通过调用同步器的release(int arg)方法可以释放同步状态，该方法在释 放了同步状态之后，会唤醒其后继节点（进而使后继节点重新尝试获取同步状态） 123456789101112public final boolean release(int arg) { // 释放同步状态 if (tryRelease(arg)) { Node h = head; // 头节点不为空 且 waitStatus 不等于零 if (h != null &amp;&amp; h.waitStatus != 0) // 调用后继节点的 unpark unparkSuccessor(h); return true; } return false; } 123456789101112131415private void unparkSuccessor(Node node) { int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); Node s = node.next; if (s == null || s.waitStatus &gt; 0) { s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; } if (s != null) //unpark 唤醒线程 LockSupport.unpark(s.thread); } 独占式超时获取同步状态通过调用同步器的doAcquireNanos(int arg,long nanosTimeout)方法可以超时获取同步状态，即在指定的时间段内获取同步状态，如果获取到同步状态则返回true，否则，返回false。 超时获取同步状态过程可以被视作响应中断获取同步状态过程的“增强版”，doAcquireNanos(int arg,long nanosTimeout)方法在支持响应中断的基础上，增加了超时获取的 特性 123456789101112131415161718192021222324252627282930313233343536private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException { if (nanosTimeout &lt;= 0L) return false; // 到期时间 final long deadline = System.nanoTime() + nanosTimeout; // 进入队列尾部 final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try { for (;;) { final Node p = node.predecessor(); // 判断获取同步状态 if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return true; } // 计算剩余时间 nanosTimeout = deadline - System.nanoTime(); if (nanosTimeout &lt;= 0L) return false; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; nanosTimeout &gt; spinForTimeoutThreshold) // park 当前线程 时间为剩余时间 LockSupport.parkNanos(this, nanosTimeout); // 判断线程的打断状态 并清空打断状态 if (Thread.interrupted()) throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); } } 共享式同步状态获取与释放共享式同步状态获取 与 独占锁的获取基本一直 差别就是共享式支持多个线程共享访问在共享式获取的自旋过程中，成功获取到同步状态并退出自旋的条件就是 tryAcquireShared(int arg)方法返回值大于等于0。 123456789101112131415161718192021222324252627private void doAcquireShared(int arg) { final Node node = addWaiter(Node.SHARED); boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head) { int r = tryAcquireShared(arg); if (r &gt;= 0) { setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; } } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 与独占式一样，共享式获取也需要释放同步状态，通过调用releaseShared(int arg)方法可以释放同步状态 12345678910111213141516171819private void doReleaseShared() { for (;;) { Node h = head; if (h != null &amp;&amp; h != tail) { int ws = h.waitStatus; if (ws == Node.SIGNAL) { if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 唤醒后记节点 unparkSuccessor(h); } else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS } if (h == head) // loop if head changed break; } }","link":"/2024/09/01/java/juc%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/juc-%20%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E5%88%86%E6%9E%90%20%20AbstractQueuedSynchronizer%20%E7%9A%84%E5%AE%9E%E7%8E%B0%20/"},{"title":"synchronized 底层原理","text":"Java对象头 来源以32位虚拟机为例 普通对象 123456|--------------------------------------------------------------|| Object Header (64 bits) ||------------------------------------|-------------------------|| Mark Word (32 bits) | Klass Word (32 bits) || （标记字段） | （类型指针） ||------------------------------------|-------------------------| 数组对象 12345|---------------------------------------------------------------------------------|| Object Header (96 bits) ||--------------------------------|-----------------------|------------------------|| Mark Word(32bits) | Klass Word(32bits) | array length(32bits) ||--------------------------------|-----------------------|------------------------| 其中Mark Word结构为 12345678910111213|-------------------------------------------------------|--------------------|| Mark Word (32 bits) | State ||-------------------------------------------------------|--------------------|| hashcode:25 | age:4 | biased_lock:0 | 01 | Normal ||-------------------------------------------------------|--------------------|| thread:23 | epoch:2 | age:4 | biased_lock:1 | 01 | Biased ||-------------------------------------------------------|--------------------|| ptr_to_lock_record:30 | 00 | Lightweight Locked ||-------------------------------------------------------|--------------------|| ptr_to_heavyweight_monitor:30 | 10 | Heavyweight Locked ||-------------------------------------------------------|--------------------|| | 11 | Marked for GC ||-------------------------------------------------------|--------------------| 64 位虚拟机 Mark Word 12345678910111213|------------------------------------------------------------------|--------------------|| Mark Word (64 bits) | State ||------------------------------------------------------------------|--------------------|| unused:25 | hashcode:31 | unused:1 | age:4 | biased_lock:0 | 01 | Normal ||------------------------------------------------------------------|--------------------|| thread:54 | epoch:2 | unused:1 | age:4 | biased_lock:1 | 01 | Biased ||------------------------------------------------------------------|--------------------|| ptr_to_lock_record:62 | 00 | Lightweight Locked ||------------------------------------------------------------------|--------------------|| ptr_to_heavyweight_monitor:62 | 10 | Heavyweight Locked ||------------------------------------------------------------------|--------------------|| | 11 | Marked for GC ||------------------------------------------------------------------|--------------------| Monitor（锁）Monitor： 被翻译为 监视器 或管程 刚开始Monitor中Owner为null 当Thread-2执行synchronized（obj）就会将Monitor 的所有者Owner置为Thread-2，Monitor中只能有一个Owner 在Thread-2上锁的过程中，如果Thread-3，Thread-4，Thread-5也来执行synchronized（obj），就会进入EntryList BLOCKED Thread-2执行完同步代码块的内容，然后唤醒 EntryList中等待的线程来竞争锁，竞争的时是非公平的 图中WaitSet中的Thread-0，Thread-1是之前获得过锁，但条件不满足进入WAITING状态的线程，后面讲wait-notify 时会分析 锁的状态 轻量级锁锁重入如果一个对象虽然有多线程访问，但多线程访问的时间是错开的（也就是没有竞争），可以用轻量级锁来优化 轻量级锁对使用者还是透明的，语法仍然是synchronized 123456789static final Object obj = new Object(); public static void method1(){ synchronized (obj) {//同步块1 method2();}} private static void method2() { synchronized (obj) { //同步块2}} 创建锁记录（Lock Record）对象，每个线程的栈帧都会包含一个锁记录结果，内部可以存储锁定对象的Mark Word（标记字段） 让锁对象中Object reference 指向，并尝试用cas替换Object的Mark Word，将Mark Word 的值存入锁记录 如果cas替换成功，最想头存储了锁记录地址和状态 00 ，表示有该线程给对象加锁 如果cas失败 如果是其他线程已经持有该Object的轻量级锁，表明有竞争，进入锁膨胀过程 如果是同一线程执行了 ==synchronized 锁重入== ，那么在天机一条Lock Record作为锁重入的计数 当退出synchronized代码块时（解锁时），如果有取值位null的锁记录，表示有锁重入，只是重置锁记录，表示锁重入数减一 - 当退出synchronized代码块时（解锁时），锁记录部位null ，这时使用cas将 Mark World的值回复给对象头 - 成功，则结果成功 - 失败，说明轻量级锁进行了锁膨胀的过程或已经升级为重量级锁，进入重量级锁解锁流程 锁膨胀 如果尝试加轻量级锁的过程中，CAS无法成功，这是有其他线程未次对象加上了轻量级锁（有竞争），这是需要进行所鹏展，将轻量级锁变成重量级锁。 当Thread-1进行轻量级加锁时，Thread-0已经对该对象加了轻量级锁 这是Thread-1加轻量级锁失败，进入做膨胀过程 Object对象申请Monitor锁，让Object指向重量级锁的地址 然后自己进入Moniter的RntryList BOLOCED 当Thread-0退出同步块解锁时，使用cas将Mark Word的值恢复给对象头，如果失败，这是会进入重量级锁解锁流程，即按照Monitor地址找到Monitor对象，是指Owner为null ，唤醒EntryList中的BLOCKED线程 自旋优化重量级锁竞争的时候，还可以用自旋来优化，如果当前线程自旋成功（即这时候持锁线程已经推出了同步块，释放了锁）这时当前线程可以避免阻塞进行上下文切换（上下文切换耗费性能）。 自旋会占用 CPU 时间，单核 CPU 自旋就是浪费，多核 CPU 自旋才能发挥优势。 在 Java 6 之后自旋锁是自适应的，比如对象刚刚的一次自旋操作成功过，那么认为这次自旋成功的可能性会高，就多自旋几次；反之，就少自旋甚至不自旋，总之，比较智能。 Java 7 之后不能控制是否开启自旋功能 偏向锁 轻量级锁在没有竞争时，每次重入依然需要执行CAS操作 Java 6 中引入了偏向锁来做进一步优化：只有第一次使用 CAS 将线程 ID 设置到对象的 Mark Word 头，之后发现这个线程 ID 是自己的就表示没有竞争，不用重新 CAS。以后只要不发生竞争，这个对象就归该线程所有 偏向状态12345678910111213|-------------------------------------------------------------------|-------------------|| Mark Word (64 bits) | State ||-------------------------------------------------------------------|-------------------|| unused:25 | hashcode:31 | unused:1 | age:4 | biased_lock:0 | 01 | Normal ||-------------------------------------------------------------------|-------------------|| thread:54 | epoch:2 | unused:1 | age:4 | biased_lock:1 | 01 | Biased ||-------------------------------------------------------------------|-------------------|| ptr_to_lock_record:62 | 00 |Lightweight Locked ||-------------------------------------------------------------------|-------------------|| ptr_to_heavyweight_monitor:62 | 10 |Heavyweight Locked ||-------------------------------------------------------------------|-------------------|| | 11 | Marked for GC ||-------------------------------------------------------------------|-------------------| 一个对象创建时： 如果开启了偏向锁（默认开启），那么对象创建后，markword 值为 0x05 即最后 3 位为 101，这时它的thread、epoch、age 都为 0 偏向锁是默认是延迟的，不会在程序启动时立即生效，如果想避免延迟，可以加 VM 参数 -XX:BiasedLockingStartupDelay=0 来禁用延迟 如果没有开启偏向锁，那么对象创建后，markword 值为 0x01 即最后 3 位为 001，这时它的hashcode、age 都为 0，第一次用到 hashcode 时才会赋值 锁撤销偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点(在这个时间点上没有正在执行的字节码)。它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态；如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。原文 调用了对象的 hashCode，但偏向锁的对象 MarkWord 中存储的是线程 id，如果调用 hashCode 会导致偏向锁被撤销，在调用 hashCode 后使用偏向锁，记得去掉 -XX:-UseBiasedLocking 偏向锁不会记录hashcode 轻量级锁会在锁记录中记录 hashCode 重量级锁会在 Monitor 中记录 hashCode 当有其它线程使用偏向锁对象时，会将偏向锁升级为轻量级锁 wait/notify 时重量级锁使用的方法 ，调用时偏向锁升级为重量级锁 批量重偏向 以class为单位，为每个class维护一个偏向锁撤销计数器。每一次该class的对象发生偏向撤销操作是，该计数器+1，当这个值达到重偏向阈值(默认20)时，JVM就认为该class的偏向锁有问题，因此会进行批量重偏向。每个class对象也会有一个对应的epoch字段，每个处于偏向锁状态对象的mark word中也有该字段，其初始值为创建该对象时，class中的epoch值。每次发生批量重偏向时，就将该值+1，同时遍历JVM中所有线程的站，找到该class所有正处于加锁状态的偏向锁，将其epoch字段改为新值。下次获取锁时，发现当前对象的epoch值和class不相等，那就算当前已经偏向了其他线程，也不会执行撤销操作，而是直接通过CAS操作将其mark word的Thread Id改为当前线程ID 如果对象虽然被多个线程访问，但没有竞争，这时偏向了线程 T1 的对象仍有机会重新偏向 T2，重偏向会重置对象的 Thread ID 当撤销偏向锁阈值超过 20 次后，jvm 会这样觉得，我是不是偏向错了呢，于是会在给这些对象加锁时重新偏向至加锁线程 批量撤销 当撤销偏向锁阈值超过 40 次后，jvm 会这样觉得，自己确实偏向错了，根本就不该偏向。于是整个类的所有对象都会变为不可偏向的，新建的对象也是不可偏向的 锁消除 锁消除是指虚拟机即时编译器在运行时，对一些代码要求同步，但是对被检测到不可能存在共享 数据竞争的锁进行消除。锁消除的主要判定依据来源于逃逸分析的数据支持 wait notify 原理 Owner 线程发现条件不满足，调用 wait 方法，即可进入 WaitSet 变为 WAITING 状态 BLOCKED 和 WAITING 的线程都处于阻塞状态，不占用 CPU 时间片 BLOCKED 线程会在 Owner 线程释放锁时唤醒 WAITING 线程会在 Owner 线程调用 notify 或 notifyAll 时唤醒，但唤醒后并不意味者立刻获得锁，仍需进入EntryList 重新竞争 12345它们都是线程之间进行协作的手段，都属于 Object 对象的方法。必须获得此对象的锁，才能调用这几个方法obj.wait()// 让进入 object 监视器的线程到 waitSet 等待wait(long n) //有时限的等待, 到 n 毫秒后结束等待，或是被 notifyobj.notify() //在 object 上正在 waitSet 等待的线程中挑一个唤醒obj.notifyAll() //让 object 上正在 waitSet 等待的线程全部唤醒 1234567891011121314151617181920212223242526272829303132final static Object obj = new Object(); public static void main(String[] args) throws InterruptedException { new Thread(() -&gt; { synchronized (obj) { log.debug(&quot;执行....&quot;); try { obj.wait(); // 让线程在obj上一直等待下去 } catch (InterruptedException e) { e.printStackTrace(); } log.debug(&quot;其它代码....&quot;); } }).start(); new Thread(() -&gt; { synchronized (obj) { log.debug(&quot;执行....&quot;); try { obj.wait(); // 让线程在obj上一直等待下去 } catch (InterruptedException e) { e.printStackTrace(); } log.debug(&quot;其它代码....&quot;); } }).start(); // 主线程两秒后执行 TimeUnit.SECONDS.sleep(2); log.debug(&quot;唤醒 obj 上其它线程&quot;); synchronized (obj) { obj.notify(); // 唤醒obj上一个线程 // obj.notifyAll(); // 唤醒obj上所有等待线程 } } 123420:40:04 [Thread-3] c.Test1 - 执行....20:40:04 [Thread-4] c.Test1 - 执行....20:40:06 [main] c.Test1 - 唤醒 obj 上其它线程20:40:06 [Thread-3] c.Test1 - 其它代码.... wait notify 的正确姿势sleep(long n) 和 wait(long n) 的区别 sleep 是 Thread 方法，而 wait 是 Object 的方法 sleep 不需要强制和 synchronized 配合使用，但 wait 需要和 synchronized 一起用 sleep 在睡眠的同时，不会释放对象锁的，但 wait 在等待的时候会释放对象锁 4) 它们状态 TIMED_WAITING 保护式暂停模式即 Guarded Suspension，用在一个线程等待另一个线程的执行结果 有一个结果需要从一个线程传递到另一个线程，让他们关联同一个 GuardedObject 如果有结果不断从一个线程到另一个线程那么可以使用消息队列（见生产者/消费者） JDK 中，join 的实现、Future 的实现，采用的就是此模式 因为要等待另一方的结果，因此归类到同步模式 1234567891011121314151617181920212223242526272829303132333435class GuardedObject { private Object response; private final Object lock = new Object(); public Object get(Long timeOut) { //超时时间 // 1) 记录最初时间 long begin = System.currentTimeMillis(); // 2) 已经经历的时间 long timePassed = 0; synchronized (lock) { // 条件不满足则等待 while (response == null) { //防止虚假唤醒 long waitTime = timeOut - timePassed; //计算剩余等待的时间 if (waitTime &lt;= 0) { log.debug(&quot;break...&quot;); break; } try { lock.wait(waitTime); //防止虚假唤醒后 等待时间大于输入时间 } catch (InterruptedException e) { e.printStackTrace(); } // 3) 记录已经经历的时间 timePassed = System.currentTimeMillis() - begin; } return response; } } public void complete(Object response) { synchronized (lock) { // 条件满足，通知等待线程 this.response = response; lock.notifyAll(); } }} 12345678910111213public static void main(String[] args) { GuardedObject guardedObject = new GuardedObject(); new Thread(() -&gt; { // 子线程执行下载 List&lt;String&gt; response = download(); log.debug(&quot;download complete...&quot;); guardedObject.complete(response); }).start(); log.debug(&quot;waiting...&quot;); // 主线程阻塞等待 Object response = guardedObject.get(); log.debug(&quot;get response: [{}] lines&quot;, ((List&lt;String&gt;) response).size()); } 12321:13:15 [main] c.Test1 - waiting...21:13:17 [Thread-3] c.Test1 - download complete...21:13:17 [main] c.Test1 - get response: [2] lines join原理利用了==保护式暂停模式== 12345678910111213141516171819202122long totalWaited = 0; long totalWaited = 0; // 记录共计等待时间 long toWait = timeoutInMilliseconds; //记录约定等待时间 boolean timedOut = false; //记录是否超时 if (timeoutInMilliseconds == 0 &amp; nanos &gt; 0) { // We either round up (1 millisecond) or down (no need to wait, just return) if (nanos &lt; 500000) timedOut = true; else toWait = 1; } while (!timedOut &amp;&amp; !isDead()) { //防止虚假环境，并且判断是否超时 long start = System.currentTimeMillis(); //记录等待开始时间 wait(toWait); // 开始等待 第一次等待时间为约定等待时间 第二次为剩余等待时间 long waited = System.currentTimeMillis() - start; //计算本次循环等待时间 totalWaited+= waited; //计算共计等待时间 toWait -= waited; //计算等待时间 // Anyone could do a synchronized/notify on this thread, so if we wait // less than the timeout, we must check if the thread really died timedOut = (totalWaited &gt;= timeoutInMilliseconds); //超时判断 } park &amp; Unpark （搁置、推迟）它们是 LockSupport 类中的方法 1234// 暂停当前线程LockSupport.park(); // 恢复某个线程的运行LockSupport.unpark(暂停线程对象) 示例 123456789101112131415Thread t1 = new Thread(() -&gt; { log.debug(&quot;start...&quot;); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } log.debug(&quot;park...&quot;); LockSupport.park(); //线程进行WAIT状态 log.debug(&quot;resume...&quot;); }, &quot;t1&quot;); t1.start(); TimeUnit.SECONDS.sleep(1); log.debug(&quot;unpark...&quot;); LockSupport.unpark(t1); 与 Object 的 wait &amp; notify 相比 wait，notify 和 notifyAll 必须配合 Object Monitor 一起使用，而 park，unpark 不必 park &amp; unpark 是以线程为单位来【阻塞】和【唤醒】线程，而 notify 只能随机唤醒一个等待线程，notifyAll 是唤醒所有等待线程，就不那么【精确】 ==park &amp; unpark 可以先 unpark==，而 wait &amp; notify 不能先 notify park unpark 原理每个线程都有自己的一个 Parker 对象，由三部分组成 _counter（计算器） ， _cond（状态） 和 _mutex （互斥锁） 当前线程调用 Unsafe.park() 方法 检查 _counter ，本情况为 0，这时，获得 _mutex 互斥锁 线程进入 _cond 条件变量阻塞 设置 _counter = 0 5. 调用 Unsafe.unpark(Thread_0) 方法，设置 _counter 为 16. 唤醒 _cond 条件变量中的 Thread_07. Thread_0 恢复运行8. 设置 _counter 为 0 9. 调用 Unsafe.unpark(Thread_0) 方法，设置 _counter 为 110. 当前线程调用 Unsafe.park() 方法11. 检查 _counter ，本情况为 1，这时线程无需阻塞，继续运行12. 设置 _counter 为 0","link":"/2024/09/01/java/juc%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/synchronized%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/"},{"title":"共享内存之volatile","text":"并发编程三大性质 原子性：保证指令不会受到线程上下文切换影响 可见性：保证指令不会受cpu缓存影响 有序性：保证指令不会受cpu指令并行优化的影响 可见性可见性是指当一个线程修改了共享变量后，其他线程能够立即得知这个修改。 volatile关键字 （易变） 可以用来修饰成员变量和竞态成员变量，它可以避免线程从自己的工作缓存中查找变量的值，必须要主存中获取他的值，线程造作volatile变量都是直接操作主存 synchronized 加锁也可以保证可见性 原子性原子性是指一个操作是不可中断的，要么全部执行成功要么全部执行失败，有着“同生共死”的感觉。及时在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程所干扰。 指令重排为了性能优化，编译器和处理器会进行指令重排序；也就是说java程序天然的有序性可以总结为：如果在本线程内观察，所有的操作都是有序的；如果在一个线程观察另一个线程，所有的操作都是无序的。 volatile 关键字修饰变量可以避免指令重排序 volatile原理volatile的底层实现原理是内存屏障，Memory Barrier （Memory Fence） 对 volatile 变量的写指令==后==会加入写屏障 对 volatile 变量的读指令==前==加入读屏障 volatile的两条实现原则 处理器缓存写回主存 一个处理器缓存写回主存时保证其他处理器缓存失效 如何保证可见性 写屏障（sfence）保证了的在该屏障之前的，对共享变量的变动，都同步到主存中 123456public void actor2 ( I_Result r) { num = 2; ready = true ; //ready 是 volatile 赋值并带写屏障 //写屏障 在写屏障之前的num 和 ready 都同步到主存中 } 读屏障（lfence）保证在该屏障之后，对共享变量的读取，加载的主存中最新的数据 12345678910public void actor2 ( I_Result r) { //读屏障 if(ready) //ready 是 volatile 读取值并带读屏障 { r.r1= num + num; }else { r.r1 = 1; } } 也就是说，写屏障保证了 写之前所有共享变量的可见性，读屏障保证了读之后所有变量的可见性 如何保证有序性 写屏障会确保指令重排时，不会将写屏障之前的代码排在写屏障之后 读屏障会确保指令重排时，不会将读屏障之后的代码排在读屏障之前 DCL 问题 double-checked locking 问题double-checked locking单例模式 123456789101112131415public class Singleton { private static Singleton INSTANSE = null; private Singleton(){}; public static Singleton getInstance(){ if (INSTANSE == null) { //无法保证 有序性 原子性 和 可见性 synchronized (Singleton.class) { if (INSTANSE == null) { INSTANSE = new Singleton(); //这里 构造函数初始化 和 实例赋值 可能出现重排序 } } } return INSTANSE; }} 懒惰实例化 首次使用getInstance时加锁 ，后续使用无需加锁 有隐含的。但很关键的一点：第一个if使用了INSTANSE 变量，是在同步块之外 INSTANSE = new Singleton(); 实际上不是原子操作 发生指令重排之后，第一步赋值 ，第二步调用构造方法，当他复制结束还没有调用构造方法时 ，另一个线程操作 if (INSTANSE == null) ，而此时这个操作在同步代码块之外，所以这是返回一个空的实例对象 ==这里可以利用 volatile 利用读写屏障保证指令重排== happens-before规则happens-before 规定了对共享变量的写操作对其他线程的读操作可见，他是可见性与有序性的一套规则总结 线程解锁 m 之前对变量的写，对于接下来对 m 加锁的其他线程对改变量的读可见 线程对volatile 变量的写，对接下来其他线程对改变量的读可见 线程start之前对变量的写，对该线程开始后对该变量的读可见 线程结束前对该变量的写，对其他线程得知他结束后的读可见 线程t1打断线程t2前对变量的写，对于其他线程得知t2被打断后对变脸的读可见 对变量默认值的写，对其他线程对该变量的读可见 具有传递性 线程安全问题-单例模式123456789101112public final class Singleton implements Serializable { //这里存在反序列化问题 private Singleton(){}; //不能防止反射创建实例 private static final Singleton INSTANSE = new Singleton(); //类初始化时初始化 public static Singleton getInstance(){ return INSTANSE; } public Object readResovle(){ //反序列化问题解决方式 JVM从内存中反序列化地&quot;组装&quot;一个新对象时,就会自动调用这个 //readResolve方法来返回我们指定好的对象了, 单例规则也就得到了保证. return INSTANSE; }} 123enum Singleton { INSTANSCE //不能用反射破环单例 可以避免反序列化破坏单例 属于饿汉式} 1234567891011public class Singleton { private static Singleton INSTANCE = null; private Singleton(){}; public static synchronized Singleton getInstance(){ //每次加锁 浪费资源 if (INSTANCE == null) { INSTANCE = new Singleton(); } return INSTANCE; }} 123456789101112131415// DCLpublic class Singleton { private volatile static Singleton INSTANCE = null; //防止指令重排序 private Singleton(){}; public static synchronized Singleton getInstance(){ if (INSTANCE== null) { synchronized (Singleton.class) { //防止 创建实例不是原子性引发的问题 if (INSTANCE == null) { INSTANCE = new Singleton(); } } } return INSTANSE; }} 12345678910public final class Singleton { private Singleton(){}; private static class LazyHolder{ static final Singleton INSTANCE = new Singleton(); //静态内部类 静态的线程安全 } public static Singleton getInstance(){ return LazyHolder.INSTANCE; //懒汉式 类加载时懒惰的 ，第一次使用时触发加载 }}","link":"/2024/09/01/java/juc%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E4%B9%8Bvolatile/"},{"title":"线程池中线程执行任务发生异常会怎么样","text":"线程池提交任务有两个方法 submit execute 使用 execute 提交任务 测试代码 123456789101112131415161718192021222324public static void main(String[] args) throws InterruptedException, ExecutionException { ThreadFactory threadFactory = new ThreadFactory() { AtomicInteger index = new AtomicInteger(1); @Override public Thread newThread(Runnable r) { Thread thread = new Thread(r); thread.setName(&quot;threadFactory-&quot;+index); index.incrementAndGet(); return thread; } }; ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(1, 1, 2, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;(), threadFactory); threadPoolExecutor.execute(() -&gt; { System.out.println(Thread.currentThread().getName()); int a = 1/0; }); TimeUnit.SECONDS.sleep(2); threadPoolExecutor.execute(() -&gt; { System.out.println(Thread.currentThread().getName()); }); } 执行结果 12345678threadFactory-1Exception in thread &quot;threadFactory-1&quot; java.lang.ArithmeticException: 除以零 at com.zclvct.leetcode.ThreadPollTest.lambda$main$0(ThreadPollTest.java:28) at com.zclvct.leetcode.ThreadPollTest$$Lambda$1/00000000037A8980.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:823)threadFactory-2 分析结果 在执行中结果中也可看出，execute执行方式抛出异常显示在控制台了 ，并且再次提交任务，发现是另一个线程去执行的任务 源码分析 在线程中提交任务是把任务包装成 worker 对象， 调用 runWorker 来执行，一下是 runWorker 方法 123456789101112131415161718192021222324252627282930313233343536373839final void runWorker(Worker w) { Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try { while (task != null || (task = getTask()) != null) { w.lock(); if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try { beforeExecute(wt, task); Throwable thrown = null; try { task.run(); } catch (RuntimeException x) { thrown = x; throw x; } catch (Error x) { thrown = x; throw x; } catch (Throwable x) { thrown = x; throw new Error(x); } finally { afterExecute(task, thrown); } } finally { task = null; w.completedTasks++; w.unlock(); } } completedAbruptly = false; } finally { processWorkerExit(w, completedAbruptly); } } 我们只看这里的主要部分 1. beforeExecute(wt, task); 执行调用前的钩子方法 2. task.run(); 执行任务 3. afterExecute(task, thrown); 执行调用后的钩子方法 4. processWorkerExit(w, completedAbruptly);其中 afterExecute(task, thrown); 在 finally 代码块中 ，并且 传递了任务本身和异常信息，可以在发生异常时提供业务补偿的方式同时 从代码中可以看出 虽然 catch 到异常了 但是没有处理 而是直接抛出。在最外层调用了processWorkerExit(w, completedAbruptly);来看一下这个方法中做了什么 123456789101112131415161718192021222324252627private void processWorkerExit(Worker w, boolean completedAbruptly) { if (completedAbruptly) // If abrupt, then workerCount wasn't adjusted decrementWorkerCount(); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { completedTaskCount += w.completedTasks; workers.remove(w); } finally { mainLock.unlock(); } tryTerminate(); int c = ctl.get(); if (runStateLessThan(c, STOP)) { if (!completedAbruptly) { int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 &amp;&amp; ! workQueue.isEmpty()) min = 1; if (workerCountOf(c) &gt;= min) return; // replacement not needed } addWorker(null, false); } } processWorkerExit 方法有两个参数，第一个参数是执行的 worker 对象， 第二个参数 代表 执行过程中是否发生了异常执行步骤 判断是否是意外退出的，如果是意外退出的话，那么就需要把WorkerCount– 加完锁后，将completedTaskCount，表示总共完成的任务数，并且从WorkerSet中将对应的Worker移除 判断当前的线程池状态，是否终止线程池 判断线程池的状态是否小于STOP，也就是处于RUNNING或者SHUTDOWN状态，如果不是不执行 上一步判断返回true 则判断线程是否抛出异常 1）如果allowCoreThreadTimeOut=true且队列不为空，那么需要至少保证有一个线程 2）如果allowCoreThreadTimeOut=false,那么需要保证线程数大于等于corePoolSize 如果线程抛出异常 放一个空的 worker 尝试新建一个线程 使用 execute 提交任务总结execute 提交任务时，当执行发生异常，那么会直接抛出异常，并且移除异常线程也就是 worker ，并且尝试放入一个新的线程 使用 submit 提交任务 测试代码 123456789101112131415161718192021222324public static void main(String[] args) throws InterruptedException, ExecutionException { ThreadFactory threadFactory = new ThreadFactory() { AtomicInteger index = new AtomicInteger(1); @Override public Thread newThread(Runnable r) { Thread thread = new Thread(r); thread.setName(&quot;threadFactory-&quot;+index); index.incrementAndGet(); return thread; } }; ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(1, 1, 2, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;(), threadFactory); Future&lt;Object&gt; future = (Future&lt;Object&gt;) threadPoolExecutor.submit(() -&gt; { System.out.println(Thread.currentThread().getName()); int a = 1 / 0; }); TimeUnit.SECONDS.sleep(2); threadPoolExecutor.execute(() -&gt; { System.out.println(Thread.currentThread().getName()); }); future.get(); } 运行结果 123456789101112131415threadFactory-1threadFactory-1Exception in thread &quot;main&quot; java.util.concurrent.ExecutionException: java.lang.ArithmeticException: 除以零 at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at com.zclvct.leetcode.ThreadPollTest.main(ThreadPollTest.java:35)Caused by: java.lang.ArithmeticException: 除以零 at com.zclvct.leetcode.ThreadPollTest.lambda$main$0(ThreadPollTest.java:28) at com.zclvct.leetcode.ThreadPollTest$$Lambda$1/000000000432EA90.run(Unknown Source) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) at java.util.concurrent.FutureTask.run(FutureTask.java) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:823) 从运行结果上来看 使用submit提交的任务，发生异常 ，不会立刻抛出异常，而是当 调用future.get();时发生异常，同时执行异常的线程没有被抛弃 源码分析 submit 提交的任务 实际上是调用FutureTask类的run方法如下： 这里他能提交 Runable 还有 Callable也是使用了适配器模式 1234567891011121314151617181920212223242526272829303132public void run() { if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try { Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) { V result; boolean ran; try { result = c.call(); ran = true; } catch (Throwable ex) { result = null; ran = false; setException(ex); } if (ran) set(result); } } finally { // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); } } 从以上代码执行中我们可以看出 result = c.call(); 被包裹在 try 代码块中，并且这里并没有和 runWorker 一样 经过异常抛出，而是在发生异常是调用了 setException(ex); 方法 1234567protected void setException(Throwable t) { if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) { outcome = t; UNSAFE.putOrderedInt(this, stateOffset, EXCEPTIONAL); // final state finishCompletion(); } } 在 setException 中 把异常信息保存了下来 ，并通过cas 操作 FutureTask 的状态为异常状态当调用get方法时，查看是异常状态则抛出异常 12345678private V report(int s) throws ExecutionException { Object x = outcome; if (s == NORMAL) return (V)x; if (s &gt;= CANCELLED) throw new CancellationException(); throw new ExecutionException((Throwable)x); } 使用 submit 提交任务 总结使用 submit 提交任务 发生异常不会直接抛出，也不会移除当前执行异常的线程，而是将异常保存在 FutureTask中，当调用get时抛出异常。 setUncaughtExceptionHandler 方法详解先对对象维护了 一个 private volatile UncaughtExceptionHandler exceptionHandler; 异常处理器 thread 定义了 一个 uncaughtException方法 ，当线程执行出现异常的时候，相当于会回调 UncaughtExceptionHandler 接口，通过 getUncaughtExceptionHandler 方法查看当前线程是否设置了 UncaughtExceptionHandler。有就调用，由于线程在创建的时候都会属于一个 ThreadGroup，会尝试调用 ThreadGroup 的 UncaughtExceptionHandler，如果还是没有设置，那么会调用 getDefaultUncaughtExceptionHandler 获取全局默认的 UncaughtExceptionHandler。 Interface for handlers invoked when a Thread abruptly terminates due to an uncaught exception.When a thread is about to terminate due to an uncaught exception the Java Virtual Machine will query the thread for its UncaughtExceptionHandler using Thread.getUncaughtExceptionHandler() and will invoke the handler’s uncaughtException method, passing the thread and the exception as arguments. If a thread has not had its UncaughtExceptionHandler explicitly set, then its ThreadGroup object acts as its UncaughtExceptionHandler. If the ThreadGroup object has no special requirements for dealing with the exception, it can forward the invocation to the default uncaught exception handler. 当线程由于未捕获的异常而突然终止时调用的处理程序接口。当线程由于未捕获的异常而即将终止时，Java 虚拟机将使用 Thread.getUncaughtExceptionHandler() 查询线程的 UncaughtExceptionHandler 并将调用处理程序的 uncaughtException 方法，将线程和异常作为参数传递。 如果一个线程没有显式设置它的 UncaughtExceptionHandler，那么它的 ThreadGroup 对象充当它的 UncaughtExceptionHandler。 如果 ThreadGroup 对象对处理异常没有特殊要求，则可以将调用转发给默认的未捕获异常处理程序","link":"/2024/09/01/java/juc%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B8%AD%E7%BA%BF%E7%A8%8B%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1%E5%8F%91%E7%94%9F%E5%BC%82%E5%B8%B8%E4%BC%9A%E6%80%8E%E4%B9%88%E6%A0%B7/"},{"title":"7种经典垃圾收集器","text":"HotSpot虚拟机的垃圾收集器两个收集器之间存在连线，就说明它们可以搭配使用，图中收集器所处的区域，则表示它是属于新生代收集器抑或是老年代收集器。 新生代收集器Serial（串行）收集器Serial收集器是一个单线程工作的收集器，但它的“单线 程”的意义并不仅仅是说明它只会使用一个处理器或一条收集线程去完成垃圾收集工作，更重要的是强调在它进行垃圾收集时，必须暂停其他所有工作线程，直到它收集结束。 优点 简单高效：由于是单线程的，实现简单，开销较小。 资源消耗低：不会占用过多的系统资源，适用于小型应用或客户端环境。 易于调试：由于是单线程的，调试和问题定位相对容易。 缺点 性能瓶颈：在多核处理器上，单线程的垃圾回收可能会成为性能瓶颈。 较长的停顿时间：由于是单线程的，垃圾回收的停顿时间可能会比较长，不适合对响应时间有严格要求的应用。 使用场景 小型应用：适用于小型应用或客户端环境，如桌面应用、嵌入式设备等。 开发和测试：在开发和测试环境中，Serial收集器因其简单性和低资源消耗而常被使用 ParNew （Parallel New）（并行）收集器ParNew收集器实质上是Serial收集器的多线程并行版本，除了同时使用多条线程进行垃圾收集之外，其余的行为包括Serial收集器可用的所有控制参数（例如：-XX：SurvivorRatio、-XX： PretenureSizeThreshold、-XX：HandlePromotionFailure等）、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一致。 优点 并行性：通过多线程并行工作，减少了垃圾回收的停顿时间。 缺点 停顿时间：虽然吞吐量高，但停顿时间可能会比其他收集器长，不适合对响应时间有严格要求的应用。 内存开销：并行回收会占用更多的系统资源 使用场景 多核处理器：在多核处理器上，ParNew收集器可以充分利用多线程的优势，提高垃圾回收的效率。 中等规模应用：对于中等规模的应用，ParNew收集器是一个不错的选择，因为它可以在保证性能的同时，减少停顿时间。 与CMS配合使用：ParNew收集器通常与CMS（Concurrent Mark-Sweep）收集器配合使用，用于年轻代的垃圾回收，而CMS用于老年代的垃圾回收。 Parallel Scavenge收集器Parallel Scavenge收集器也是一款新生代收集器，它同样是基于标记-复制算法实现的收集器，也是能够并行收集的多线程收集器。 Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量（Throughput）。所谓吞吐量就是处理器用于运行用户代码的时间与处理器总消耗时间的比值，. 123吞吐量 = \\frac{运行用户代码时间}{运行用户代码时间+运行垃圾收集时间)} Parallel Scavenge收集器也经常被称作“吞吐量优先收集器”，可以根据应用程序的运行情况自动调整垃圾回收的参数，以达到最佳性能。 +UseAdaptiveSizePolicy当这个参数被激活之后，就不需要人工指定新生代的大小、Eden与Survivor区的比例、晋升老年代对象大小等细节参数，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时 间或者最大的吞吐量。这种调节方式称为垃圾收集的自适应的调节策略（GC Ergonomics）。 优点 高吞吐量：Parallel Scavenge收集器的设计目标是最大化吞吐量，即尽可能多地让应用程序运行，减少垃圾回收的时间。 并行性：通过多线程并行工作，减少了垃圾回收的停顿时间。自适应调整：可以根据应用程序的运行情况自动调整垃圾回收的参数，以达到最佳性能。 缺点 停顿时间：虽然吞吐量高，但停顿时间可能会比其他收集器长，不适合对响应时间有严格要求的应用。 内存开销：并行回收会占用更多的系统资源。 使用场景 批处理任务：对于批处理任务或后台处理任务，Parallel Scavenge收集器是一个不错的选择，因为它可以最大化吞吐量。 大数据处理：在大数据处理场景中，Parallel Scavenge收集器可以有效地利用多核处理器的优势，提高处理速度 老年代收集器Serial Old收集器Serial Old收集器是一个单线程收集器，使用标记-整理算法。这个收集器的主要意义也是供客户端模式下的HotSpot虚拟机使用。如果在服务端模式下，它也可能有两种用途：一种是在JDK5以及之前的版本中与Parallel Scavenge收集器搭配使用，另外一种就是作为CMS收集器发生失败时的后备预案。 Parallel Old收集器Parallel Old是Parallel Scavenge收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现。在注重 吞吐量或者处理器资源较为稀缺的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器这个组合。 CMS收集器CMS（Concurrent Mark Sweep ）收集器是一种以获取最短回收停顿时间为目标的收集器。 CMS收集器是基于标记-清除算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为四个步骤。 1. 初始标记（CMS initial mark） 2. 并发标记（CMS concurrent mark） 3. 重新标记（CMS remark） 4. 并发清除（CMS concurrent sweep ） 初始标记 标记一下GC Roots能直接关联到的对象，需要Stop the world 并发标记 从GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程不需要停顿用户线程，可以与垃圾收集线程一起并发运行。 重新标记 重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录。 这个阶段的停顿时间通常会比初始标记阶段稍长一些，但也远比并发标记阶段的时间短。 并发清除 清理删除掉标记阶段判断的已经死亡的对象，由于CMS收集器是基于标记-清除算法不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发。 CMS收集器缺点 CMS收集器对处理器资源非常敏感，虽然不会导致用户线程停顿，但却会因为占用了一部分线程（或者说处理器的 算能力），降低总吞吐量。CMS默认启动的回收线程数是（处理器核心数量 +3）/4 CMS收集器无法处理“浮动垃圾”（Floating Garbage），有可能出现“Con-current Mode Failure”失败进而导致另一次完全“Stop The World”的Full GC的产生。在CMS的并发标记和并发清理阶段，用户线程是还在继续运行的，程序在运行自然就还会伴随有新的垃圾对象不断产生，但这一部分 垃圾对象是出现在标记过程结束以后，CMS无法在当次收集中处理掉它们，只好留待下一次垃圾收集时再清理掉。这一部分垃圾就称为“浮动垃圾”。 -XX：CMSInitiatingOccu-pancy Fraction cms触发内存百分比 同样也是由于在垃圾收集阶段用户线程还需要持续运行，那就还需要预留足够内存空间提供给用户线程使用，因此CMS收集器不能像其他收集器那样等待 到老年代几乎完全被填满了再进行收集，必须预留一部分空间供并发收集时的程序运作使用。要是CMS运行期间预留的内存无法满足程序分配新对象的需要，就会出现一次“并发失败”（ConcurrentM ode Failure），这时候虚拟机将 得不启动后备预案：冻结用户线程的执行，临时启用Serial Old收集器来重新进行老年代的垃圾收集， 但这样停顿时间就很长了。 CMS是一款基于“标记-清除”算法实现的收集器，空间碎片过多会导致大对象分配困难，即使老年代还有很多剩余空间，也可能因为找不到足够大的连续空间而不得不提前触发一次Full GC。 -XX:+UseCMSCompactAtFullCollection： 在CMS收集器不得不进行Full GC时，开启内存碎片的合并整理过程。默认是开启的。内存整理必须移动存活对象，无法并发进行，导致停顿时间变长。从JDK 9开始废弃。 -XX:CMSFullGCsBeforeCompaction： 要求CMS收集器在执行过若干次（数量由参数值决定）不整理空间的Full GC之后，下一次进入Full GC前会先进行碎片整理。默认值为0，表示每次进入Full GC时都进行碎片整理。通过控制碎片整理的频率，平衡停顿时间和空间碎片问题。从JDK 9开始废弃 Garbage First收集器G1是一款主要面向服务端应用的垃圾收集器。G1收集器旨在减少垃圾收集过程中的停顿时间，同时保持较高的吞吐量。可以由用户指定期望的停顿时间，与CMS 的“标记-清除”算法不同，G1从整体来看是基于“标记-整理”算法实现的收集器，但从局部（两个Region 之间）上看又是基于“标记-复制”算法实现，无论如何，这两种算法都意味着G1运作期间不会产生内存 空间碎片，垃圾收集完成之后能提供规整的可用内存。 G1基于Region的堆内存布局，G1不再坚持固定大小以及固定数量的分代区域划分，而是把连续的Java堆划分为多个大小相等的独立区域（Region），每一个Region都可以根据需要，扮演新生代的Eden空间、Survivor空间，或者老年代空间。收集器能够对扮演不同角色的Region采用不同的策略去处理。Region中还有一类特殊的Humongous区域，专门用来存储大对象。G1认为只要大小超过了一个 Region容量一半的对象即可判定为大对象。 它将Region作为单次回收的最小单元，即每次收集到的内存空间都是Region大小的整数倍，这样可以有计划地避免在整个Java堆中进行全区域的垃圾收集。让G1收集器去跟踪各个Region里面的垃圾堆积的“价值”大小，价值即回收所获得的空间大小以及回收所需时间的经验值，然后在后台维护一个优先级列表，每次根据用户设定允许的收集停顿时间（使用参数-XX：MaxGCPauseMillis指定，默认值是200毫秒），优先处理回收价值收益最大的那些Region，这也就是“Garbage First”名字的由来。 这种使用Region划分内存空间，以及具有优先级的区域回收方式，保证了G1收集器在有限的时间内获 取尽可能高的收集效率。 运作过程大致可划分为以下四个步骤： 初始标记（Initial M arking）： 仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS指针的值，让下一阶段用户线程并发运行时，能正确地在可用的Region中分配新对象。这个阶段需要 停顿线程，但耗时很短，而且是借用进行Minor GC的时候同步完成的，所以G1收集器在这个阶段实际并没有额外的停顿。 并发标记（Concurrent Marking）： 从GC Root开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。当对象图扫描完成以后，还要重新处理SATB记录下的在并发时有引用变动的对象。 最终标记（Final Marking）： 对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留下来的最后那少量的SATB记录。 筛选回收（Live Data Counting and Evacuation）： 负责更新Region的统计数据，对各个Region的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个Region 构成回收集，然后把决定回收的那一部分Region的存活对象复制到空的Region中，再清理掉整个旧 Region的全部空间。这里的操作涉及存活对象的移动，是必须暂停用户线程，由多条收集器线程并行完成的。 回收阶段（Evacuation）其实本也有想过设计成与用户程序 一起并发执行，但这件事情做起来比较复杂，考虑到G1只是回收一部分Region，停顿时间是用户可控制的，所以并不迫切去实现，而选择把这个特性放到了G1之后出现的低延迟垃圾收集器（即ZGC） 中","link":"/2024/09/01/java/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/7%E7%A7%8D%E7%BB%8F%E5%85%B8%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"},{"title":"GC中基本概念","text":"根节点枚举 迄今为止，所有收集器在根节点枚举这一步骤时都是必须暂停用户线程的，现在可达性分析算法耗时最长的查找引用链的过程已经可以做到与用户线程一起并发，但根节点枚举始终还是必须在一个能保障一致性的快照中才得以进行——这里“一致性”的意思是整个枚举期间执行子系统看起来就像被冻结在某个时间点上，不会出现分析过程中，根节点集合的对象引用关系还在不断变化的情况，若这点不能满足的话，分析结果准确性也就无法保证。这是导致垃圾收集过程必须停顿所有用户线程的其中一个重要原因，即使是号称停顿时间可控，或者几乎不会发生停顿的CMS、G1、ZGC等收集器，枚举根节点时也是必须要停顿的。 当用户线程停顿下来之后，其实并不需要一个不漏地检查完所有 执行上下文和全局的引用位置，虚拟机应当是有办法直接得到哪些地方存放着对象引用的。在HotSpot 的解决方案里，是使用一组称为OopMap（Ordinary Object Pointer Map） 的数据结构来达到这个目的。一旦类加载动作完成的时候， HotSpot就会把对象内什么偏移量上是什么类型的数据计算出来，在即时编译过程中，也 会在特定的位置记录下栈里和寄存器里哪些位置是引用。这样收集器在扫描时就可以直接得知这些信息了，并不需要真正一个不漏地从方法区等GC Roots开始查找。 OopMap（Ordinary Object Pointer Map）普通对象指针图OopMap是 HotSpot 虚拟机中的一个重要概念，主要用于垃圾回收和异常处理。以下是关于 OopMap 的关键点： 定义：OopMap 是一个位图，记录了方法执行过程中各个寄存器和栈位置中哪些位置存储了对象引用（OOPs, Ordinary Object Pointers）。 用途： 垃圾回收：在垃圾回收过程中，OopMap 帮助快速定位到所有活动的对象引用，从而确定哪些对象是可达的，哪些可以被回收。 异常处理：在发生异常时，OopMap 帮助虚拟机快速恢复到异常处理代码的位置，并确保所有对象引用的正确性。 生成时机：OopMap 在方法编译时生成，由 JIT 编译器负责创建。编译器会在方法的关键点（如 safepoint 检查点）生成 OopMap，以便在这些点上进行垃圾回收或异常处理。 安全点 定义： 安全点是JVM可以安全地中断线程而不影响程序正确性的点。 用途： 垃圾回收：在进行垃圾回收时，JVM需要确保所有线程都处于一个稳定状态，以便安全地回收不再使用的对象。 线程调度：在进行线程调度时，JVM可以利用安全点来暂停或恢复线程。 动态代码优化：JIT编译器可以在安全点对字节码进行优化。 触发条件： 显式调用：通过System.gc()等方法显式请求垃圾回收。 内存不足：当堆内存不足时，JVM会自动触发垃圾回收。 定时检查：JVM会定期检查是否需要进行垃圾回收或其他维护操作。 实现机制： 轮询：JVM在每个安全点位置插入轮询代码，检查是否需要暂停线程。 同步屏障：在多线程环境中，JVM使用同步屏障确保所有线程都能到达安全点。 性能影响： 暂停时间：频繁的垃圾回收会导致“停顿”（Stop-The-World），影响应用程序的响应时间。 优化：现代JVM通过各种优化技术（如增量收集、并发收集等）减少安全点的性能开销。 安全区域 安全点机制保证了程序执行时，在不太长的时间内就会遇到可进入垃圾收集 过程的安全点。但是，程序“不执行”的时候呢？所谓的程序不执行就是没有分配处理器时间，典型的 场景便是用户线程处于Sleep 状态或者Blocked状态，这时候线程无法响应虚拟机的中断请求，不能再走到安全的地方去中断挂起自己，虚拟机也显然不可能持续等待线程重新被激活分配处理器时间。对于这种情况，就必须引入安全区域（Safe Region）来解决。 定义： 安全区域是指能够确保在某一段代码片段之中，引用关系不会发生变化，因此，在这个区域中任意地方开始垃圾收集都是安全的。我们也可以把安全区域看作被扩展拉伸了的安全点。 当用户线程执行到安全区域里面的代码时，首先会标识自己已经进入了安全区域，那样当这段时间里虚拟机要发起垃圾收集时就不必去管这些已声明自己在安全区域内的线程了。当线程要离开安全区域时，它要检查虚拟机是否已经完成了根节点枚举（或者垃圾收集过程中其他需要暂停用户线程的阶段），如果完成了，那线程就当作没事发生过，继续执行；否则它就必须一直等待，直到收到可以离开安全区域的信号为止。 记忆集与卡表 定义 记忆集是一种用于记录从非收集区域指向收集区域的指针集合的抽象数据结构。 卡精度：每个记录精确到一块内存区域，该区域内有对象含有跨代指针。 “卡精度”所指的是用一种称为“卡表”（Card Table）的方式去实现记忆集[1]，这也是目前最常用的一种记忆集实现形式。 记忆集其实是一种“抽象”的数据结构，抽象的意思是只定义了记忆集的行为意图，并没有定义其行为的具体实现。卡表就是记忆集的一种具体实现，它定义了记忆集的记录精度、与堆内存的映射关系等。 字节数组CARD_TABLE的每一个元素都对应着其标识的内存区域中一块特定大小的内存块，这个 内存块被称作“卡页”（Card Page）。一般来说，卡页大小都是以2的N次幂的字节数，通过上面代码可 以看出HotSp ot中使用的卡页是2的9次幂，即512字节（地址右移9位，相当于用地址除以512）。那如果卡表标识内存区域的起始地址是0x0000的话，数组CARD_TABLE的第0、1、2号元素，分别对应了 地址范围为0x0000～0x01FF、0x0200～0x03FF、0x0400～0x05FF的卡页内存块[4] 一个卡页的内存中通常包含不止一个对象，只要卡页内有一个（或更多）对象的字段存在着跨代指针，那就将对应卡表的数组元素的值标识为1，称为这个元素变脏（Dirty ），没有则标识为0。在垃圾收集发生时，只要筛选出卡表中变脏的元素，就能轻易得出哪些卡页内存块中包含跨代指针，把它们加入GC Roots中一并扫描。 写屏障 写屏障可以看作在虚拟机层面对*引用类型字段赋值这个动作的AOP切面，在引用对象赋值时会产生一个环形（Around）通知，供程序执行额外的动作，也就是说赋值的 前后都在写屏障的覆盖范畴内。在赋值前的部分的写屏障叫作写前屏障（Pre-Write Barrier），在赋值后的则叫作写后屏障（Post-Write Barrier)。 定义写屏障是一种在对象引用发生变化时插入的额外代码，用于通知垃圾回收器对象引用的更改。通过写屏障，垃圾回收器可以及时更新其内部的数据结构，确保在垃圾回收过程中能够准确地识别出所有可达对象。 作用 增量更新：在对象引用发生变化时，立即更新相关数据结构，避免在垃圾回收时进行全量扫描。 并发标记：支持并发垃圾回收，允许多个线程同时运行应用程序和垃圾回收器，提高整体性能。 实现方式 预写屏障（Pre-Write Barrier）：在写操作之前插入的屏障，通常用于记录对象引用的变化。 后写屏障（Post-Write Barrier）：在写操作之后插入的屏障，用于更新相关数据结构。","link":"/2024/09/01/java/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/GC%E4%B8%AD%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"},{"title":"低延迟垃圾收集器","text":"下图中浅色阶段表示必须挂起用户线程，深色表示收集器线程与用户线程是并发工作的。 在CMS和G1之前的全部收集器，其工作的所有步骤都会产生“Stop The World”式的停顿； CMS和G1分别使用增量更新和原始快照技术，实现了标记阶段的并发，不会因管理的堆内存变大，要标记的对象变多而导致停顿时间随之增长。但是对于标记阶段之后的处理，仍未得到妥善解决。CMS使用标记-清除算法，虽然避免了整理阶段收集器带来的停顿，但是清除算法不论如何 化改进，在设计原理上避免不了空间碎片的产生，随着空间碎片不断淤积最终依然逃不过“Stop The World”的命运。G1虽然可以按更小的粒度进行回收，从而抑制整理阶段出现时间过长的停顿，但毕竟 也还是要暂停的。 Shenandoah(谢南多亚)收集器Shenandoah也是使用基于Region的堆内存布局，同样 有着用于存放大对象的Humongous Region，默认的回收策略也同样是优先处理回收价值最大的 Region……但在管理堆内存方面，它与G1至少有三个明显的不同之处. 支持并发的整理算法,G1的回收阶段是可以多线程并行的，但却不能与用户线程并发. Shenandoah（目前）是默认不使用分代收集的，换言之，不会有 专门的新生代Region或者老年代Region的存在，没有实现分代，并不是说分代对Shenandoah没有价值， 这更多是出于性价比的权衡，基于工作量上的考虑而将其放到优先级较低的位置上. Shenandoah摒弃了在G1中耗费大量内存和计算资源去维护的记忆集，改用名为“连接矩阵”（Connection Matrix）的全局数据结构来记录跨Region的引用关系，降低了处理跨代指针时的记忆集维护消耗，也降低了伪共享问题的发生概率. 初始标记（Initial Marking）： 与G1一样，首先标记与GC Roots直接关联的对象，这个阶段仍是“Stop The World”的，但停顿时间与堆大小无关，只与GC Roots的数量相关。 并发标记（Concurrent Marking）： 与G1一样，遍历对象图，标记出全部可达的对象，这个阶段是与用户线程一起并发的，时间长短取决于堆中存活对象的数量以及对象图的结构复杂程度。 最终标记（Final M arking）： 与G1一样，处理剩余的SATB扫描，并在这个阶段统计出回收价值 最高的Region，将这些Region构成一组回收集（Collection Set）。最终标记阶段也会有一小段短暂的停顿。 并发清理（Concurrent Cleanup ）： 这个阶段用于清理那些整个区域内连一个存活对象都没有找到的Region（这类Region被称为Immediate Garbage Region)。 并发回收（Concurrent Evacuation）： 并发回收阶段是Shenandoah与之前HotSpot中其他收集器的核心差异。在这个阶段，Shenandoah要把回收集里面的存活对象先复制一份到其他未被使用的Region之中。复制对象这件事情如果将用户线程冻结起来再做那是相当简单的，但如果两者必须要同时并发进行的话，就变得复杂起来了。其困难点是在移动对象的同时，用户线程仍然可能不停对被移动的对象 进行读写访问，移动对象是一次性的行为，但移动之后整个内存中所有指向该对象的引用都还是旧对象的地址，这是很难一瞬间全部改变过来的。对于并发回收阶段遇到的这些困难，Shenandoah将会通过读屏障和被称为“Brooks Pointers”的转发指针来解决。并发回收阶段运行的时间长短取决于回收集的大小。 初始引用更新（Initial Update Reference）： 并发回收阶段复制对象结束后，还需要把堆中所有指向旧对象的引用修正到复制后的新地址，这个操作称为引用更新。引用更新的初始化阶段实际上并未做什么具体的处理，设立这个阶段只是为了建立一个线程集合点，确保所有并发回收阶段中进行的收集器线程都已完成分配给它们的对象移动任务而已。初始引用更新时间很短，会产生一个非常短暂的停顿。 并发引用更新（Concurrent Update Reference）： 真正开始进行引用更新操作，这个阶段是与用户线程一起并发的，时间长短取决于内存中涉及的引用数量的多少。并发引用更新与并发标记不同，它不再需要沿着对象图来搜索，只需要按照内存物理地址的顺序，线性地搜索出引用类型，把旧值改为新值即可。 最终引用更新（Final Update Reference）： 解决了堆中的引用更新后，还要修正存在于GC Roots 中的引用。这个阶段是Shenandoah的最后一次停顿，停顿时间只与GC Roots的数量相关。 并发清理（Concurrent Cleanup ）： 经过并发回收和引用更新之后，整个回收集中所有的Region已再无存活对象，这些Region都变成Immediate Garbage Regions了，最后再调用一次并发清理过程来回收 这些Region的内存空间，供以后新对象分配使用。 ZGC收集器ZGC和Shenandoah的目标是高度相似的，都希望在尽可能对吞吐量影响不太大的前提下，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟。 ZGC也采用基于Region的堆内存布局，但与它们不同的是，ZGC的Region（在一些官方资料中将它称为Page或者ZPage）具有动态性——动态创建和销毁，以及动态的区域容量大小。在x64硬件平台下，ZGC的 Region可以具有如图3-19所示的大、中、小三类容量： 小型Region（Small Region）：容量固定为2MB，用于放置小于256KB的小对象。 中型Region（Medium Region）：容量固定为32MB，用于放置大于等于256KB但小于4M B的对 象。 大型Region（Large Region）：容量不固定，可以动态变化，但必须为2MB的整数倍，用于放置4M B或以上的大对象。每个大型Region中只会存放一个大对象，这也预示着虽然名字叫作“大型Region”，但它的实际容量完全有可能小于中型Region，最小容量可低至4MB。大型Region在ZGC的实现中是不会被重分配的，因为复制一个大对象的代价非常高昂。 ZGC收集器有一个标志性的设计是它采用的染色指针技术（Colored Pointer，其他类似的技术中可 能将它称为Tag Pointer或者Version Pointer） 染色指针可以使得一旦某个Region的存活对象被移走之后，这个Region立即就能够被释放和重用掉，而不必等待整个堆中所有指向该Region的引用都被修正后才能清理。这点相比起Shenandoah是一个颇大的优势，使得理论上只要还有一个空闲Region，ZGC就能完成收集，而Shenandoah需要等到引用 更新阶段结束以后才能释放回收集中的Region，这意味着堆中几乎所有对象都存活的极端情况，需要 1∶1复制对象到新Region的话，就必须要有一半的空闲Region来完成收集。至于为什么染色指针能够导致这样的结果，笔者将在后续解释其“自愈”特性的时候进行解释。 染色指针可以大幅减少在垃圾收集过程中内存屏障的使用数量，设置内存屏障，尤其是写屏障的 目的通常是为了记录对象引用的变动情况，如果将这些信息直接维护在指针中，显然就可以省去一些 专门的记录操作。实际上，到目前为止ZGC都并未使用任何写屏障，只使用了读屏障（一部分是染色指针的功劳，一部分是ZGC现在还不支持分代收集，天然就没有跨代引用的问题）。内存屏障对程序 运行时性能的损耗在前面章节中已经讲解过，能够省去一部分的内存屏障，显然对程序运行效率是大 有裨益的，所以ZGC对吞吐量的影响也相对较低。 染色指针可以作为一种可扩展的存储结构用来记录更多与对象标记、重定位过程相关的数据，以便日后进一步提高性能。现在Linux下的64位指针还有前18位并未使用，它们虽然不能用来寻址，却可 以通过其他手段用于信息记录。如果开发了这18位，既可以腾出已用的4个标志位，将ZGC可支持的 最大堆内存从4TB拓展到64TB，也可以利用其余位置再存储更多的标志，譬如存储一些追踪信息来让 垃圾收集器在移动对象时能将低频次使用的对象移动到不常访问的内存区域。 并发标记（Concurrent Mark）：与G1、Shenandoah一样，并发标记是遍历对象图做可达性分析的 阶段，前后也要经过类似于G1、Shenandoah的初始标记、最终标记（尽管ZGC中的名字不叫这些）的 短暂停顿，而且这些停顿阶段所做的事情在目标上也是相类似的。与G1、Shenandoah不同的是，ZGC 的标记是在指针上而不是在对象上进行的，标记阶段会更新染色指针中的M arked 0、M arked 1标志 位。 并发预备重分配（Concurrent Prep are for Relocate）：这个阶段需要根据特定的查询条件统计得出 本次收集过程要清理哪些Region，将这些Region组成重分配集（Relocation Set）。重分配集与G1收集器 的回收集（Collection Set）还是有区别的，ZGC划分Region的目的并非为了像G1那样做收益优先的增量回收。相反，ZGC每次回收都会扫描所有的Region，用范围更大的扫描成本换取省去G1中记忆集的 维护成本。因此，ZGC的重分配集只是决定了里面的存活对象会被重新复制到其他的Region中，里面 的Region会被释放，而并不能说回收行为就只是针对这个集合里面的Region进行，因为标记过程是针对 全堆的。此外，在JDK 12的ZGC中开始支持的类卸载以及弱引用的处理，也是在这个阶段中完成的。 并发重分配（Concurrent Relocate）：重分配是ZGC执行过程中的核心阶段，这个过程要把重分 配集中的存活对象复制到新的Region上，并为重分配集中的每个Region维护一个转发表（Forward Table），记录从旧对象到新对象的转向关系。得益于染色指针的支持，ZGC收集器能仅从引用上就明 确得知一个对象是否处于重分配集之中，如果用户线程此时并发访问了位于重分配集中的对象，这次 访问将会被预置的内存屏障所截获，然后立即根据Region上的转发表记录将访问转发到新复制的对象 上，并同时修正更新该引用的值，使其直接指向新对象，ZGC将这种行为称为指针的“自愈”（Self- Healing）能力。这样做的好处是只有第一次访问旧对象会陷入转发，也就是只慢一次，对比 Shenandoah的Brooks转发指针，那是每次对象访问都必须付出的固定开销，简单地说就是每次都慢， 因此ZGC对用户程序的运行时负载要比Shenandoah来得更低一些。还有另外一个直接的好处是由于染 色指针的存在，一旦重分配集中某个Region的存活对象都复制完毕后，这个Region就可以立即释放用于 新对象的分配（但是转发表还得留着不能释放掉），哪怕堆中还有很多指向这个对象的未更新指针也 没有关系，这些旧指针一旦被使用，它们都是可以自愈的。 并发重映射（Concurrent Remap ）：重映射所做的就是修正整个堆中指向重分配集中旧对象的所 有引用，这一点从目标角度看是与Shenandoah并发引用更新阶段一样的，但是ZGC的并发重映射并不 是一个必须要“迫切”去完成的任务，因为前面说过，即使是旧引用，它也是可以自愈的，最多只是第 一次使用时多一次转发和修正操作。重映射清理这些旧引用的主要目的是为了不变慢（还有清理结束 后可以释放转发表这样的附带收益），所以说这并不是很“迫切”。因此，ZGC很巧妙地把并发重映射 阶段要做的工作，合并到了下一次垃圾收集循环中的并发标记阶段里去完成，反正它们都是要遍历所 有对象的，这样合并就节省了一次遍历对象图[9]的开销。一旦所有指针都被修正之后，原来记录新旧 对象关系的转发表就可以释放掉了。","link":"/2024/09/01/java/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E4%BD%8E%E5%BB%B6%E8%BF%9F%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"},{"title":"三色标记","text":"可达性分析算法理论上要求全过程都基于一个能保障一致性的快照中才能够进行分析， 这意味着必须全程冻结用户线程的运行。在根节点枚举这个步骤中，由于GC Roots相比 起整个Java堆中全部的对象毕竟还算是极少数，且在各种优化技巧（如OopMap ）的加持下，它带来的停顿已经是非常短暂且相对固定（不随堆容量而增长）的了。可从GC Roots再继续往下遍历对象图，这一步骤的停顿时间就必定会与Java堆容量直接成正比例关系：堆越大，存储的对象越多，对象图结构越复杂，要标记更多对象而产生的停顿时间自然就更长。 三色标记三色标记（Tri-color Marking）把遍历对象图过程中遇到的对象，按照“是否访问过”这个条件标记成以下三种颜色： 白色：表示对象尚未被垃圾收集器访问过。显然在可达性分析刚刚开始的阶段，所有的对象都是白色的，若在分析结束的阶段，仍然是白色的对象，即代表不可达。 黑色：表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经扫描过。黑色的对象代表已经扫描过，它是安全存活的，如果有其他对象引用指向了黑色对象，无须重新扫描一遍。黑色对象不可能直接（不经过灰色对象）指向某个白色对象。 灰色：表示对象已经被垃圾收集器访问过，但这个对象上至少存在一个引用还没有被扫描过。 如果在标记过程中用户线程此时是冻结的，只有收集器线程在工作，那不会有任何问题。但如果用户线程与收集器是并发工作，收集器在对象图上标记颜色，同时用户线程在修改引用 关系——即修改对象图的结构，这样可能出现两种后果： 多标：把原本消亡的对象错误标记为存活，这不是好事，但其实是可以容忍的，只不过产生了一点逃过本次收集的浮动垃圾而已，下次收集清理掉就好。 漏标：把原本存活的对象错误标记为已消亡，这就是非常致命的后果，程序肯定会因此发生错误。 Wilson于1994年在理论上证明了，当且仅当以下两个条件同时满足时，会产生“对象消失”的问题，即原本应该是黑色的对象被误标为白色： 赋值器插入了一条或多条从黑色对象到白色对象的新引用 赋值器删除了全部从灰色对象到该白色对象的直接或间接引用 我们要解决并发扫描时的对象消失问题，只需破坏这两个条件的任意一个即可。由此分别 产生了两种解决方案：增量更新（Incremental Update）和原始快照（Snapshot At The Beginning，SATB）。 增量更新 增量更新要破坏的是第一个条件，当黑色对象插入新的指向白色对象的引用关系时，就将这个新插入的引用记录下来，等并发扫描结束之后，再将这些记录过的引用关系中的黑色对象为根，重新扫描一次。这可以简化理解为，黑色对象一旦新插入了指向白色对象的引用之后，它就变回灰色对象了。 原始快照 原始快照要破坏的是第二个条件，当灰色对象要删除指向白色对象的引用关系时，就将这个要删除的引用记录下来，在并发扫描结束之后，再将这些记录过的引用关系中的灰色对象为根，重新扫描一次。这也可以简化理解为，无论引用关系删除与否，都会按照刚刚开始扫描那一刻的对象图快照来进行搜索。 以上无论是对引用关系记录的插入还是删除，虚拟机的记录操作都是通过写屏障实现的。在 HotSpot虚拟机中，增量更新和原始快照这两种解决方案都有实际应用，譬如，CMS是基于增量更新来做并发标记的，G1、Shenandoah则是用原始快照来实现","link":"/2024/09/01/java/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E4%B8%89%E8%89%B2%E6%A0%87%E8%AE%B0/"},{"title":"分代收集理论","text":"分代假说它建立在三个分代假说之上： 弱分代假说（Weak Generational Hypothesis）：绝大多数对象都是朝生夕灭的。 强分代假说（Strong Generational Hypothesis）：熬过越多次垃圾收集过程的对象就越难以消亡。 跨代引用假说（Intergenerational Reference Hypothesis）：跨代引用相对于同代引用来说仅占极少数。 收集器应该将Java堆划分出不同的区域，然后将回收对象依据其年龄（年龄即对象熬过垃圾收集过程的次数）分配到不同的区域之中存储： 如果一个区域中大多数对象都是朝生夕灭，难以熬过垃圾收集过程的话，那么把它们集中放在一起，每次回收时只关注如何保留少量存活而不是去标记那些大量将要被回收的对象，就能以较低代价回收到大量的空间；*—- 弱分代假说* 如果剩下的都是难以消亡的对象，那把它们集中放在一块， 虚拟机便可以使用较低的频率来回收这个区域，这就同时兼顾了垃圾收集的时间开销和内存的空间有效利用。*—- 强分代假说* 我们就不应再为了少量的跨代引用去扫描整个老年代，也不必浪费空间专门记录每一个对象是否存在及存在哪些跨代引用，只需在新生代上建立一个全局的数据结构（该结构被称 为“记忆集”，Remembered Set），这个结构把老年代划分成若干小块，标识出老年代的哪一块内存会存在跨代引用。此后当发生MinorGC时，只有包含了跨代引用的小块内存里的对象才会被加入到GC Roots进行扫描。虽然这种方法需要在对象改变引用关系（如将自己或者某个属性赋值）时维护记录数据的正确性，会增加一些运行时的开销，但比起收集时扫描整个老年代来说仍然是划算的. —- 跨代引用假说 GC收集名称定义部分收集（Partial GC）：指目标不是完整收集整个Java堆的垃圾收集，其中又分为： 新生代收集（Minor GC/Young GC）：指目标只是新生代的垃圾收集。 老年代收集（Major GC/Old GC）：指目标只是老年代的垃圾收集。目前只有CMS收集器会有单独收集老年代的行为。另外请注意“Major GC”这个说法现在有点混淆，在不同资料上常有不同所指，读者需按上下文区分到底是指老年代的收集还是整堆收集。 混合收集（Mixed GC）：指目标是收集整个新生代以及部分老年代的垃圾收集。目前只有G1收集器会有这种行为。 整堆收集（Full GC）：收集整个Java堆和方法区的垃圾收集。","link":"/2024/09/01/java/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%88%86%E4%BB%A3%E6%94%B6%E9%9B%86%E7%90%86%E8%AE%BA/"},{"title":"可达性分析算法","text":"通过一系列称为“GC Roots”的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过 程所走过的路径称为“引用链”（Reference Chain），如果某个对象到GC Roots间没有任何引用链相连， 或者用图论的话来说就是从GC Roots到这个对象不可达时，则证明此对象是不可能再被使用的。 在Java技术体系里面，固定可作为GC Roots的对象包括以下几种： 在虚拟机栈（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的 参数、局部变量、临时变量等。 在方法区中类静态属性引用的对象，譬如Java类的引用类型静态变量。 在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用。 在本地方法栈中JNI（即通常所说的Native方法）引用的对象。 Java虚拟机内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象（比如 NullPointExcepiton、OutOfMemoryError）等，还有系统类加载器。 所有被同步锁（synchronized关键字）持有的对象。 反映Java虚拟机内部情况的JM XBean、JVM TI中注册的回调、本地代码缓存等。","link":"/2024/09/01/java/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%8F%AF%E8%BE%BE%E6%80%A7%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95/"},{"title":"基本垃圾回收算法","text":"在对基本回收策略进行描述时，我们假设赋值器运行在一个或者多个线程之上，且只有一个回收器线程，当回收器线程运行时，所有的赋值器线程均处于停止状态。这种万物静止(stop the world) 的策略大幅简化了回收器的实现。从赋值器线程角度来看，回收过程的执行是原子性的，即赋值器线程感知不到回收器的任何中间状态，回收器也不会受到赋值器线程的任何干扰。 标记－清除（mark-sweep) 回收过程分为两个阶段 第一阶段为追踪(trace) 阶段，即回收器从根集合（寄存器、线程栈、全局变址）开始遍历对象图，并标记(mark) 所遇到的每个对象； 第二阶段为清扫 (sweep)阶段，即回收器检查堆中每一个对象，并将所有未标记的对象当作垃圾进行回收。 主要缺点有两个： 第一个是执行效率不稳定，如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增长而降低； 第二个是内存空间的碎片化问题，标记、清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 标记－复制 (mark-copy) 它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 每次都是针对整个半区进行内存回收，分配内存时也就不用考虑有 间碎片的复杂情况，只要移动堆顶指针，按顺序分配即可。这样实现简单，运行高效。 这种复制回收算法的代价是将可用内存缩小为了原来的一半，空间浪费太多。标 标记－整理 (mark-compact) 标记-清除算法与标记-整理算法的本质差异在于前者是一种非移动式的回收算法，而后者是移动式的。其中的标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存。","link":"/2024/09/01/java/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AD%96%E7%95%A5-%E5%9F%BA%E6%9C%AC%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95/"},{"title":"慢查询分析","text":"对于MySQL，最简单的衡量查询开销的三个指标如下： 响应时间：服务时间和排队时间之和。服务时间是指数据库处理这个查询真正花了多长时间。排队时间是指服务器因为等待某些资源而没有真正执行查询的时间——可能是等I/O操作完成，也可能是等待行锁。 扫描的行数：一条查询，如果性能很差，最常见的原因是访问的数据太多。大部分性能低下的查询都可以通过减少访问的数据量的方式进行优化。有时候也可能是访问了太多的列；（每次看到SELECT*的时候都需要用怀疑的眼光审视，是不是真的需要返回全部的列，很可能不是必需的。取出全部列，会让优化器无法完成索引覆盖扫描这类优化，还会为服务器带来额外的I/O、内存和CPU的消耗） 返回的行数：会给服务器带来额外的I/O、内存和CPU的消耗（使用limit限制返回行数） 分析示例我们看一下示例数据库Sakila中的一个查询案例： 1select * from film_actor where film_id = 1 -- film_id列有索引 这个查询将返回10行数据，从EXPLAIN的结果可以看到，MySQL在索引idx_fk_film_id上使用了ref访问类型来执行查询： 图片 图片 EXPLAIN的结果还显示MySQL预估需要访问10行数据。换句话说，查询优化器认为这种访问类型可以高效地完成查询。 如果没有合适的索引会怎样呢？MySQL就不得不使用一种糟糕的访问类型，下面来看看如果删除对应的索引再来运行这个查询会发生什么情况： 图片 访问类型变成了一个全表扫描(ALL)，现在MySQL预估需要扫描5462条记录来完成这个查询。这里的“Using where”表示MySQL将通过WHERE条件来筛选存储引擎返回的记录。 一般地，MySQL能够使用如下三种方式应用WHERE条件，从好到坏依次为： 在索引中使用WHERE条件来过滤不匹配的记录。这是在存储引擎层完成的。 使用索引覆盖扫描（在Extra列中出现了Using index）来返回记录，直接从索引中过滤不需要的记录并返回命中的结果。这是在MySQL服务器层完成的，但无须再回表查询记录。 举例说明一下：索引列a，b，c，查询条件时 a = xx and c = xx 从数据表中返回数据，然后过滤不满足条件的记录（在Extra列中出现Using where）。这在MySQL服务器层完成，MySQL需要先从数据表中读出记录然后过滤。 三星索引“三星系统”(three-star system)评价体系，用以判断一个索引是不是适合某个查询语句： 索引将相关的记录放到一起则获得“一星”； 如果索引中的数据顺序和查找中的排列顺序一致则获得“二星”； 如果索引中的列包含了查询中需要的全部列则获得“三星”。 聚簇索引聚簇索引并不是一种单独的索引类型，而是一种数据存储方式，术语“聚簇”表示数据行和相邻的键值紧凑地存储在一起。InnoDB的聚簇索引实际上保存了B-tree索引和数据行（聚簇索引的每一个叶子节点都包含了主键值、事务ID、用于事务和MVCC的回滚指针，以及所有的剩余列）。当表有聚簇索引时，它的数据行实际上存放在索引的叶子页(leaf page)中。一个表只能有一个聚簇索引，叶子页包含了一条记录的全部数据。 InnoDB根据主键聚簇数据。如果你没有定义主键，InnoDB会选择一个唯一的非空索引代替。如果没有这样的索引，InnoDB会隐式定义一个主键来作为聚簇索引。聚集的数据有一些重要的优点： 可以把相互关联的数据保存在一起，数据访问更快。聚簇索引将索引和数据保存在同一个B-tree中，因此从聚簇索引中获取数据通常比在非聚簇索引中查找要快。 同时，聚簇索引也有一些缺点： 插入速度严重依赖于插入顺序。按照主键的顺序插入行是将数据加载到InnoDB表中最快的方式。但如果不是按照主键的顺序加载数据，那么在加载完成后最好使用OPTIMIZE TABLE命令重新组织一下表。 更新聚簇索引列的代价很高，因为它会强制InnoDB将每个被更新的行移动到新的位置。 基于聚簇索引的表在插入新行，或者主键被更新导致需要移动行的时候，可能面临页分裂(page split)的问题。当行的主键值要求必须将这一行插入某个已满的页中时，存储引擎会将该页分裂成两个页面来容纳该行，这就是一次页分裂操作。页分裂会导致表占用更多的磁盘空间。 覆盖索引如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为覆盖索引。覆盖索引是非常有用的工具，能够极大地提高性能。试想一下，如果查询只需要扫描索引而无须回表，会带来多少好处： 索引条目通常远小于数据行大小，所以如果只需要读取索引，那么MySQL就会极大地减少数据访问量。 因为索引是按照列值的顺序存储的（至少在单页内如此），所以对于I/O密集型的范围查询会比随机从磁盘读取每一行数据的I/O要少得多。 由于InnoDB的聚簇索引的特点，覆盖索引对InnoDB表特别有用。InnoDB的二级索引在叶子节点中保存了记录的主键值，所以如果二级索引能够覆盖查询，则可以避免对主键索引的二次查询。（二级索引访问需要两次索引查找，而不是一次，通过二级索引查找行，存储引擎需要找到二级索引的叶子节点，以获得对应的主键值，然后根据这个值去聚簇索引中查找对应的行。） 索引列顺序（怎么指定索引列顺序）不需要考虑排序和分组时，将选择性最高的列放在前面通常是很好的。这时索引的作用只是优化查询语句中的WHERE条件。在这种情况下，按这个原则设计的索引确实能够最快地过滤出需要的行。以下面的查询为例： 1select * from payment where staff_id = 123 and customer_id = 456 是应该创建一个(staff_id、customer_id)索引还是应该颠倒一下顺序？这时，可以通过运行某些查询来确定在这个表中值的分布情况，并确定哪列的选择性更高。先用下面的查询预测一下，看看各个WHERE条件的分支对应的数据基数有多大： 根据前面的经验法则，应该将索引列customer_id放到前面，因为对应条件值的customer_id数量更小。 索引最左前缀法则‌指在MySQL中使用索引时，查询条件需要从索引的最左边开始，并且不能跳过索引中的列。如果查询条件跳过了索引中的某列，那么索引将失效，后续的列也不会被使用。‌ 12select * from payment where staff_id = 123 -- 索引是customer_id,staff_id，因为漏掉了customer_id，所以走不上索引排序是否命中索引 无论如何排序都是一个成本很高的操作，所以从性能角度考虑，应尽可能避免排序或者尽可能避免对大量数据进行排序。当不能使用索引生成排序结果的时候，MySQL需要自己进行排序，如果数据量小则在内存中进行，如果数据量大则需要使用磁盘，不过MySQL将这个过程统一称为文件排序(filesort)，即使完全是在内存中排序不需要任何磁盘文件时也是如此。 只有当索引的顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向（倒序或正序）都一样时，MySQL才能使用索引来对结果做排序。如果查询需要联接多张表，则只有当ORDER BY子句引用的字段全部在第一个表中时，才能使用索引做排序。ORDER BY子句和查找型查询的限制是一样的：需要满足索引的最左前缀的要求，否则，MySQL需要执行排序操作，而无法利用索引排序。 有一种特殊情况，如果前导列为常量的时候，ORDER BY子句中的列也可以不满足索引的最左前缀的要求。如果在WHERE子句或者JOIN子句中将这些列指定为了常量，就可以“填补”索引字段的间隙了。 例如，Sakila示例数据库的表rental在列(rental_date，inventory_id，customer_id)上建有名称为rental_date的索引： 不需要文件排序MySQL可以使用rental_date索引为下面的查询做排序，从EXPLAIN中可以看到没有出现文件排序(filesort)操作： 1select * from rental where rental_date = '2005-05-25' order by inventory_id, customer_id -- rental_date是常量，后面的排序列则认为符合最左前缀 1select * from rental where rental_date = '2005-05-25' order by inventory_id, customer_id,id -- rental_date是常量，后面的排序列则认为符合最左前缀，虽然索引里面没有显示指定id列，id是隐性的包含在索引中的，所以也无需文件排序 即使ORDER BY子句不满足索引的最左前缀的要求，也可以用于查询排序，这正是因为索引的第一列被指定为了一个常数。 需要文件排序 下面这个查询也没有问题，因为ORDER BY使用的两列就是索引的最左前缀(索引顺序：rental_date，inventory_id，customer_id)： 1select * from rental where rental_date &gt; '2005-05-25' order by rental_date, inventory_id -- 符合最左前缀 下面是一些不能使用索引做排序的查询： 下面这个查询使用了两种不同的排序方向，但是索引中的列都是按正序排序的(索引顺序：rental_date，inventory_id，customer_id)：1select * from rental where rental_date = '2005-05-25' order by inventory_id asc, customer_id desc -- 满足最左前缀，但是排序方向不一样 在下面这个查询的ORDER BY子句中，引用了一个不在索引中的列(索引顺序：rental_date，inventory_id，customer_id)：1select * from rental where rental_date = '2005-05-25' order by inventory_id ,staff_id -- staff_id不在索引中 下面这个查询的WHERE和ORDER BY中的列无法组合成索引的最左前缀(索引顺序：rental_date，inventory_id，customer_id)：1select * from rental where rental_date = '2005-05-25' order by customer_id -- 不满足最左前缀，漏掉了inventory_id 下面这个查询在索引列的第一列上是范围条件，所以MySQL无法使用索引的其余列(索引顺序：rental_date，inventory_id，customer_id)：1select * from rental where rental_date &gt; '2005-05-25' order by inventory_id -- 第一列如果是范围查询，则认为不符合最左前缀 这个查询在inventory_id列上有多个等于条件。对于排序来说，这也是一种范围查询(索引顺序：rental_date，inventory_id，customer_id)：1select * from rental where rental_date = '2005-05-25' and inventory_id in (1,2) order by customer_id -- inventory_id 条件是范围查询，则认为不符合最左前缀","link":"/2024/09/01/mysql/%E7%B4%A2%E5%BC%95/%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90/"},{"title":"Java线程调度","text":"线程调度是指系统为线程分配处理器使用权的过程，调度主要方式有两种，分别是协同式 （Coop erative Threads-Scheduling）线程调度和抢占式（Preemptive Threads-Scheduling）线程调度。 协同式调度的多线程系统: 线程的执行时间由线程本身来控制，线程把自己的工作执行完了之后，要主动通知系统切换到另外一个线程上去。协同式多线程的最大好处是实现简单，而且由于线程要把自己的事情干完后才会进行线程切换，切换操作对线程自己是可知的，所以一般没有什么 线程同步的问题。Lua语言中的“协同例程”就是这类实现。它的坏处也很明显：线程执行时间不可控制，甚至如果一个线程的代码编写有问题，一直不告知系统进行线程切换，那么程序就会一直阻塞在那里。很久以前的Windows 3.x系统就是使用协同式来实现多进程多任务的，那是相当不稳定的，只要有一个进程坚持不让出处理器执行时间，就可能会导致整个系统崩溃。 抢占式调度的多线程系统 每个线程将由系统来分配执行时间，线程的切换不由线程本身来决定。譬如在Java中，有Thread::yield()方法可以主动让出执行时间，但是如果想要主动获取执行时间，线程本身是没有什么办法的。在这种实现线程调度的方式下，线程的执行时间是系统可 的，也不会有一个线程导致整个进程甚至整个系统阻塞的问题。Java使用的线程调度方式就是抢占式调度。与前面所说的Windows 3.x的例子相对，在Windows 9x/NT内核中就是使用抢占式来实现多进 的，当一个进程出了问题，我们还可以使用任务管理器把这个进程杀掉，而不至于导致系统崩溃。 线程优先级java 语言一共设置了10个级别的线程优先级（Thread.MIN_PRIORITY至Thread.M AX_PRIORITY）。在两个线程同时处于Ready状态时，优先级越高的线程越容易被系统选择执行。 不过，线程优先级并不是一项稳定的调节手段，很显然因为主流虚拟机上的Java线程是被映射到系统的原生线程上来实现的，所以线程调度最终还是由操作系统说了算。尽管现代的操作系统基本都提供线程优先级的概念，但是并不见得能与Java线程的优先级一一对应，如Solaris中线程有 2147483648（2的31次幂）种优先级，但Windows中就只有七种优先级。如果操作系统的优先级比Java线程优先级更多，那问题还比较好处理，中间留出一点空位就是了，但对于比Java线程优先级少的 统，就不得不出现几个线程优先级对应到同一个操作系统优先级的情况了。 线程优先级并不是一项稳定的调节手段，这不仅仅体现在某些操作系统上不同的优先级实际会变得相同这一点上，还有其他情况让我们不能过于依赖线程优先级：优先级可能会被系统自行改变，例如在Windows系统中存在一个叫“优先级推进器”的功能（Priority Boosting，当然它可以被关掉），大致作用是当系统发现一个线程被执行得特别频繁时，可能会越过线程优先级去为它分配执行时间，从 而减少因为线程频繁切换而带来的性能损耗。因此，我们并不能在程序中通过优先级来完全准确判断 一组状态都为Ready 的线程将会先执行哪一个。","link":"/2024/09/01/java/jvm/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/Java%E7%BA%BF%E7%A8%8B%E8%B0%83%E5%BA%A6/"},{"title":"volatile型变量","text":"当一个变量被定义成volatile之后，它将具备两项特性： 保证此变量对所有线程的可见性，这里的“可见性”是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。而普通变量并不能做到这一点，普通变量的值在线程间传递时均需要通过主内存来完成。比如， 线程A修改一个普通变量的值，然后向主内存进行回写，另外一条线程B在线程A回写完成了之后再对主内存进行读取操作，新变量值才会对线程B可见。 禁止指令重排序优化，普通的变量仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致。因为在同一个线程的方法执行过程中无法感知到这点，这就是Java内存模型中描述的 所谓“线程内表现为串行的语义”。 volatile变量定义的特殊规则的定义。假定T表示一个线程，V和W分别表示两个volatile型变量，那么在进行read、load、use、assign、store和write操作时需要满足如下规则： 只有当线程T对变量V执行的前一个动作是load的时候，线程T才能对变量V执行use动作；并且， 只有当线程T对变量V执行的后一个动作是use的时候，线程T才能对变量V执行load动作。线程T对变量V的use动作可以认为是和线程T对变量V的load、read动作相关联的，必须连续且一起出现。 这条规则要求在工作内存中，每次使用V前都必须先从主内存刷新最新的值，用于保证能看见其他线程对变量V所做的修改。 只有当线程T对变量V执行的前一个动作是assign的时候，线程T才能对变量V执行store动作；并 且，只有当线程T对变量V执行的后一个动作是store的时候，线程T才能对变量V执行assign动作。线程 T对变量V的assign动作可以认为是和线程T对变量V的store、write动作相关联的，必须连续且一起出 现。 这条规则要求在工作内存中，每次修改V后都必须立刻同步回主内存中，用于保证其他线程可以 看到自己对变量V所做的修改。 假定动作A是线程T对变量V实施的use或assign动作，假定动作F是和动作A相关联的load或store动 作，假定动作P是和动作F相应的对变量V的read或write动作；与此类似，假定动作B是线程T对变量W 实施的use或assign动作，假定动作G是和动作B相关联的load或store动作，假定动作Q是和动作G相应的 对变量W的read或write动作。如果A先于B，那么P先于Q。 这条规则要求volatile修饰的变量不会被指令重排序优化，从而保证代码的执行顺序与程序的顺序 相同。","link":"/2024/09/01/java/jvm/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/volatile%E5%9E%8B%E5%8F%98%E9%87%8F/"},{"title":"主内存与工作内存","text":"Java内存模型规定了所有的变量都存储在主内存（Main Memory ）中，每条线程还有自己的工作内存（Working Memory ），线程的工作内存中保存了被该线程使用的变量的主内存副本，线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的数据。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成，线程、主内存、工作内存三者的交互关系如图： 从变量、主内存、工作内存的定义来看，主内存主要对应于Java堆中的对象实例数据部分，而工作内存则对应于虚拟机栈中的部分区域。从更基础的层次上说，主内存直接对应于物理硬件的内存，而为了获取更好的运行速度，虚拟机（或者是硬件、操作系统本身的优化措施）可能会让工作内存优先存储于寄存器和高速缓存中，因为程序运行时主要访问的是工作内存。","link":"/2024/09/01/java/jvm/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/%E4%B8%BB%E5%86%85%E5%AD%98%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%86%85%E5%AD%98/"},{"title":"主内存与工作内存间交互操作","text":"关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存、如何从作内存同步回主内存这一类的实现细节，Java内存模型中定义了以下8种操作来完成。Java虚拟机实现时必须保证下面提及的每一种操作都是原子的、不可再分的. lock（锁定）：作用于主内存的变量，它把一个变量标识为一条线程独占的状态。 unlock（解锁）：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用。 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store（存储）：作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的write操作使用。 write（写入）：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。 如果要把一个变量从主内存拷贝到工作内存，那就要按顺序执行read和load操作，如果要把变量从工作内存同步回主内存，就要按顺序执行store和write操作。注意，Java内存模型只要求上述两个操作必须按顺序执行，但不要求是连续执行。也就是说read与load之间、store与write之间是可插入其他指令的，如对主内存中的变量a、b进行访问时，一种可能出现的顺序是read a、read b、load b、load a。除此之外，Java内存模型还规定了在执行上述8种基本操作时必须满足如下规则： 不允许read和load、store和write操作之一单独出现，即不允许一个变量从主内存读取了但工作内存不接受，或者工作内存发起回写了但主内存不接受的情况出现。 不允许一个线程丢弃它最近的assign操作，即变量在工作内存中改变了之后必须把该变化同步回主内存。 不允许一个线程无原因地（没有发生过任何assign操作）把数据从线程的工作内存同步回主内存中。 一个新的变量只能在主内存中“诞生”，不允许在工作内存中直接使用一个未被初始化（load或 assign）的变量，换句话说就是对一个变量实施use、store操作之前，必须先执行assign和load操作。 一个变量在同一个时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。 如果对一个变量执行lock操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作以初始化变量的值。 如果一个变量事先没有被lock操作锁定，那就不允许对它执行unlock操作，也不允许去unlock一个被其他线程锁定的变量。 对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store、write操作）。","link":"/2024/09/01/java/jvm/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/%E4%B8%BB%E5%86%85%E5%AD%98%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%86%85%E5%AD%98%E9%97%B4%E4%BA%A4%E4%BA%92%E6%93%8D%E4%BD%9C/"},{"title":"原子性、可见性与有序性","text":"原子性（Atomicity ） 由Java内存模型来直接保证的原子性变量操作包括read、load、assign、use、store和write这六个， 我们大致可以认为，基本数据类型的访问、读写都是具备原子性的。如果应用场景需要一个更大范围的原子性保证（经常会遇到），Java内存模型还提供了lock和 unlock操作来满足这种需求，尽管虚拟机未把lock和unlock操作直接开放给用户使用，但是却提供了更高层次的字节码指令monitorenter和monitorexit来隐式地使用这两个操作。这两个字节码指令反映到Java 代码中就是同步块——synchronized关键字，因此在synchronized块之间的操作也具备原子性。 可见性（Visibility ）可见性就是指当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改。Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性的，无论是普通变量还是volatile变量都是如此。普通变量与volatile变量的区别是，volatile的特殊规则保证了新 能立即同步到主内存，以及每次使用前立即从主内存刷新。因此我们可以说volatile保证了多线程操作 时变量的可见性，而普通变量则不能保证这一点。 除了volatile之外，Java还有两个关键字能实现可见性，它们是synchronized和final。 同步块的可见性是由“对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store、write操作）”这条规则获得的。 而final关键字的可见性是指：被final修饰的字段在构造器中一旦被初始化完成，并且构造器没有把“this”的引用传递出去（this引用逃逸是一件很危险的事情，其他线程有可能通 过这个引用访问到“初始化了一半”的对象），那么在其他线程中就能看见final字段的值。 有序性（Ordering）Java程序中天然的有序性可总结为一句话：如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程， 所有的操作都是无序的。前半句是指“线程内似表现为串行的语义”（Within-Thread As-If-Serial Semantics），后半句是指“指令重排序”现象和“工作内存与主内存同步延迟”现象。 Java语言提供了volatile和sychronized两个关键字来保证线程之间操作的有序性，volatile关键字本身就包含了禁止指令重排序的语义，而synchronized则是由“一个变量在同一个时刻只允许一条线程对其进行lock操作”这条规则获得的，这个规则决定了持有同一个锁的两个同步块只能串行地进入。","link":"/2024/09/01/java/jvm/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/%E5%8E%9F%E5%AD%90%E6%80%A7%E3%80%81%E5%8F%AF%E8%A7%81%E6%80%A7%E4%B8%8E%E6%9C%89%E5%BA%8F%E6%80%A7/"},{"title":"线程的实现","text":"实现线程主要有三种方式：使用内核线程实现（1：1实现），使用用户线程实现（1：N实现）， 使用用户线程加轻量级进程混合实现（N：M 实现） 内核线程实现内核线程实现的方式也被称为1：1实现。内核线程（Kernel-Level Thread，KLT）就是直接由操作系统内核（Kernel，下称内核）支持的线程，这种线程由内核来完成线程切换，内核通过操纵调度器（Scheduler）对线程进行调度，并负责将线程的任务映射到各个处理器上。每个内核线程可以视为内核的一个分身，这样操作系统就有能力同时处理多件事情，支持多线程的内核就称为多线程内核（Multi-Threads Kernel）。程序一般不会直接使用内核线程，而是使用内核线程的一种高级接口——轻量级进程（Light Weight Process，LWP），轻量级进程就是我们通常意义上所讲的线程，由于每个轻量级进程都由一个内核线程支持，因此只有先支持内核线程，才能有轻量级进程。这种轻量级进程与内核线程之间1：1 的关系称为一对一的线程模型， 由于内核线程的支持，每个轻量级进程都成为一个独立的调度单元，即使其中某一个轻量级进程在系统调用中被阻塞了，也不会影响整个进程继续工作。轻量级进程也具有它的局限性：首先，由于是基于内核线程实现的，所以各种线程操作，如创建、析构及同步，都需要进行系统调用。而系统调用的代价相对较高，需要在用户态（User Mode）和内核态（Kernel Mode）中来回切换。其次，每个轻量级进程都需要有一个内核线程的支持，因此轻量级进程要消耗一定的内核资源（如内核线程的栈空间），因此一个系统支持轻量级进程的数量是有限的。 用户线程实现使用用户线程实现的方式被称为1：N实现。广义上来讲，一个线程只要不是内核线程，都可以认为是用户线程（User Thread，UT）的一种，因此从这个定义上看，轻量级进程也属于用户线程，但轻量级进程的实现始终是建立在内核之上的，许多操作都要进行系统调用，因此效率会受到限制，并不具备通常意义上的用户线程的优点。 而狭义上的用户线程指的是完全建立在用户空间的线程库上，系统内核不能感知到用户线程的存在及如何实现的。用户线程的建立、同步、销毁和调度完全在用户态中完成，不需要内核的帮助。如果程序实现得当，这种线程不需要切换到内核态，因此操作可以是非常快速且低消耗的，也能够支持规模更大的线程数量，部分高性能数据库中的多线程就是由用户线程实现的。这种进程与用户线程之间1：N的关系称为一对多的线程模型 用户线程的优势在于不需要系统内核支援，劣势也在于没有系统内核的支援，所有的线程操作都需要由用户程序自己去处理。线程的创建、销毁、切换和调度都是用户必须考虑的问题，而且由于操作系统只把处理器资源分配到进程，那诸如“阻塞如何处理”“多处理器系统中如何将线程映射到其他处理器上”这类问题解决起来将会异常困难，甚至有些是不可能实现的。因为使用用户线程实现的程序通常都比较复杂，除了有明确的需求外（譬如以前在不支持多线程的操作系统中的多线程程序、需要支持大规模线程数量的应用），一般的应用程序都不倾向使用用户线程。Java、Ruby 等语言都曾经使用过用户线程，最终又都放弃了使用它。但是近年来许多新的、以高并发为卖点的编程语言又普遍支持了用户线程，譬如Golang、Erlang等，使得用户线程的使用率有所回升。 混合实现线程除了依赖内核线程实现和完全由用户程序自己实现之外，还有一种将内核线程与用户线程一起使用的实现方式，被称为N：M 实现。在这种混合实现下，既存在用户线程，也存在轻量级进程。 用户线程还是完全建立在用户空间中，因此用户线程的创建、切换、析构等操作依然廉价，并且可以 支持大规模的用户线程并发。而操作系统支持的轻量级进程则作为用户线程和内核线程之间的桥梁， 这样可以使用内核提供的线程调度功能及处理器映射，并且用户线程的系统调用要通过轻量级进程来完成，这大大降低了整个进程被完全阻塞的风险。在这种混合模式中，用户线程与轻量级进程的数量 比是不定的，是N：M 的关系。 许多UNIX系列的操作系统，如Solaris、HP-UX等都提供了M ：N的线程模型实现。在这些操作系 统上的应用也相对更容易应用M ：N的线程模型。 Java线程的实现以HotSpot为例，它的每一个Java线程都是直接映射到一个操作系统原生线程来实现的，而且中间没有额外的间接结构，所以HotSpot自己是不会去干涉线程调度的（可以设置线程优先级给操作系统提供调度建议），全权交给底下的操作系统去处理，所以何时冻结或唤醒线程、该给线程分配多少处理器执行时间、该把线程安排给哪个处理器核心去执行等，都是由操作系统完成的，也都是由操作系统全权决定的。","link":"/2024/09/01/java/jvm/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"title":"线程状态转换","text":"Java语言定义了6种线程状态，在任意一个时间点中，一个线程只能有且只有其中的一种状态，并 且可以通过特定的方法在不同状态之间转换。这6种状态分别是: 新建（New）：创建后尚未启动的线程处于这种状态。 运行（Runnable）：包括操作系统线程状态中的Running和Ready ，也就是处于此状态的线程有可能正在执行，也有可能正在等待着操作系统为它分配执行时间。 无限期等待（Waiting）：处于这种状态的线程不会被分配处理器执行时间，它们要等待被其他线 程显式唤醒。以下方法会让线程陷入无限期的等待状态： 没有设置Timeout参数的Object::wait()方法； 没有设置Timeout参数的Thread::join()方法； LockSupport::park()方法。 限期等待（Timed Waiting）：处于这种状态的线程也不会被分配处理器执行时间，不过无须等待 被其他线程显式唤醒，在一定时间之后它们会由系统自动唤醒。以下方法会让线程进入限期等待状 态： Thread::sleep ()方法； 设置了Timeout参数的Object::wait()方法； 设置了Timeout参数的Thread::join()方法； LockSup p ort::p arkNanos()方法； LockSup p ort::p arkUntil()方法。 阻塞（Blocked）：线程被阻塞了，“阻塞状态”与“等待状态”的区别是“阻塞状态”在等待着获取到 一个排它锁，这个事件将在另外一个线程放弃这个锁的时候发生；而“等待状态”则是在等待一段时 间，或者唤醒动作的发生。在程序等待进入同步区域的时候，线程将进入这种状态。 结束（Terminated）：已终止线程的线程状态，线程已经结束执行。","link":"/2024/09/01/java/jvm/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/%E7%BA%BF%E7%A8%8B%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2/"},{"title":"类加载器","text":"对于任意一个类，都必须由加载它的类加载器和这个类本身一起共同确立其在Java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类名称空间。这句话可以表达得更通俗一些：比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来源于同一个 Class文件，被同一个Java虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相等。这里所指的“相等”，包括代表类的Class对象的equals()方法、isAssignableFrom()方法、isInstance() 方法的返回结果，也包括了使用instanceof关键字做对象所属关系判定等各种情况。 双亲委派模型站在Java虚拟机的角度来看，只存在两种不同的类加载器： 一种是启动类加载器（Bootstrap ClassLoader），这个类加载器使用C++语言实现，是虚拟机自身的一部分； 一种是其他所有的类加载器，这些类加载器都由Java语言实现，独立存在于虚拟机外部，并且全都继承自抽象类 java.lang.ClassLoader。 站在Java开发人员的角度来看，类加载器就应当划分得更细致一些。Java一直保持着三层类加载器、双亲委派的类加载架构，尽管这套架构在Java模块化系统出现后有了一些调整变动，但依然未改变其主体结构。 启动类加载器（Bootstrap ClassLoader）：这个类加载器负责加载存放在 \\lib目录，或者被-Xbootclasspath参数所指定的路径中存放的，而且是Java虚拟机能够识别的（按照文件名识别，如rt.jar、tools.jar，名字不符合的类库即使放在lib目录中也不会被加载）类库加载到虚拟机的内存中。启动类加载器无法被Java程序直接引用，用户在编写自定义类加载器时， 如果需要把加载请求委派给引导类加载器去处理，那直接使用null代替即可。 ·扩展类加载器（Extension ClassLoader）：这个类加载器是在类sun.misc.Launcher$ExtClassLoader 中以Java代码的形式实现的。它负责加载\\lib\\ext目录中，或者被java.ext.dirs系统变量所指定的路径中所有的类库。根据“扩展类加载器”这个名称，就可以推断出这是一种Java系统类库的扩 展机制，JDK的开发团队允许用户将具有通用性的类库放置在ext目录里以扩展Java SE的功能，在JDK9之后，这种扩展机制被模块化带来的天然的扩展能力所取代。由于扩展类加载器是由Java代码实现的，开发者可以直接在程序中使用扩展类加载器来加载Class文件。 应用程序类加载器（Application ClassLoader）：这个类加载器由sun.misc.Launcher$AppClassLoader来实现。由于应用程序类加载器ClassLoader类中的getSystem-ClassLoader()方法的返回值，所以有些场合中也称它为“系统类加载器”。它负责加载用户类路径（ClassPath）上所有的类库，开发者同样可以直接在代码中使用这个类加载器。如果应用程序中没有 自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载 器。不过这里类加载器之间的父子关系一般不是以继承（Inheritance）的关系来实现的，而是通常使用组合（Composition）关系来复用父加载器的代码。 双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到最顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去完成加载。 使用双亲委派模型来组织类加载器之间的关系，一个显而易见的好处就是Java中的类随着它的类加载器一起具备了一种带有优先级的层次关系。例如类java.lang.Object，它存放在rt.jar之中，无论哪一个类加载器要加载这个类，最终都是委派给处于模型最顶端的启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都能够保证是同一个类。反之，如果没有使用双亲委派模型，都由各个类加载器自行去加载的话，如果用户自己也编写了一个名为java.lang.Object的类，并放在程序的ClassPath中，那系统中就会出现多个不同的Object类，Java类型体系中最基础的行为也就无从保证，应 用程序将会变得一片混乱。如果读者有兴趣的话，可以尝试去写一个与rt.jar类库中已有类重名的Java 类，将会发现它可以正常编译，但永远无法被加载运行。 123456789101112131415161718192021222324protected synchronized Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { // 首先，检查请求的类是否已经被加载过了 Class c = findLoadedClass(name); if (c == null) { try { if (parent != null) { c = parent.loadClass(name, false); } else { c =findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e){ // 如果父类加载器抛出ClassNotFoundException //说明父类加载器无法完成加载请求 } if(c ==null) { //在父类加载器无法加载时 //再调用本身的findClass方法来进行类加载 c =findClass(name); } } if (resolve) { resolveClass(c); } return c; }","link":"/2024/09/01/java/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/"},{"title":"类加载机制","text":"以个类型从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期将会经历加载 （Loading）、验证（Verification）、准备（Prep aration）、解析（Resolution）、初始化 （Initialization）、使用（Using）和卸载（Unloading）七个阶段，其中验证、准备、解析三个部分统称为连接（Linking） 《Java虚拟机规范》 则是严格规定了有且只有六种情况必须立即对类进行“初始化”（而加载、验证、准备自然需要在此之前开始）： 遇到new、getstatic、putstatic或invokestatic这四条字节码指令时，如果类型没有进行过初始 化，则需要先触发其初始化阶段。能够生成这四条指令的典型Java代码场景有： 使用new关键字实例化对象的时候。 读取或设置一个类型的静态字段（被final修饰、已在编译期把结果放入常量池的静态字段除外） 的时候。 调用一个类型的静态方法的时候。 使用java.lang.reflect包的方法对类型进行反射调用的时候，如果类型没有进行过初始化，则需 要先触发其初始化。 当初始化类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。 当使用JDK 7新加入的动态语言支持时，如果一个java.lang.invoke.M ethodHandle实例最后的解 析结果为REF_getStatic、REF_p utStatic、REF_invokeStatic、REF_newInvokeSp ecial四种类型的方法句 柄，并且这个方法句柄对应的类没有进行过初始化，则需要先触发其初始化。 当一个接口中定义了JDK 8新加入的默认方法（被default关键字修饰的接口方法）时，如果有 这个接口的实现类发生了初始化，那该接口要在其之前被初始化。 加载“加载”（Loading）阶段是整个“类加载”（Class Loading）过程中的一个阶段，在加载阶段，Java虚拟机需要完成以下三件事情： 通过一个类的全限定名来获取定义此类的二进制字节流 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入 口。 一个数组类（下面简称 为C）创建过程遵循以下规则： 如果数组的组件类型是引用类型，那就递归采用加载过程去加载这个组件类型，数组C将被标识在加载该组件类型的类加载器的类名称空间上； 如果数组的组件类型不是引用类型（例如int[]数组的组件类型为int），Java虚拟机将会把数组C标记为与引导类加载器关联。 数组类的可访问性与它的组件类型的可访问性一致，如果组件类型不是引用类型，它的数组类的可访问性将默认为public，可被所有的类和接口访问到。 加载阶段与连接阶段的部分动作（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的一部分，这两个阶段的开始时间仍然保持着固定的先后顺序 验证这一阶段的目的是确保Class文件的字节流中包含的信息符合《Java虚拟机规范》的全部约束要求，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。 验证阶段大致上会完成下面四个阶段的检验动作：文件格式验证、元数据验证、字节码验证和符号引用验证。 准备准备阶段是正式为类中定义的变量（即静态变量，被static修饰的变量）分配内存并设置类变量初始值的阶段。 进行内存分配的仅包括类变量，而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在Java堆中。这里所说的初始值“通常情况”下是数据类型的零值，假设一个类变量的定义为： 1public static int value = 123; 那变量value在准备阶段过后的初始值为0而不是123，因为这时尚未开始执行任何Java方法，而把 value赋值为123的putstatic指令是程序被编译后，存放于类构造器()方法之中，所以把value赋值为123的动作要到类的初始化阶段才会被执行。 如果类字段 的字段属性表中存在ConstantValue属性，那在准备阶段变量值就会被初始化为ConstantValue属性所指定 的初始值，假设上面类变量value的定义修改为： 1public static final int value = 123; 编译时Javac将会为value生成ConstantValue属性，在准备阶段虚拟机就会根据Con-stantValue的设置 将value赋值为123。 解析解析阶段是Java虚拟机将常量池内的符号引用替换为直接引用的过程。 符号引用（Sy mbolic References）：符号引用以一组符号来描述所引用的目标，符号可以是任何 形式的字面量，只要使用时能无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定是已经加载到虚拟机内存当中的内容。各种虚拟机实现的内存布局可以各不相同， 但是它们能接受的符号引用必须都是一致的，因为符号引用的字面量形式明确定义在《Java虚拟机规范》的Class文件格式中。 直接引用（Direct References）：直接引用是可以直接指向目标的指针、相对偏移量或者是一个能 间接定位到目标的句柄。直接引用是和虚拟机实现的内存布局直接相关的，同一个符号引用在不同虚 拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那引用的目标必定已经在虚拟机 的内存中存在。 类加载过程中的解析阶段是Java虚拟机将常量池内的符号引用转换为直接引用的过程。以下是解析阶段的主要步骤 符号引用验证：首先，虚拟机会检查符号引用是否能定位到方法区中的类或接口。如果找不到，则抛出NoClassDefFoundError错误。 解析类和接口：对于每个符号引用，虚拟机会尝试加载对应的类或接口。如果类或接口尚未加载，则会触发其加载、链接（包括准备、验证、解析）过程。 解析字段：解析类或接口中的字段，确定字段在运行时常量池中的位置。如果字段不存在，则抛出NoSuchFieldError错误。 解析类方法：解析类的方法，确定方法在运行时常量池中的位置。如果方法不存在，则抛出NoSuchMethodError错误。 解析接口方法：解析接口的方法，确定方法在运行时常量池中的位置。如果方法不存在，则抛出NoSuchMethodError错误。 转换为直接引用：一旦符号引用被成功解析，虚拟机会将其转换为直接引用，即指向方法区中数据的具体位置。 解析阶段确保了程序中使用的类、接口、字段和方法在运行时能够正确访问。这是类加载机制中非常关键的一环，保证了Java程序的动态性和安全性。 初始化类的初始化阶段是类加载过程的最后一个步骤，之前介绍的几个类加载的动作里，除了在加载阶段用户应用程序可以通过自定义类加载器的方式局部参与外，其余动作都完全由Java虚拟机来主导控制。直到初始化阶段，Java虚拟机才真正开始执行类中编写的Java程序代码，将主导权移交给应用程序。 初始化阶段就是执行类构造器clinit()方法的过程， clinit()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static{}块）中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问。 clinit()方法与类的构造函数（即在虚拟机视角中的实例构造器init()方法）不同，它不需要显式地调用父类构造器，Java虚拟机会保证在子类的clinit()方法执行前，父类的clinit()方法已经执行完毕。因此在Java虚拟机中第一个被执行的clinit()方法的类型肯定是java.lang.Object。 由于父类的clinit()方法先执行，也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作 clinit()方法对于类或接口来说并不是必需的，如果一个类中没有静态语句块，也没有对变量的 赋值操作，那么编译器可以不为这个类生成clinit()方法。 接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会生成 clinit()方法。但接口与类不同的是，执行接口的clinit()方法不需要先执行父接口的clinit()方法， 因为只有当父接口中定义的变量被使用时，父接口才会被初始化。此外，接口的实现类在初始化时也一样不会执行接口的clinit()方法。 Java虚拟机必须保证一个类的clini&gt;()方法在多线程环境中被正确地加锁同步，如果多个线程同时去初始化一个类，那么只会有其中一个线程去执行这个类的clinit()方法，其他线程都需要阻塞等待，直到活动线程执行完毕clinit&gt;)方法。如果在一个类的clinit()方法中有耗时很长的操作，那就可能造成多个进程阻塞。","link":"/2024/09/01/java/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/"},{"title":"Tomcat：正统的类加载器架构","text":"一个功能健全的Web服务器，都要解决 如下的这些问题： 部署在同一个服务器上的两个Web应用程序所使用的Java类库可以实现相互隔离。这是最基本的需求，两个不同的应用程序可能会依赖同一个第三方类库的不同版本，不能要求每个类库在一个服务器中只能有一份，服务器应当能够保证两个独立应用程序的类库可以互相独立使用。 部署在同一个服务器上的两个Web应用程序所使用的Java类库可以互相共享。这个需求与前面一点正好相反，但是也很常见，例如用户可能有10个使用Spring组织的应用程序部署在同一台服务器上，如果把10份Spring分别存放在各个应用程序的隔离目录中，将会是很大的资源浪费——这主要倒不是浪费磁盘空间的问题，而是指类库在使用时都要被加载到服务器内存，如果类库不能共享，虚拟机的方法区就会很容易出现过度膨胀的风险 在Tomcat目录结构中，可以设置3组目录（/common/*、/server/*和/shared/*，但默认不一定是开放的，可能只有/lib/*目录存在）用于存放Java类库，另外还应该加上Web应用程序自身的“/WEB- INF/*”目录，一共4组。把Java类库放置在这4组目录中，每一组都有独立的含义，分别是： 放置在/common目录中。类库可被Tomcat和所有的Web应用程序共同使用。 放置在/server目录中。类库可被Tomcat使用，对所有的Web应用程序都不可见。 放置在/shared目录中。类库可被所有的Web应用程序共同使用，但对Tomcat自己不可见。 放置在/WebApp/WEB-INF目录中。类库仅仅可以被该Web应用程序使用，对Tomcat和其他Web应用程序都不可见. 为了支持这套目录结构，并对目录里面的类库进行加载和隔离，Tomcat自定义了多个类加载器， 这些类加载器按照经典的双亲委派模型来实现。","link":"/2024/09/01/java/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD/Tomcat%EF%BC%9A%E6%AD%A3%E7%BB%9F%E7%9A%84%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%E6%9E%B6%E6%9E%84/"},{"title":"容错机制","text":"容错机制 1. 熔断（Circuit Breaker）概念当依赖的服务出现严重故障（如响应超时、异常率过高）时，暂时切断对该服务的调用，避免级联故障，快速返回降级结果。类似于电路中的保险丝，过载时自动断开。 核心逻辑 状态机：通常包含三种状态： 关闭（Closed）：正常调用依赖服务。 打开（Open）：熔断生效，直接返回降级结果。 半开（Half-Open）：试探性地恢复调用，若成功则关闭熔断，否则重新打开。 应用场景 依赖服务不可用时，防止级联雪崩。 快速失败，避免资源浪费（如线程池被长时间占用）。 示例（Python 伪代码）123456789101112131415161718192021222324252627282930class CircuitBreaker: def __init__(self, max_failures=3, reset_timeout=10): self.max_failures = max_failures # 最大失败次数 self.reset_timeout = reset_timeout # 重试等待时间 self.failures = 0 self.last_failure_time = 0 self.state = &quot;closed&quot; # 初始状态：关闭 def call(self, service_method): if self.state == &quot;open&quot;: # 检查是否过了重试时间 if time.time() - self.last_failure_time &gt; self.reset_timeout: self.state = &quot;half-open&quot; else: return self.fallback() # 直接返回降级结果 try: result = service_method() self.failures = 0 self.state = &quot;closed&quot; return result except Exception: self.failures += 1 if self.failures &gt;= self.max_failures: self.state = &quot;open&quot; self.last_failure_time = time.time() return self.fallback() def fallback(self): return &quot;降级结果&quot; # 如缓存数据、默认值等 2. 限流（Rate Limiting）概念限制系统的输入或输出流量，使系统处理的请求量不超过其最大容量，防止因过载而崩溃。类似于水管的流量控制阀门。 常见算法 固定窗口（Fixed Window）：统计单位时间内的请求数，超过阈值则拒绝。 滑动窗口（Sliding Window）：将时间窗口细分，更精确地控制流量。 令牌桶（Token Bucket）：按固定速率生成令牌，请求需获取令牌才能被处理。 漏桶（Leaky Bucket）：请求以固定速率被处理，多余请求被丢弃。 应用场景 保护核心服务不被突发流量压垮。 控制第三方接口调用频率（如API限流）。 示例（Python 令牌桶算法）1234567891011121314151617181920212223import timeclass TokenBucket: def __init__(self, capacity, rate): self.capacity = capacity # 令牌桶容量 self.rate = rate # 令牌生成速率（个/秒） self.tokens = capacity # 初始令牌数 self.last_refill_time = time.time() # 上次填充时间 def allow_request(self): # 计算从上次填充到现在应生成的令牌数 now = time.time() delta = now - self.last_refill_time new_tokens = delta * self.rate self.tokens = min(self.capacity, self.tokens + new_tokens) self.last_refill_time = now # 判断是否有足够的令牌 if self.tokens &gt;= 1: self.tokens -= 1 return True else: return False # 限流，拒绝请求 3. 降级（Degradation）概念当系统负载过高或依赖服务故障时，主动牺牲部分非核心功能或服务质量，保证核心功能可用。类似于飞机降落时丢弃副油箱。 降级策略 自动降级：根据预设规则（如响应时间、错误率）自动触发。 人工降级：运维人员手动干预（如双十一前降级非核心业务）。 数据降级：返回缓存数据、简化数据或默认值。 应用场景 高并发时，放弃次要业务（如评论、推荐）。 依赖服务故障时，提供兜底方案（如返回静态页面）。 示例（电商系统降级）1234567891011121314def get_product_info(product_id): try: # 核心逻辑：查询数据库 return db.query(product_id) except Exception as e: logger.error(f&quot;数据库查询失败: {e}&quot;) # 降级策略：返回缓存数据或默认值 return cache.get(product_id) or {&quot;name&quot;: &quot;默认商品&quot;, &quot;price&quot;: 0}def submit_order(request): if system_load &gt; THRESHOLD: # 系统负载过高 # 降级：拒绝非核心业务（如积分、优惠券） request.discard_non_essential_features() return process_order(request) 4. 三者对比 机制 核心目标 触发条件 实现方式 示例场景 熔断 防止级联故障，快速失败 依赖服务不可用（超时、异常） 状态机（关闭→打开→半开） 微服务调用失败 限流 防止系统过载 流量超过预设阈值 令牌桶、漏桶等算法 API网关限流 降级 保证核心功能，牺牲非核心功能 系统负载过高或资源不足 拒绝部分请求、返回简化结果 电商大促时关闭评论功能 5. 组合使用三者常结合使用形成多级防御： 限流：作为第一道防线，防止系统被过量请求压垮。 熔断：作为第二道防线，切断不可用服务的调用。 降级：作为最终手段，牺牲非核心功能保核心。 典型案例：电商秒杀系统 限流：限制每秒请求数为系统能处理的上限。 熔断：若库存服务故障，直接返回”已售罄”。 降级：关闭评论、推荐等非核心功能，集中资源处理订单。 6. 工具与框架 熔断：Hystrix（Java）、Sentinel（Java）、Pybreaker（Python）。 限流：Guava RateLimiter（Java）、Redis + Lua（分布式）。 降级：Sentinel、Resilience4j（Java）。 通过合理配置这三种机制，可显著提升分布式系统的稳定性和可用性。","link":"/2025/04/26/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/"},{"title":"InnoDB 体系架构","text":"InnoDB 体系架构 💓 后台线程 InnDB存储引擎是多线程模型，不同的线程处理不同的任务。根据任务的不同可以分为以下几种线程 Master Thread ：Master 线程是一个核心后台线程，主要负责将缓冲池中的数据异步刷新到磁盘，保证数据一致性 IO Thread ： InnoDB存储引擎中大量使用了 AIO 来处理 IO 请求，主要负责 IO 请求的回调处理， Purge（清除） Thread ：事务提交后，其所使用的 undo log 就不在需要了，因此需要 Purge Thread 回收已经使用并分配 undo 页 Page Cleaner Thread ： 将之前版本中脏页的刷新操作都放入到单独的线程来完成，为了减轻 Master Thread 的工作对于用户查询线程的阻塞。 💓内存 💓缓冲池 InnoDB存储引擎是基于磁盘存储的，由于CPU速度与磁盘速度之间的鸿沟，基于磁盘的数据库系统通常使用缓冲池技术来提高数据库的整体性能。 缓冲池简单来说就是一块内存区域，在数据库中进行读取页的操作，通过内存的速度来弥补磁盘速度较慢满对数据库性能的影响。首先在数据中进行读取页的操作，先将磁盘督导的页存放在缓冲池中，这个过程称为将页 “FIX” 在缓冲池中。 下一次再度相同的页时，首先判断该页是否在缓冲池中，若在缓冲池中，直接读取该页。否则，读取磁盘上的页。 对于数据库中页的修改操作，首先修改在缓冲池中的页，然后在以一定的频率刷新搭配磁盘上，页从缓冲池刷新会磁盘的操作并不是在每次页发生更新时触发。而是通过 ** checkpoint** 的机制刷新回磁盘 缓存池中的数据页类型： 索引页 数据也 undo页 插入缓冲页 自适应哈希索引 InnoDB存储的锁信息 数据字典信息 💓缓冲池管理数据库的缓冲池时通过 LRU （Latest Recent Used ， 最近最少使用）算法来进行管理的， 与 LRA 不同的是 LRU列表中还加入了midPoint 位置，新读取到的页，虽然是最新访问的页，但并不是直接放入到LRU列表的首部，而是放入到LRU列表的 midPoint 位置。在默认配置之下，该位置在LRU列表长度的 5/8 处。midpoint 之后的列表称为 old 列表 ， 之前的列表称为 new 列表 。可以理解为 new 列表中的页是最为活跃的热点数据 。** 如果将直接读取的页放入到LRU列表的首部，那么某些SQL操作可能会使缓冲中的页被刷新出，从而影响缓冲池的效率**。例如索引或数据的草庙操作。这类操作需要访问表中的许多也，甚至是全部的页，而这些页通常是这次查询工作中需要，并不是活跃的热点数据，如果页被放入LRU列表的首部，那么非常可能将所需要的热点数据也从LRU列表中移除。 为了解决上图中的问题，InnoDB 存储引入了 innodb_old_blocks_time, 用于表示页读取到mid位置后需要等待多久才会被加入到LRU列表的热端。 从 InnoDB 1.2 版本开始，可以通过表 INNODB_BUFFER_POOL_STATS 观察缓冲池的运行状态, 还可以通过 INNODB_BUFFER_PAGE_LRU 来观察每个列表中每个页的具体信息。 从 InnoDB 1.0.x 开始 支持压缩页的功能，即将原本 16KB 的页压缩未1KB 2kb 4kb 8kb 。 由于也得大小发生了变化，LRU 列表也有了些许改变， 通过 unzip_LRU 列表进行管理。 💓重做日志缓冲 （redo log buffer） InnoDB 存储引擎首先将重做日志信息放入到这个缓冲区，然后按照一定频率将其刷新到重做日志问价。一般情况下每一秒会将重做日志缓冲刷新到日志文件， 因此用户只需要保证每秒产生的事务量在这个缓冲大小之内即可 。 可以使用配置参数innodb_log_buffer_size 控制，默认为 8 MB重做日志缓冲中的内容刷新到外部磁盘的重做日志文件中 Master Thread 每一秒将重做日志缓冲刷新到重做日志文件 每个事务提交时会将重做日志缓冲刷新到重做日志文件 当重做日志缓冲池剩余空间小于 1/2 时，重做日志缓冲刷新到重做日志文件 💓额外的内存池 在 InnoDB 存储引擎中，最内存的管理是通过一种称为内存堆的方式进项的，在对一些数据结构本身的内存进行分配时，需要从额外的内存池中进行申请，当该区域的内存不够时，会从缓冲池中进行申请。例如 分配了缓冲池（innodb_buffer_pool）， 但是每个缓冲池中的帧缓冲（frame buffer）还有对应的缓冲控制对象 （buffer controrl block）， 这些对象记录了一下 LRU、锁、等待等信息，而这个对象的内存需要从额外的内存池中申请","link":"/2025/05/26/mysql/innodb/InnoDB%20%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/"},{"title":"InnoDB 的索引结构","text":"Mysql高级特性】 InnoDB 的索引结构 💓 索引innoDB中支持的索引类型 B+树索引 全文索引 hash索引 💓B+树索引B+树是由B树和索引顺序访问演化而来。B+树是为磁盘或者直接存取辅助设备设计的一种平衡查找树。在B+树中，索引记录节点都是按键值的大小顺序存放在哦那个一侧的叶子节点上，由个叶子节点指针进行连接。B+相比于B树b+树的中间节点不保存数据，所以磁盘页能容纳更多节点元素，更“矮胖”，并且叶子节点存在双向指针可以更好的全文查找在数据库中，B+树的高度一般都在2——4层，也就是说查找某一键值的行记录最多只需要2到4次I/O。 💓聚集索引聚集索引就是按照每张便的主键狗仔一颗B+树，同时叶子节点中存放的即为整张表的行记录数据，聚集索引决定了索引组织表中的数据也是索引的一部分。同时B+树数据结构一样，每个数据页都是通过一个双向链表来进行链接，因此每张表都只能有一个聚集索引 💓辅助索引辅助索引，叶子节点不包含行记录的全部数据。叶子节点除了包含键值意外，每个叶子节点中的所银行中害包含了一个书签。这个书签就是相应数据行的聚集索引键辅助索引的存在并不影响数据在聚集索引中的组织，一次每张表可以有多个辅助索引。当通过辅助索引来查找数据是，InnoDB引擎会便利辅助索引并沟通过叶级别的指针获得只想主键索引的主键，然后再通过主键主键索引来找到一个完整的行记录，这个操作就叫回表 💓联合索引联合索引是指对表上的多列进行索引。本质上说联合索引也是一颗B+树，只不过是按照多列的优先级进行排序联合索引的好处是已经对第二个键值进行了排序处理。 💓覆盖索引覆盖索引，即从辅助索引中就可以得到查询的记录，从而不需要回表查询聚集索引中的记录。使用覆盖索引的好吃是辅助索引不包含整行的所有信息，故其大小要远小于聚集索引，因此可以减少大量的io Multi-Range Read 优化 （MRR）mysql 5.6 版本支持 Multi-Range Read 优化简单来说，就是将随机访问转化为顺序的数据访问，MRR优化可用于 rang、ref、eq_ref类型MRR优化的好处 是数据访问变得较为顺序。在查找辅助索引时，首先根据得到的查询结果，按照主键进行排序，并按照主键排序进行回表查找 减少缓冲池中页被替换的次数 批量处理对键值的查询操作 Index Condition Pubshdown 优化 （ICP）当进行索引查询时，首先根据索引来查询记录，让后再根据Where条件过滤记录，在支持ICP后，Mysql数据库会在取出索引的同时，判断是否可以使用WHERE条件过滤，也就是将WHERE的部分过滤操作放在了存储引擎层，在某些查询下可以大大减少上层SQL层对记录的索取，从而提高数据的整体性能。","link":"/2025/05/26/mysql/innodb/Mysql%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E3%80%91%20InnoDB%20%E7%9A%84%E7%B4%A2%E5%BC%95%E7%BB%93%E6%9E%84/"},{"title":"InnoDB 的逻辑存储结构","text":"Mysql高级特性】 InnoDB 的逻辑存储结构 # 💓 索引组织表 索引组织表就是**表都是根据主键顺序组织存放的**，在 InnoDB 存储引擎表中，每张表都有一个主键 * 当表中存在非空的唯一索引，则该列为主键 * 没指定主键时，InnoDB 存储引擎自动创建一个6字节大小的指针。 💓 InnoDB 的逻辑存储结构所有数据都存放在表空间中，表空间又由 段、区、页组成。 💓 表空间表空间可以看作是InnoDB存储引擎逻辑结构的最高层，所有数据都存放在表空间中默认情况下 InnoDB 存储引擎有一个共享表空间 ibdata1 ， 所有的数据都存储到这个表空间中。如果开启了参数 innodb_file_per_table，则每张表内的数据可以单独放到一个表空间中. 💓 段表空间是由各个段组成的，常见的有数据段、回滚段、索引段等。数据段即为 B+ 树的叶子节点，索引段即为B+树的非索引节点 💓 区区是连续的页组成的空间，在任何情况下每个区的大小都为1mb，为了保证区中页的连续性，InnoDB 存储引擎此役从磁盘申请4到5个区，默认情况下 ，InnoDB村粗引擎的页大小为16kb，即一个区共有64个连续的页。如果是压缩页的话每个区的数量也对应增加 💓 页页是 InnoDB 磁盘管理的最小单位。在InnoDB 存储引擎中，默认每个页的大小为16KB。而从 InnoDB 1.2.x版本来时。可以通过参数innodb_page_size将页的大小设置为4k、8k、16k。常见的页类型 数据页 （B-tree Node） undo 页 系统页 事务数据页 插入缓冲位图页 插入缓冲空闲列表页 为压缩的二进制对象页 压缩的二进制对象页 数据页的结构 💓 行InnoDB 数据是按行进行存放的，每个页存放的行记录也是有硬性定义的 ， 最多允许存放 16kb /2 - 200 行的记录，及 7992行记录。 💓 Compact 行记录存储格式 变长字段长度列表：大小不可以超过两个字节， 因 VARCHAR 类型的最大长度限制为65530 NULL 标志位：指示行数据中是否有NULL 值 记录头信息： 固定占用5字节 列数据 ：列中的NULL 不占用该部分任何空间。包括两个隐藏列，事务id、回滚指针 若列中没有定义主键，每行还会增加一个6字节的rowid列","link":"/2025/05/26/mysql/innodb/Mysql%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E3%80%91%20InnoDB%20%E7%9A%84%E9%80%BB%E8%BE%91%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84%20/"},{"title":"什么是MVCC","text":"全称Multi-Version Concurrency Control，即多版本并发控制，主要是为了提高数据库的并发性能。以下文章都是围绕InnoDB引擎来讲，因为myIsam不支持事务。 同一行数据平时发生读写请求时，会上锁阻塞住。但mvcc用更好的方式去处理读—写请求，做到在发生读—写请求冲突时不用加锁。 这个读是指的快照读，而不是当前读，当前读是一种加锁操作，是悲观锁。 那它到底是怎么做到读—写不用加锁的，快照读和当前读又是什么鬼，跟着你们的贴心老哥，继续往下看。 当前读、快照读都是什么鬼什么是MySQL InnoDB下的当前读和快照读？ 当前读它读取的数据库记录，都是当前最新的版本，会对当前读取的数据进行加锁，防止其他事务修改数据。是悲观锁的一种操作。 如下操作都是当前读： select lock in share mode (共享锁) select for update (排他锁) update (排他锁) insert (排他锁) delete (排他锁) 串行化事务隔离级别 快照读快照读的实现是基于多版本并发控制，即MVCC，既然是多版本，那么快照读读到的数据不一定是当前最新的数据，有可能是之前历史版本的数据。 如下操作是快照读： 不加锁的select操作（注：事务级别不是串行化） 数据库并发场景 读-读：不存在任何问题，也不需要并发控制 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读 写-写：有线程安全问题，可能会存在更新丢失问题，比如第一类更新丢失，第二类更新丢失 MVCC解决并发哪些问题？mvcc用来解决读—写冲突的无锁并发控制，就是为事务分配单向增长的时间戳。为每个数据修改保存一个版本，版本与事务时间戳相关联。 读操作只读取该事务开始前的数据库快照。 解决问题如下： 并发读-写时：可以做到读操作不阻塞写操作，同时写操作也不会阻塞读操作。 解决脏读、幻读、不可重复读等事务隔离问题，但不能解决上面的写-写 更新丢失问题。 因此有了下面提高并发性能的组合拳： MVCC + 悲观锁：MVCC解决读写冲突，悲观锁解决写写冲突 MVCC + 乐观锁：MVCC解决读写冲突，乐观锁解决写写冲突 MVCC的实现原理它的实现原理主要是版本链，undo日志 ，Read View 来实现的 版本链我们数据库中的每行数据，除了我们肉眼看见的数据，还有几个隐藏字段，得开天眼才能看到。分别是db_trx_id、db_roll_pointer、db_row_id。 db_trx_id 事务id 6byte，最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID。 db_roll_pointer（版本链关键） 回滚指针 7byte，回滚指针，指向这条记录的上一个版本（存储于rollback segment里） db_row_id 隐藏主键 6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以db_row_id产生一个聚簇索引。 实际还有一个删除flag隐藏字段, 记录被更新或删除并不代表真的删除，而是删除flag变了 如上图，db_row_id是数据库默认为该行记录生成的唯一隐式主键，db_trx_id是当前操作该记录的事务ID，而db_roll_pointer是一个回滚指针，用于配合undo日志，指向上一个旧版本。 每次对数据库记录进行改动，都会记录一条undo日志，每条undo日志也都有一个roll_pointer属性（INSERT操作对应的undo日志没有该属性，因为该记录并没有更早的版本），可以将这些undo日志都连起来，串成一个链表，所以现在的情况就像下图一样： 对该记录每次更新后，都会将旧值放到一条undo日志中，就算是该记录的一个旧版本，随着更新次数的增多，所有的版本都会被roll_pointer属性连接成一个链表，我们把这个链表称之为版本链，版本链的头节点就是当前记录最新的值。另外，每个版本中还包含生成该版本时对应的事务id，这个信息很重要，在根据ReadView判断版本可见性的时候会用到。 undo日志Undo log 主要用于记录数据被修改之前的日志，在表信息修改之前先会把数据拷贝到undo log里。 当事务进行回滚时可以通过undo log 里的日志进行数据还原。 Undo log 的用途保证事务进行rollback时的原子性和一致性，当事务进行回滚的时候可以用undo log的数据进行恢复。 用于MVCC快照读的数据，在MVCC多版本控制中，通过读取undo log的历史版本数据可以实现不同事务版本号都拥有自己独立的快照数据版本。 undo log主要分为两种： insert undo log 代表事务在insert新记录时产生的undo log , 只在事务回滚时需要，并且在事务提交后可以被立即丢弃 update undo log（主要） 事务在进行update或delete时产生的undo log ; 不仅在事务回滚时需要，在快照读时也需要； 所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除 Read View(读视图)事务进行快照读操作的时候生产的读视图(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照。 记录并维护系统当前活跃事务的ID(没有commit，当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以越新的事务，ID值越大)，是系统中当前不应该被本事务看到的其他事务id列表。 Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。 Read View几个属性 trx_ids: 当前系统活跃(未提交)事务版本号集合。 low_limit_id: 创建当前read view 时“当前系统最大事务版本号+1”。 up_limit_id: 创建当前read view 时“系统正处于活跃事务最小版本号” creator_trx_id: 创建当前read view的事务版本号； Read View可见性判断条件 db_trx_id &lt; up_limit_id || db_trx_id == creator_trx_id（显示） 如果数据事务ID小于read view中的最小活跃事务ID，则可以肯定该数据是在当前事务启之前就已经存在了的,所以可以显示。 或者数据的事务ID等于creator_trx_id ，那么说明这个数据就是当前事务自己生成的，自己生成的数据自己当然能看见，所以这种情况下此数据也是可以显示的。 db_trx_id &gt;= low_limit_id（不显示） 如果数据事务ID大于read view 中的当前系统的最大事务ID，则说明该数据是在当前read view 创建之后才产生的，所以数据不显示。如果小于则进入下一个判断 db_trx_id是否在活跃事务（trx_ids）中 不存在：则说明read view产生的时候事务已经commit了，这种情况数据则可以显示。 已存在：则代表我Read View生成时刻，你这个事务还在活跃，还没有Commit，你修改的数据，我当前事务也是看不见的。 MVCC和事务隔离级别上面所讲的Read View用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。 RR、RC生成时机 RC隔离级别下，是每个快照读都会生成并获取最新的Read View； 在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View，之后的查询就不会重复生成了，所以一个事务的查询结果每次都是一样的。 解决幻读问题 快照读：通过MVCC来进行控制的，不用加锁。按照MVCC中规定的“语法”进行增删改查等操作，以避免幻读。 当前读：通过next-key锁（行锁+gap锁）来解决问题的。 RC、RR级别下的InnoDB快照读区别在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照及Read View， 将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View，所以对之后的修改不可见； 即RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见 而在RC级别下的，事务中，每次快照读都会新生成一个快照和Read View, 这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因","link":"/2025/05/26/mysql/%E9%94%81%E4%B8%8E%E5%B9%B6%E5%8F%91/%E4%BB%80%E4%B9%88%E6%98%AFMVCC/"},{"title":"InnoDB Checkpoint概述","text":"💓 Checkpoint 技术背景 InnoDB 对于数据的 update 和 delete 改变了 缓存池中页的记录，那么此时缓存和页的数据比磁盘的数据要新， 数据库需要将新版本的页从缓存池刷新到磁盘中。如果说每做一次更新和删除操作，就把缓存中页的数据刷新到磁盘，那么开销非常大，而且如果把缓存池中页的新版本杀心到磁盘是发生了宕机，那么数据就不能恢复了。为了避免这种情况发生，**当前事务数据库都采用了 Wrte Ahead Log 策略，即当事务提交时，先写重做日志，再修改页。当数据库宕机时，可以通过重做日志来完成数据的恢复。** 💓Checkpoint 的职责与类型 Checkpoint 的职责 将缓存池中的脏页刷回磁盘 重点在于，每次刷新多少也到磁盘，每次从哪里取脏页，以及什么时候触发 Checkpoint Sharp Checkpoint 发生在数据库关闭时将所有脏页都刷回磁盘 Fuzzy Checkpoint 之刷新一部分脏页，而不是刷新所有脏页回磁盘 Master Thread Checkpoint：每秒或者10秒的频率异步刷新缓冲池的脏页到磁盘。（由Page Cleaner Thread完成） FLUSH_LRU_LIST Checkpoint：缓冲池不够用时，根据LRU算法会淘汰掉最近最少使用的页，如果该页是脏页的话，会强制执行CheckPoint，将该脏页刷回磁盘（由Page Cleaner Thread完成）； Async/Sync Flush Checkpoint：重做日志不可用的情况，需要强制从脏页列表中选取一些脏页刷新磁盘到缓存（由Page Cleaner Thread完成）。 💓Checkpoint 主要解决的问题 缩短数据库的恢复时间 缓冲池不够用将脏页刷新到磁盘 重做日志不可用时，刷新脏页 当数据库宕机时 ， 数据库不需要重做所有的日志，因为CheckPoint 之前的页已经刷回磁盘，只需对checkpoint后的重做日志进行恢复。这样就大大缩短了恢复时间。 对于 重做日志来说，没有必要让他无限增大，重做日志的设计都是循环使用的，如果数据库恢复不需要这部分日志，那么他就是可以被覆盖重用。若此时当前重做日志还需要被使用，不可以被覆盖，那么必须强制产生 Checkpoint，将缓冲池中的页至少刷新到当前重做日志的位置。","link":"/2025/05/26/mysql/innodb/%E3%80%90Mysql%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E3%80%91InnoDB%20Checkpoint%E6%A6%82%E8%BF%B0/"},{"title":"InnoDB 的关键特性","text":"💓 InnoDB 的关键特性 InnoDB 存储引擎关键特性包括： 插入缓冲 （insert Buffer） 两次写 （Double Whrite） 自适应哈希索引 （Adaptice Hash Index） 异步IO （Async IO） 刷新邻接页 （Flush Neighbor Page） 💓插入缓冲 （insert Buffer） 在 InnoDB 存储引擎中，主键时行唯一的标识符。通常应用程序中行记录的插入顺序按照主键递增的顺序进行插入的。因此，插入聚集索引一般是顺序的，但是不可能在每张表上只有一个聚集索引，大多数，一张表上有多个非聚集的辅助索引。这种情况下产生了一个非聚集的列不是唯一的索引。在进行插入操作时，对于非聚集索引叶子节点的插入不再是顺序的，这是需要离散式访问非聚集索引页，由于搜集读取存在而导致插入的操作性能下降。** 对于非聚集索引插入和更新操作，不是每一次一次直接插入到索引页中，而是先判断插入的非聚集索引页是否在缓冲池中，若在，则直接插入；若不在，则先放入到一个 insert Buffer 对象中，然后在以一定的频率和情况进行 insert Buffer 和辅助索引叶子节点 merge 操作，通常能将多个插入合并到一个操作中，从而提高了对于非聚集索引插入的性能。 InnoDB 缓冲池确实包含了Insert Buffer的信息，但Insert Buffer 和数据页一样，也是物理存在的（以B+树的形式存在共享表空间中）。使用 insert Buffer 的条件： 索引是辅助索引 ： 主键是唯一索引 索引不是唯一的 ： 如果索引是唯一的，需要对索引的唯一性进行判断，那么就避免不了查找索引页。 insert Buffer存在的问题，在密集写的情况加，插入缓存占用过多的缓冲池内存，默认可以最大占用1/2的缓冲池内存。如果数据库宕机，必有大量的 insert Buffer 并没有合并到实际的非聚集索引中，因此恢复需要很长的时间 💓 change BufferInnoDB 从1.0.x版本引入了 changge Buffer ，可将其视为 Insert Buffer 的升级，增加了对 insert、 delete、 update都进行了缓冲，change Buffer适用的对象依然是 非唯一的辅助索引 insert buffer ： 先写 insert buffer 之后再 merge到辅助索引的叶子节点 detete buffer ： 先将记录标记为已删除， purge buffer ： 真正将记录删除 insert 对应 insert bufferupdate 和 delete 对应 detete buffer 和 purge buffer 因为这里只是对索引树和索引列的维护操作，对于一个索引值没有更新操作只有删除 💓 insert Buffer 数据结构insert Buffer 的数据结构是一颗B+树，在 mysql 4.1之后 insert Buffer 由一颗B+数构成。负责对所有表的辅助索引进行 insert Buffer 。 这个B+树存放在共享表了空间中。 💓 Merge insert BufferMerge insert Buffer 就是将 Insert Buffer 中的记录合并到真正的辅助索引中在下面几种情况下发生 辅助索引页呗读取到缓冲池中 当辅助索引页呗读取到缓冲池中时，如果辅助所有也又记录存在于 insert Buffer B+书中，则将该页的记录插入到该辅助索引页中 Insert Buffer Bitmap 页追踪到该辅助索引页一无可用空间时 若插入辅助索引记录时检测到插入记录后可用空间会小于1/32页，则会强制进行一个合并操作 Master Thread 刷新 每秒或者每10秒进行一次 Merge insert Buffer 操作 💓两次写 （Double Write） insert Buffer 带给 InnoDB 存储引擎时性能上的提升， 那么 doublewrite 带给 InnoDB 的时数据也的可靠性。 数据库放生宕机时，可能innoDB 存储引擎正在是写入某个页到表中，而这个页写了一部分，比如16KB的页 ，只写了前 4 KB ，之后就发生了宕机，这种情况被称为部分写失效 （partial page write）。 重做日志中记录的时对页的物理操作。如果这个页本身已经发生了损坏，在对其重做时没有意义的。Double Write： 在应用重做日志之前，用户需要一个页得副本，当写入失效发生时，先通过页的副本来还原该页，再进行重做。 💓Double Write 组成 doublewrite buffer 大小为 2MB 内存 共享表空间 大小为 2MB 磁盘在对缓冲池进行刷新时，并不直接写磁盘，而是会员通过memcpy函数将脏页先复制到内存中的 doublewrite buffer，之后通过 doublewrite buffer 再分两次，每次1MB 顺序低写入共享表空间的物理磁盘上，然后马上调用fsync 函数，同步输盘，避免缓冲来的问题 💓自适应哈希索引 （Adaptive Hash Index，AHI）哈希查找的一般情况下的时间复杂度为 0（1），而 B+ 树的查找次数，取决于B+树的高度，在生产环境B+树的高度一般为3到4层，需要3到4此查询。 自适应哈希索引 ： InnoDB 存储引擎会监控对表上各索引页的查询。如果观察到简历哈希索引可以带来速度提升，则建立哈希索引 AHI 是通过缓冲池的B+树页构造而来，因此建立的速度很快，而且不需要对整张表结构建立哈希索引。 在启用 AHI 后，读取和写入的速度可以提升2呗，辅助索引的连接操作性能可以提高5倍。 AHI 构成的条件 对一个页的连续访问模式必须是一致的，也就是查询条件必须是一致的 以同一种查询条件进行了100次以上的访问 💓异步IO （Async IO）为了提高磁盘操作性能，当前的数据库系统都采用异步 io 的方式来处理磁盘操作。 异步IO 的优势： 如果用户发出的是一条索引三秒的查询，那么这条SQL 查询语句可能需要扫描多个索引页，也就是要进行多次IO 操作。用户可以在发出一个 IO 请求后立即再发出两一个IO 请求，当全部IO 请求发送完毕后，等待所有IO 操作的完成。 可以进行IO Merge 操作，将多个IO 合并成一个 IO。 例如需要访问页 （8，6） （8，7） （8，8） ，就会进行 IO merge 操作 合并成 （8，6）的访问 💓刷新邻接页 （Flush Neighbor Page）刷新邻接页：当刷新一个脏页时，InnoDB 存储引擎会检测该页所在区的所有也，如果时脏页，那么一起进行刷新，通过 AIO 可以将多个IO 写入操作合并为一个 IO 操作。","link":"/2025/05/26/mysql/innodb/InnoDB%20%E7%9A%84%E5%85%B3%E9%94%AE%E7%89%B9%E6%80%A7/"},{"title":"函数","text":"一等函数在 Python 中，函数是一等对象。编程语言理论家把“一等对象”定义为满足下述条件的程序实体： 在运行时创建 能赋值给变量或数据结构中的元素 能作为参数传给函数 能作为函数的返回结果 函数视作对象Python 函数是对象。这里我们创建了一个函数，然后调用它，读取它的__doc__ 属性，并且确定函数对象本身是 function 类的实例 12345678910&gt;&gt;&gt; def factorial(n): ➊... '''returns n!'''... return 1 if n &lt; 2 else n * factorial(n-1)...&gt;&gt;&gt; factorial(42)1405006117752879898543142606244511569936384000000000&gt;&gt;&gt; factorial.__doc__ ➋'returns n!'&gt;&gt;&gt; type(factorial) ➌&lt;class 'function'&gt; 我们可以把 factorial 函数赋值给变量 fact，然后通过变量名调用。我们还能把它作为参数传给 map 函数。map 函数返回一个可迭代对象，里面的元素是把第一个参数（一个函数）应用到第二个参数（一个可迭代对象，这里是 range(11)）中各个元素上得到的结果。 123456789&gt;&gt;&gt; fact = factoria&gt;&gt;&gt; fact&lt;function factorial at 0x...&gt;&gt;&gt;&gt; fact(5)120&gt;&gt;&gt; map(factorial, range(11))&lt;map object at 0x...&gt;&gt;&gt;&gt; list(map(fact, range(11)))[1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800 高阶函数接受函数为参数，或者把函数作为结果返回的函数是高阶函数（higher-orderfunction） 内置函数 sorted 可选的key 参数用于提供一个函数，它会应用到各个元素上进行排序， 1234&gt;&gt;&gt; fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']&gt;&gt;&gt; sorted(fruits, key=len)['fig', 'apple', 'cherry', 'banana', 'raspberry', 'strawberry']&gt;&gt;&gt; 根据反向拼写给一个单词列表排序 123456789&gt;&gt; def reverse(word):... return word[::-1]&gt;&gt;&gt; reverse('testing')'gnitset'# 列表中的字符串元素会先被反转，然后根据反转后的字符串进行排序。&gt;&gt;&gt; sorted(fruits, key=reverse)['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry']&gt;&gt;&gt; map、filter和reduce的现代替代品在 Python 3 中，map 和 filter 还是内置函数，但是由于引入了列表推导和生成器表达式，它们变得没那么重要了。列表推导或生成器表达式具有 map 和 filter 两个函数的功能，而且更易于阅读. 在 Python 3 中，map 和 filter 返回生成器（一种迭代器） 1234567891011121314&gt;&gt;&gt; list(map(fact, range(6))) 构建 0! 到 5! 的一个阶乘列表。[1, 1, 2, 6, 24, 120]&gt;&gt;&gt; [fact(n) for n in range(6)] 使用列表推导执行相同的操作。[1, 1, 2, 6, 24, 120]使用 map 和 filter 计算直到 5! 的奇数阶乘列表。&gt;&gt;&gt; list(map(factorial, filter(lambda n: n % 2, range(6)))) [1, 6, 120]使用列表推导做相同的工作，换掉 map 和 filter，并避免了使用 lambda 表达式。&gt;&gt;&gt; [factorial(n) for n in range(6) if n % 2] [1, 6, 120]&gt;&gt;&gt; sum 和 reduce 的通用思想是把某个操作连续应用到序列的元素上，累计之前的结果，把一系列值归约成一个值。 使用 reduce 和 sum 计算 0~99 之和 1234567&gt;&gt;&gt; from functools import reduce ➊&gt;&gt;&gt; from operator import add ➋&gt;&gt;&gt; reduce(add, range(100)) ➌4950&gt;&gt;&gt; sum(range(100)) ➍4950&gt;&gt;&gt; 匿名函数lambda 关键字在 Python 表达式内创建匿名函数。Python 简单的句法限制了 lambda 函数的定义体只能使用纯表达式。换句话说，lambda 函数的定义体中不能赋值，也不能使用 while 和 try 等 Python 语句。 lambda 句法只是语法糖：与 def 语句一样，lambda 表达式会创建函数对象。这是Python 中几种可调用对象的一种。 使用 lambda 表达式反转拼写，然后依此给单词列表排序 1234&gt;&gt;&gt; fruits = ['strawberry', 'fig', 'apple', 'cherry', 'raspberry', 'banana']&gt;&gt;&gt; sorted(fruits, key=lambda word: word[::-1])['banana', 'apple', 'fig', 'raspberry', 'strawberry', 'cherry']&gt;&gt;&gt; 可调用对象除了用户定义的函数，调用运算符（即 ()）还可以应用到其他对象上。如果想判断对象能否调用，可以使用内置的 callable() 函数。Python 数据模型文档列出了 7 种可调用对象。 用户定义的函数 使用 def 语句或 lambda 表达式创建。 内置函数 使用 C 语言（CPython）实现的函数，如 len 或 time.strftime。 内置方法 使用 C 语言实现的方法，如 dict.get。 方法 在类的定义体中定义的函数。 类 调用类时会运行类的__new__方法创建一个实例，然后运行__init__方法初始化实例，最后把实例返回给调用方。因为 Python 没有 new 运算符，所以调用类相当于调用函数。 类的实例 如果类定义了__call__ 方法，那么它的实例可以作为函数调用。 生成器函数 使用 yield 关键字的函数或方法。调用生成器函数返回的是生成器对象。 用户定义的可调用类型不仅 Python 函数是真正的对象，任何 Python 对象都可以表现得像函数。为此，只需实现实例方法__call__。 实现了 BingoCage 类。这个类的实例使用任何可迭代对象构建，而且会在内部存储一个随机顺序排列的列表。调用实例会取出一个元素。 123456789101112import randomclass BingoCage: def __init__(self, items): self._items = list(items) random.shuffle(self._items) def pick(self): try: return self._items.pop() except IndexError: raise LookupError('pick from empty BingoCage') def __call__(self): return self.pick() 1234567&gt;&gt;&gt; bingo = BingoCage(range(3))&gt;&gt;&gt; bingo.pick()1&gt;&gt;&gt; bingo()0&gt;&gt;&gt; callable(bingo)True 函数内省除了 __doc__，函数对象还有很多属性。 123456789&gt;&gt;&gt; dir(factorial)['__annotations__', '__call__', '__class__', '__closure__', '__code__','__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__','__format__', '__ge__', '__get__', '__getattribute__', '__globals__','__gt__', '__hash__', '__init__', '__kwdefaults__', '__le__', '__lt__','__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__','__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__','__subclasshook__']&gt;&gt;&gt; 用户定义的函数的属性 函数参数Python 3 进一步提供了仅限关键字参数（keyword-only argument）。，调用函数时使用 * 和**“展开”可迭代对象，映射到单个参数。 关键字参数（Keyword Arguments） 关键字参数是指在函数调用时，通过参数名来传递值的方式。这样可以不按照函数定义的参数顺序传值。 仅限关键字参数（Keyword-Only Arguments）仅限关键字参数是 Python 3 引入的特性，要求某些参数必须通过关键字参数的形式传递，不能通过位置传递。 *args 和 **kwargs 是两种特殊的参数语法，用于处理函数调用时的可变参数。 *args（可变位置参数） 作用：接收任意数量的位置参数（非关键字参数），并将它们打包成一个元组（tuple）。 语法：在函数定义中使用 *args（args 可自定义名称，通常用 args）。 **kwargs（可变关键字参数） 作用：接收任意数量的关键字参数，并将它们打包成一个字典（dict）。 语法：在函数定义中使用 **kwargs（kwargs 可自定义名称，通常用 kwargs）。 12345678910111213def tag(name, *content, cls=None, **attrs): &quot;&quot;&quot;生成一个或多个HTML标签&quot;&quot;&quot; if cls is not None: attrs['class'] = cls if attrs: attr_str = ''.join(' %s=&quot;%s&quot;' % (attr, value) for attr, value in sorted(attrs.items())) else: attr_str = '' if content: return '\\n'.join('&lt;%s%s&gt;%s&lt;/%s&gt;' % (name, attr_str, c, name) for c in content) else: return '&lt;%s%s /&gt;' % (name, attr_str) 123456789101112131415161718192021222324&gt;&gt;&gt; tag('br') 传入单个定位参数，生成一个指定名称的空标签。'&lt;br /&gt;'&gt;&gt;&gt; tag('p', 'hello') 第一个参数后面的任意个参数会被 *content 捕获，存入一个元组。'&lt;p&gt;hello&lt;/p&gt;'&gt;&gt;&gt; print(tag('p', 'hello', 'world'))&lt;p&gt;hello&lt;/p&gt;&lt;p&gt;world&lt;/p&gt;&gt;&gt;&gt; tag('p', 'hello', id=33) tag 函数签名中没有明确指定名称的关键字参数会被 **attrs 捕获，存入一个字典。'&lt;p id=&quot;33&quot;&gt;hello&lt;/p&gt;'&gt;&gt;&gt; print(tag('p', 'hello', 'world', cls='sidebar')) cls 参数只能作为关键字参数传入。&lt;p class=&quot;sidebar&quot;&gt;hello&lt;/p&gt;&lt;p class=&quot;sidebar&quot;&gt;world&lt;/p&gt;&gt;&gt;&gt; tag(content='testing', name=&quot;img&quot;) 调用 tag 函数时，即便第一个定位参数也能作为关键字参数传入。'&lt;img content=&quot;testing&quot; /&gt;'&gt;&gt;&gt; my_tag = {'name': 'img', 'title': 'Sunset Boulevard',... 'src': 'sunset.jpg', 'cls': 'framed'}&gt;&gt;&gt; tag(**my_tag) 在 my_tag 前面加上 **，字典中的所有元素作为单个参数传入，同名键会绑定到对应的具名参数上，余下的则被 **attrs 捕获。'&lt;img class=&quot;framed&quot; src=&quot;sunset.jpg&quot; title=&quot;Sunset Boulevard&quot; /&gt;' 获取函数参数信息在 Python 里，可借助 inspect 模块获取函数的参数信息 获取函数的参数名称 1234567891011121314import inspectdef add(a, b=1, *args, **kwargs): passsig = inspect.signature(add)params = sig.parametersfor name, param in params.items(): print(f&quot;参数名: {name}&quot;) print(f&quot; 参数类型: {param.kind}&quot;) print(f&quot; 默认值: {param.default if param.default is not inspect.Parameter.empty else '未设置'}&quot;) print(f&quot; 是否必须: {param.default is inspect.Parameter.empty and param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)}&quot;) print() 在函数内部获取调用时的参数值 12345678910111213import inspectdef greet(name, message=&quot;你好&quot;): frame = inspect.currentframe() try: args, _, _, values = inspect.getargvalues(frame) print(&quot;函数被调用时的参数值:&quot;) for arg in args: print(f&quot;{arg}: {values[arg]}&quot;) finally: del frame # 防止内存泄漏greet(&quot;小明&quot;, &quot;吃了吗？&quot;) 获取方法的参数信息（包含 self） 12345678910111213141516import inspectclass Calculator: def multiply(self, x, y): pass# 获取未绑定的方法method = Calculator.multiplysig = inspect.signature(method)print(&quot;方法参数:&quot;, list(sig.parameters.keys())) # 输出: ['self', 'x', 'y']# 或者通过实例获取绑定的方法calc = Calculator()method = calc.multiplysig = inspect.signature(method)print(&quot;方法参数:&quot;, list(sig.parameters.keys())) # 输出: ['self', 'x', 'y'] 获取任意可调用对象的参数信息 12345678910111213import inspect# 对于 lambda 函数lambda_func = lambda x, y: x + ysig = inspect.signature(lambda_func)print(&quot;lambda 参数:&quot;, list(sig.parameters.keys())) # 输出: ['x', 'y']# 对于内置函数（部分内置函数可能无法获取）try: sig = inspect.signature(len) print(&quot;len 参数:&quot;, list(sig.parameters.keys()))except ValueError as e: print(f&quot;无法获取 len 函数的参数信息: {e}&quot;) 函数注解在 Python 中，函数注解（Function Annotations）是一种为函数参数和返回值添加元数据的语法特性。从 Python 3.0 开始引入，到 Python 3.5 通过类型提示（Type Hints）进一步增强。函数注解不会影响函数的运行逻辑，但可以提供类型信息、文档说明或其他元数据。 基本语法函数注解的语法非常简单，使用冒号 : 为参数添加注解，使用 -&gt; 为返回值添加注解： 12def add(a: int, b: int) -&gt; int: return a + b 参数注解参数注解可以是任何表达式或对象，通常用于： 类型提示：指定参数期望的类型 文档说明：提供参数的描述或约束 默认值结合：注解在默认值之前 12def greet(name: str, age: int = 18) -&gt; str: return f&quot;Hello, {name} ({age})&quot; 返回值注解使用 -&gt; 符号指定返回值的注解： 1234567def get_name() -&gt; str: return &quot;Alice&quot;def divide(a: float, b: float) -&gt; float | str: if b == 0: return &quot;Cannot divide by zero&quot; return a / b 访问注解信息函数注解存储在函数的 __annotations__ 属性中，可以在运行时访问： 12345def example(a: int, b: str = &quot;test&quot;) -&gt; bool: passprint(example.__annotations__)# 输出: {'a': &lt;class 'int'&gt;, 'b': &lt;class 'str'&gt;, 'return': &lt;class 'bool'&gt;} 高级用法 自定义类型：使用 NewType 创建自定义类型 123456from typing import NewTypeUserId = NewType(&quot;UserId&quot;, int)def get_user(id: UserId) -&gt; str: pass 泛型：使用 TypeVar 和 Generic 创建泛型函数 123456from typing import TypeVar, ListT = TypeVar('T')def first(items: List[T]) -&gt; T: return items[0] 可选类型：使用 Optional 表示参数可以为 None 1234from typing import Optionaldef find_user(name: str) -&gt; Optional[dict]: pass","link":"/2025/05/26/python/%E5%9F%BA%E7%A1%80/%E5%87%BD%E6%95%B0/"},{"title":"字典和集合","text":"泛映射类型 collections.abc 模块中有 Mapping 和 MutableMapping 这两个抽象基类，它们的作用是为 dict 和其他类似的类型定义形式接口（在 Python 2.6 到 Python 3.2 的版本中，这些类还不属于 collections.abc 模块，而是隶属于collections 模块）。 标准库里的所有映射类型都是利用 dict 来实现的，因此它们有个共同的限制，即只有可散列的数据类型才能用作这些映射里的键。 如果一个对象是可散列的，那么在这个对象的生命周期中，它的散列值是不变的，而且这个对象需要实现__hash__() 方法。另外可散列对象还要有__qe__() 方法，这样才能跟其他键做比较。 字典推导12345678910111213141516171819202122&gt;&gt;&gt; DIAL_CODES = [ 一个承载成对数据的列表，它可以直接用在字典的构造方法中... (86, 'China'),... (91, 'India'),... (1, 'United States'),... (62, 'Indonesia'),... (55, 'Brazil'),... (92, 'Pakistan'),... (880, 'Bangladesh'),... (234, 'Nigeria'),... (7, 'Russia'),... (81, 'Japan'),... ]&gt;&gt;&gt; country_code = {country: code for code, country in DIAL_CODES} 这里把配好对的数据左右换了下，国家名是键，区域码是值。&gt;&gt;&gt; country_code{'China': 86, 'India': 91, 'Bangladesh': 880, 'United States': 1,'Pakistan': 92, 'Japan': 81, 'Russia': 7, 'Brazil': 55, 'Nigeria':234, 'Indonesia': 62}&gt;&gt;&gt; {code: country.upper() for country, code in country_code.items() if code &lt; 66} 跟上面相反，用区域码作为键，国家名称转换为大写，并且过滤掉区域码大于或等于66 的地区。{1: 'UNITED STATES', 55: 'BRAZIL', 62: 'INDONESIA', 7: 'RUSSIA'} 常见的映射方法dict、collections.defaultdict和collections.OrderedDict这三种映射类型的方法列表 defaultdict有时候为了方便起见，就算某个键在映射里不存在，我们也希望在通过这个键读取值的时候能得到一个默认值。有两个途径能帮我们达到这个目的，一个是通过defaultdict 这个类型而不是普通的 dict，另一个是给自己定义一个 dict 的子类，然后在子类中实现__missing__ 方法。 123456789101112131415import sysimport reimport collectionsWORD_RE = re.compile(r'\\w+')index = collections.defaultdict(list) 把 list 构造方法作为 default_factory 来创建一个 defaultdict。with open(sys.argv[1], encoding='utf-8') as fp: for line_no, line in enumerate(fp, 1): for match in WORD_RE.finditer(line): word = match.group() column_no = match.start()+1 location = (line_no, column_no) index[word].append(location) 如果 index 并没有 word 的记录，那么 default_factory 会被调用，为查询不到的键创造一个值。这个值在这里是一个空的列表，然后这个空列表被赋值给 index[word]，继而被当作返回值返回，因此 .append(location) 操作总能成功。 for word in sorted(index, key=str.upper): print(word, index[word]) 特殊方法 __missing__。它会在 defaultdict 遇到找不到的键的时候调用 default_factory，而实际上这个特性是所有映射类型都可以选择去支持的。 __missing__方法虽然基类 dict 并没有定义这个方法，但是 dict 是知道有这么个东西存在的。也就是说，如果有一个类继承了 dict，然后这个继承类提供了__missing__ 方法，那么在__getitem__ 碰到找不到的键的时候，Python 就会自动调用它，而不是抛出一个 KeyError 异常。 missing 方法只会被__getitem__ 调用（比如在表达式 d[k] 中）。提供__missing__ 方法对 get 或者 __contains__（in 运算符会用到这个方法）这些方法的使用没有影响。这也是defaultdict 中的default_factory 只对 getitem 有作用的原因。 字典的变种collections.OrderedDict这个类型在添加键的时候会保持顺序，因此键的迭代次序总是一致的。OrderedDict 的 popitem 方法默认删除并返回的是字典里的最后一个元素，但是如果像my_odict.popitem(last=False) 这样调用它，那么它删除并返回第一个被添加进去的元素。 collections.ChainMap该类型可以容纳数个不同的映射对象，然后在进行键查找操作的时候，这些对象会被当作一个整体被逐个查找，直到键被找到为止。这个功能在给有嵌套作用域的语言做解释器的时候很有用，可以用一个映射对象来代表一个作用域的上下文。 collections.Counter这个映射类型会给键准备一个整数计数器。每次更新一个键的时候都会增加这个计数器。所以这个类型可以用来给可散列表对象计数，或者是当成多重集来用——多重集合就是集合里的元素可以出现不止一次。Counter 实现了 + 和 - 运算符用来合并记录，还有像 most_common([n]) 这类很有用的方法。most_common([n]) 会按照次序返回映射里最常见的 n 个键和它们的计数。 colllections.UserDict这个类其实就是把标准 dict 用纯 Python 又实现了一遍。跟 OrderedDict、ChainMap 和 Counter 这些开箱即用的类型不同，UserDict 是让用户继承写子类的 子类化UserDict就创造自定义映射类型来说，以 UserDict 为基类，总比以普通的 dict 为基类要来得方便。 UserDict 并不是 dict 的子类，但是 UserDict 有一个叫作 data 的属性，是 dict 的实例，这个属性实际上是 UserDict 最终存储数据的地方。这样做的好处是，UserDict 的子类就能在实现__setitem__ 的时候避免不必要的递归，也可以让__contains__ 里的代码更简洁。 12345678910import collectionsclass StrKeyDict(collections.UserDict): def __missing__(self, key): if isinstance(key, str): raise KeyError(key) return self[str(key)] def __contains__(self, key): return str(key) in self.data def __setitem__(self, key, item): self.data[str(key)] = item ➍ UserDict 继承的是 MutableMapping，所以 StrKeyDict 里剩下的那些映射类型的方法都是从 UserDict、MutableMapping 和 Mapping 这些超类继承而来的。特别是最后的 Mapping 类，它虽然是一个抽象基类（ABC），但它却提供了好几个实用的方法。以下两个方法值得关注MutableMapping.update 这个方法不但可以为我们所直接利用，它还用在__init__ 里，让构造方法可以利用传入的各种参数（其他映射类型、元素是 (key, value) 对的可迭代对象和键值参数）来新建实例。因为这个方法在背后是用 self[key] = value 来添加新值的，所以它其实是在使用我们的__setitem__ 方法。 Mapping.get 不可变映射类型types 模块中引入了一个封装类名叫 MappingProxyType。如果给这个类一个映射，它会返回一个只读的映射视图。虽然是个只读视图，但是它是动态的。这意味着如果对原映射做出了改动，我们通过这个视图可以观察到，但是无法通过这个视图对原映射做出修改。 123456789101112131415161718192021&gt;&gt;&gt; from types import MappingProxyType&gt;&gt;&gt; d = {1:'A'}&gt;&gt;&gt; d_proxy = MappingProxyType(d)&gt;&gt;&gt; d_proxymappingproxy({1: 'A'})&gt;&gt;&gt; d_proxy[1] 'A'&gt;&gt;&gt; d_proxy[2] = 'x' Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: 'mappingproxy' object does not support item assignment&gt;&gt;&gt; d[2] = 'B'&gt;&gt;&gt; d_proxy d_proxy 是动态的，也就是说对 d 所做的任何改动都会反馈到它上面。mappingproxy({1: 'A', 2: 'B'})&gt;&gt;&gt; d_proxy[2]'B'&gt;&gt;&gt; 集合论集合的本质是许多唯一对象的聚集。因此，集合可以用于去重 12345&gt;&gt;&gt; l = ['spam', 'spam', 'eggs', 'spam']&gt;&gt;&gt; set(l){'eggs', 'spam'}&gt;&gt;&gt; list(set(l))['eggs', 'spam'] 集合中的元素必须是可散列的，set 类型本身是不可散列的，但是 frozenset 可以。因此可以创建一个包含不同 frozenset 的 set。 集合还实现了很多基础的中缀运算符。给定两个集合 a 和 b，a | b 返回的是它们的合集，a &amp; b 得到的是交集，而 a - b 得到的是差集。 集合字面量在 Python 3 里面，除了空集，集合的字符串表示形式总是以 {…} 的形式出现。 123456789101112&gt;&gt;&gt; s = {1}&gt;&gt;&gt; type(s)&lt;class 'set'&gt;&gt;&gt;&gt; s{1}&gt;&gt;&gt; s.pop()1&gt;&gt;&gt; sset()&gt;&gt;&gt; frozenset(range(10))frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}) 像 {1, 2, 3} 这种字面量句法相比于构造方法（set([1, 2, 3])）要更快且更易读。后者的速度要慢一些，因为 Python 必须先从 set 这个名字来查询构造方法，然后新建一个列表，最后再把这个列表传入到构造方法里。但是如果是像 {1, 2, 3} 这样的字面量，Python 会利用一个专门的叫作 BUILD_SET 的字节码来创建集合。 集合推导123456从 unicodedata 模块里导入 name 函数，用以获取字符的名字。&gt;&gt;&gt; from unicodedata import name 把编码在 32~255 之间的字符的名字里有“SIGN”单词的挑出来，放到一个集合里。&gt;&gt;&gt; {chr(i) for i in range(32, 256) if 'SIGN' in name(chr(i),'')} {'§', '=', '¢', '#', '¤', '&lt;', '¥', 'μ', '×', '$', '¶', '£', '©','°', '+', '÷', '±', '&gt;', '¬', '®', '%'} 集合的操作 集合的数学运算：这些方法或者会生成新集合，或者会在条件允许的情况下就地修改集合. 集合的比较运算符，返回值是布尔类型 集合类型的其他方法","link":"/2025/05/29/python/%E5%9F%BA%E7%A1%80/%E5%AD%97%E5%85%B8%E5%92%8C%E9%9B%86%E5%90%88/"},{"title":"globals","text":"","link":"/2025/05/30/python/%E5%9F%BA%E7%A1%80/globals/"}],"tags":[{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"python基础","slug":"python基础","link":"/tags/python%E5%9F%BA%E7%A1%80/"},{"name":"架构安全性","slug":"架构安全性","link":"/tags/%E6%9E%B6%E6%9E%84%E5%AE%89%E5%85%A8%E6%80%A7/"},{"name":"事务处理","slug":"事务处理","link":"/tags/%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/"},{"name":"jvm","slug":"jvm","link":"/tags/jvm/"},{"name":"透明多级分流系统","slug":"透明多级分流系统","link":"/tags/%E9%80%8F%E6%98%8E%E5%A4%9A%E7%BA%A7%E5%88%86%E6%B5%81%E7%B3%BB%E7%BB%9F/"},{"name":"缓存","slug":"缓存","link":"/tags/%E7%BC%93%E5%AD%98/"},{"name":"juc","slug":"juc","link":"/tags/juc/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"垃圾回收","slug":"垃圾回收","link":"/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"name":"sql优化","slug":"sql优化","link":"/tags/sql%E4%BC%98%E5%8C%96/"},{"name":"内存模型","slug":"内存模型","link":"/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"},{"name":"类加载","slug":"类加载","link":"/tags/%E7%B1%BB%E5%8A%A0%E8%BD%BD/"},{"name":"容错机制","slug":"容错机制","link":"/tags/%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/"},{"name":"InnoDB","slug":"InnoDB","link":"/tags/InnoDB/"},{"name":"锁与并发","slug":"锁与并发","link":"/tags/%E9%94%81%E4%B8%8E%E5%B9%B6%E5%8F%91/"}],"categories":[{"name":"redis","slug":"redis","link":"/categories/redis/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"分布式","slug":"分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"java","slug":"java","link":"/categories/java/"},{"name":"mysql","slug":"mysql","link":"/categories/mysql/"}],"pages":[{"title":"关于","text":"","link":"/about/index.html"},{"title":"Untitled","text":"","link":"/media/Template/Untitled.html"}]}